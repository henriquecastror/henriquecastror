{
  "hash": "5ee738b284d52e327fbc911a0429f27c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Empirical Methods in Finance'\nsubtitle: 'Practicing 5'\nauthor: 'Henrique C. Martins'\nformat:\n  revealjs: \n    slide-number: true\n    theme: simple\n    chalkboard: true\n    preview-links: auto\n    logo: figs/background8.png\n    css: logo.css\n    footer: '**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  '\n    multiplex: true\n    scrollable: true \ntitle-slide-attributes:\n    data-background-color: \"#b1cafa\"\ninclude-after: |\n  <script type=\"text/javascript\">\n    Reveal.on('ready', event => {\n      if (event.indexh === 0) {\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n      }\n    });\n    Reveal.addEventListener('slidechanged', (event) => {\n      if (event.indexh === 0) {\n        Reveal.configure({ slideNumber: null });\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n      }\n      if (event.indexh === 1) { \n        Reveal.configure({ slideNumber: 'c' });\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n      }\n    });\n  </script>\n\n---\n\n\n\n\n\n\n\n# Linear regression {.smaller}\n\n## Regression Basics {.smaller} \n\n\nLet's learn how to run a regression in R and how to compute the Beta.\n\nFirst, let's remember what a linear regression is. In a linear regression, we want to estimate the population paramenters $\\beta_0$ and $\\beta_1$ of the following model.\n\n$$ y = \\beta_0 + \\beta_1 \\times x + \\mu$$\n\nIn this setting, the variables $y$ and $x$ can have several names.\n\n| Y                  | X                    |\n|--------------------|----------------------|\n| Dependent variable | Independent variable |\n| Explained variable | Explanatory variable |\n| Response variable  | Control variable     |\n| Predicted variable | Predictor variable   |\n| Regressand         | Regressor            |\n\n\n## Regression Basics {.smaller} \n\nThe variable $\\mu$, called the error term, represents all factors that are $X$ that also affect $y$. These factors are unobserved in your model. It has specific properties and assumptions. \n\nThe parameter $\\beta_0$, i.e., the intercept, is often called the *constant term*, but it is rarely useful in the type of analysis we'll run. \n\nWe can estimate these parameters as follows:\n\n$$\\beta_1 = \\frac{Cov(x,y)}{Var(x)}$$\n\n$$\\beta_0 = \\hat{y} - \\hat{\\beta_1} \\hat{x}$$\n\n\n## Regression Basics {.smaller} \n\n\nLet's estimate Example 2.3 of Wooldridge (2020).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(wooldridge)\ndata(ceosal1)\nreg <- lm(ceosal1$salary ~ ceosal1$roe)\nsummary(reg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = ceosal1$salary ~ ceosal1$roe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1160.2  -526.0  -254.0   138.8 13499.9 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   963.19     213.24   4.517 1.05e-05 ***\nceosal1$roe    18.50      11.12   1.663   0.0978 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1367 on 207 degrees of freedom\nMultiple R-squared:  0.01319,\tAdjusted R-squared:  0.008421 \nF-statistic: 2.767 on 1 and 207 DF,  p-value: 0.09777\n```\n\n\n:::\n:::\n\n\n\n## Regression Basics {.smaller} \n\nLet's find the parameters manually now.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncov(ceosal1$roe, ceosal1$salary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1342.538\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(ceosal1$roe)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 72.56499\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(ceosal1$salary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1281.12\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(ceosal1$roe)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 17.18421\n```\n\n\n:::\n\n```{.r .cell-code}\n(b1 <- cov(ceosal1$roe, ceosal1$salary)/var(ceosal1$roe))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18.50119\n```\n\n\n:::\n\n```{.r .cell-code}\n(b0 <- mean(ceosal1$salary) - b1*mean(ceosal1$roe))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 963.1913\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n## Graph  {.smaller} \n\nLet's visualize the relationship between these variables now.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(ggthemes)\nggplot(ceosal1) +  geom_point( aes(x=roe, y=salary)) +\n                   geom_smooth(data = ceosal1, aes(x=roe, y=salary) , method = lm) + \n                   theme_solarized()\n```\n\n::: {.cell-output-display}\n![](part_O5_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nSo we see the positive relationship between a firm's ROE and the Salary paid to the CEO as the regression showed before.\n\n\n\n\n\n\n\n## Regression's coefficients {.smaller} \n\nLet's run the regression again.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel <- lm(ceosal1$salary ~ ceosal1$roe)\nsummary(model)\n## \n## Call:\n## lm(formula = ceosal1$salary ~ ceosal1$roe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1160.2  -526.0  -254.0   138.8 13499.9 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   963.19     213.24   4.517 1.05e-05 ***\n## ceosal1$roe    18.50      11.12   1.663   0.0978 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1367 on 207 degrees of freedom\n## Multiple R-squared:  0.01319,\tAdjusted R-squared:  0.008421 \n## F-statistic: 2.767 on 1 and 207 DF,  p-value: 0.09777\n```\n:::\n\n\n\n## Regression's coefficients {.smaller} \n\nBeta of ROE is **18.501**\n\nStandard error of ROE is **11.123**.\n\nT-stat of ROE is **1.663**.\n\nAnd p-value is **0.098**. So it is barely significantly different from zero at the 10% threshold. \n\nThe intercept is **963.191**.\n\n\n\n\n\n\n\n\n\n\n## Predicting salary {.smaller} \n\nThe line from the previous graph contains the estimated salary value of each CEO.\n\nLet's say that Firm A shows a ROE of 14.1 and a Salary of 1095. You know that the Beta of the linear regression is 18.501 and the intercept is 963.191. Using these estimates, you can estimate that the salary of Firm A's CEO is:\n\n$$14.1 \\times  18.501  + 963.191 = 1224.058$$\n\nIf you do the same to all observations in the dataset, you get the red points below. The darkgreen line connects the points and represents the association between ROE and Salary.\n\nNow, if you can trust your estimates you can estimate (\"predict\") the salary for a given new ROE. For instance, you could estimate what is the salary of a new firm that shows a ROE of, let's say, 30%.\n\n\n\n\n\n\n\n## R-squared {.smaller} \n\nWe can see in the previous example that the r-squared of the regression is 1.319 percent. This means that the variable ROE explains around 1.3% of the variation of Salary.\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"R\" code-line-numbers=\"true\"}\niris <- iris \n\nsummary(model)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01318862\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"R\" code-line-numbers=\"true\"}\nsummary(model)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.008421419\n```\n\n\n:::\n:::\n\n\n\n\n## Residual {.smaller} \n\nNotice in the graph above that there is a distance between the **\"real\"** values of salary (i.e., blue points) and the **estimated** values of salary (i.e., the red points).\n\nThis distance is called **error** or **residual**.\n\nOne thing that you need to understand is that, in a OLS model, the regression line is selected in a way that **minimizes the sum of the squared values of the residuals**.\n\nYou can compute the errors as follows.\n\nNotice that most residuals are very close to zero. This basically shows that the red points (i.e., the estimated value) are very close to the blue points (i.e., the \"real\" data) in most of the observations.\n\nBut notice there are some observations with large residual, showing they are very far from the estimated value.\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"R\" code-line-numbers=\"true\"}\niris <- iris \nceosal1$uhat <- resid(model)\nggplot(ceosal1) +  geom_point( aes(x=roe, y=uhat), color = \"darkred\") +\n                   theme_solarized()\n```\n\n::: {.cell-output-display}\n![](part_O5_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nThe residuals have a mean of zero.\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"R\" code-line-numbers=\"true\"}\niris <- iris \nsummary(ceosal1$uhat) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-1160.2  -526.0  -254.0     0.0   138.8 13499.9 \n```\n\n\n:::\n:::\n\n\n\n\n\n## Standard error and T-stat {.smaller} \n\n\nTo assess if the variables are significantly related, you need to assess the significance of $\\beta$ coefficients.\n\nUsing the example from Wooldridge, we know that the Beta of ROE is **18.501**, while the standard error of ROE is **11.123**.\n\nThe standard error is a measure of the accuracy of your estimate. If you find a large standard error, your estimate does not have good accuracy. Ideally, you would find small standard errors, meaning that your coefficient is accurately estimated. However, you do not have good control over the magnitude of the standard errors. \n\n\n## Standard error and T-stat {.smaller} \n\nIf you have a large standard error, probably you coefficient will not be significantly different from zero. You can test whether your coefficient is significantly different from zero computing the t-statistics as follows:\n\n$$t_{\\beta} = \\frac{\\hat{\\beta}}{se(\\hat{\\beta})}$$\n\nIf $t_{\\beta}$ is large enough, you can say that $\\beta$ is significantly different from zero.  Usually, $t_{\\beta}$ larger than 2 is enough to be significant. \n\n\n\n\n\n## Standard error and T-stat {.smaller} \n\nIn the previous example, you can find the  t-stat manually as follows:\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"R\" code-line-numbers=\"true\"}\niris <- iris \nsummary(model)$coefficients[2,1] / summary(model)$coefficients[2,2] \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.663289\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"R\" code-line-numbers=\"true\"}\nsummary(model)$coefficients[2,3]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.663289\n```\n\n\n:::\n:::\n\n\nLarge t-stats will lead you to low p-values. Usually, we interpret that p-values lower than 10% suggest significance, but you would prefer p-values lower than at least 5%.\n\nYou can find the p-values of the previous example as follows.\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"R\" code-line-numbers=\"true\"}\niris <- iris \nsummary(model)$coefficients[2,4]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09776775\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"R\" code-line-numbers=\"true\"}\npt(summary(model)$coefficients[2,3], 207, lower.tail=FALSE) * 2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09776775\n```\n\n\n:::\n:::\n\n\nIn this case, p-value is lower than 10% so you can make the case that the relationship is significant at the 10% level. But notice that the relationship is not significant at the level of 5%.\n\n\n\n\n\n## Confidence intervals {.smaller} \n\nWe can further explore significance calculating confidence intervals. First, let's compute the intervals at 5%. Because our test is a two-tailed test, 5% means 2.5% in each tail.\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"R\" code-line-numbers=\"true\"}\niris <- iris \nconfint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 2.5 %     97.5 %\n(Intercept) 542.790219 1383.59245\nceosal1$roe  -3.428196   40.43057\n```\n\n\n:::\n:::\n\n\n\n\n## Confidence intervals {.smaller} \n\n\nWe can see above that the interval contains the value of Zero (notice that the estimate of ROE goes from a negative to a positive value, thus containing the zero). This means that you cannot separate your estimate of Beta from zero. In other words, your coefficient is not *significantly different from zero* in this case. This supports the previous finding that the coefficient is not significantly different from zero at the level of 5%.\n\n\n## Confidence intervals {.smaller} \n\n\nLet's see what are the confidence intervals at the level of 10%.\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"R\" code-line-numbers=\"true\"}\niris <- iris \nconfint(model, level = 0.90)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    5 %       95 %\n(Intercept) 610.8655301 1315.51714\nceosal1$roe   0.1228163   36.87956\n```\n\n\n:::\n:::\n\n\n\nNow, we can see that the interval does not contain zero, which means that Beta is *significantly different from zero* at the level of 10%. Again, it confirms the previous findings.  \n\n\n\n\n\n\n\n\n## Multiple regressions {.smaller} \n\nA multiple regression follows a model like:\n\n$$ y = \\beta_0 + \\beta_1 \\times x_1 + \\beta_2 \\times x_2 +  ... + \\beta_k \\times x_k + \\mu$$\n\n## Multiple regressions {.smaller} \n\nLet's estimate this equation using Example 3.1 of Wooldridge.\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"R\" code-line-numbers=\"true\"}\niris <- iris \nlibrary(wooldridge)\ndata(gpa1)\ngpa <- lm(gpa1$colGPA ~ gpa1$hsGPA + gpa1$ACT)\nsummary(gpa)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = gpa1$colGPA ~ gpa1$hsGPA + gpa1$ACT)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.85442 -0.24666 -0.02614  0.28127  0.85357 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1.286328   0.340822   3.774 0.000238 ***\ngpa1$hsGPA  0.453456   0.095813   4.733 5.42e-06 ***\ngpa1$ACT    0.009426   0.010777   0.875 0.383297    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3403 on 138 degrees of freedom\nMultiple R-squared:  0.1764,\tAdjusted R-squared:  0.1645 \nF-statistic: 14.78 on 2 and 138 DF,  p-value: 1.526e-06\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n## **THANK YOU!** {background=\"#b1cafa\"}\n\n::: columns\n::: {.column width=\"60%\"}\n**QUESTIONS?**\n\n![](figs/qa2.png){width=\"150%\" heigth=\"150%\"}\n:::\n\n::: {.column width=\"40%\"}\n**Henrique C. Martins**\n\n-   [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)\n-   [Personal Website](https://henriquemartins.net/)\n-   [LinkedIn](https://www.linkedin.com/in/henriquecastror/)\n-   [Lattes](http://lattes.cnpq.br/6076997472159785)\n-   [Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)\\\n:::\n:::\n\n::: footer\n:::\n",
    "supporting": [
      "part_O5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}