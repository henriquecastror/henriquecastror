{
  "hash": "82914139452ec1dd82fd40caddda5ea5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Empirical Methods in Finance'\nsubtitle: 'Part 6'\nauthor: 'Henrique C. Martins'\nformat:\n  revealjs: \n    slide-number: true\n    theme: simple\n    chalkboard: true\n    preview-links: auto\n    logo: figs/background8.png\n    css: logo.css\n    footer: '**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  '\n    multiplex: true\n    scrollable: true \ntitle-slide-attributes:\n    data-background-color: \"#b1cafa\"\ninclude-after: |\n  <script type=\"text/javascript\">\n    Reveal.on('ready', event => {\n      if (event.indexh === 0) {\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n      }\n    });\n    Reveal.addEventListener('slidechanged', (event) => {\n      if (event.indexh === 0) {\n        Reveal.configure({ slideNumber: null });\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n      }\n      if (event.indexh === 1) { \n        Reveal.configure({ slideNumber: 'c' });\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n      }\n    });\n  </script>\n\n---\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Selection Bias {.smaller background=\"#e3e2b8\"}\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nBack to the selection bias example of before.\n\n-   Imagine that John and Mary are moving to the north of Canada.\n\n-   John has a history of respiratory disease and decide to buy insurance.\n\n-   Mary does not have a history of respiratory disease and decide not to buy insurance.\n\n\n\n| Default                     | John | Mary |\n|-----------------------------|:-----|-----:|\n| State of insurance          | 1    |    0 |\n| Situation without insurance | `3`  |    5 |\n| Situation with insurance    | 4    |  `5` |\n| Observed                    | 4    |    5 |\n| Effect                      | ?    |    ? |\n\n$$(Y_{1,john} - Y_{0,john}) + (Y_{1,Mary}- Y_{0,Mary}) = 4 - 3 + 5 - 5 = 0.5$$\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nRearranging the terms:\n\n\n$$(Y_{1,john} - Y_{0,Mary})   + (Y_{1,Mary}  - Y_{0,john})  = (4 - 5) + (5 - 3)  = 0.5$$\n$$We\\;see   + We\\;do\\;not\\;see  = (4 - 5) + (5 - 3)  = 0.5$$\n\nThe term $(Y_{1,Mary}  - Y_{0,john}) =  (5 - 3) = 2$ is the **selection bias**.\n\nIt exists because we are comparing two people that should not be compared.\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nSome notation:\n\n$d=1$ for the treated units (treatment group)\n\n$d=0$ for the treated units (control group)\n\n\n. . . \n\n\n$Y_{i}$ = Potential outcome of individual *i*.\n\n$Y_{i,1}$ or  $Y(1)$ = Potential outcome of individual *i*, treatement group.\n\n$Y_{i,0}$ or  $Y(0)$ = Potential outcome of individual *i*, control group.\n\n\n\n\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nSome notation:\n\nThese are the representations of the **causal effect** we often want to estimate.\n\n**Average Treatment Effect:**\n\nATE = $\\frac{1}{N} (E[Y_{i,1}] - E[Y_{i,0}])$\n\n. . . \n\n**Average Treatment Effect on the treated:**\n\nATET = $\\frac{1}{N} (E[Y_{i,1}|D_i=1] - E[Y_{i,0}|D_i=1])$\n\n. . . \n\n**Average Treatment Effect on the untreated:**\n\nATEU = $\\frac{1}{N} (E[Y_{i,1}|D_i=0] - E[Y_{i,0}|D_i=0])$\n\n. . . \n\nOf course, again, we cannot observe both potential outcomes of the same unit *i*.\n\n\n\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nWhen dealing with **causal inference**, we have to find ways to approximate what the hidden potential outcome of the treated units is. \n\nThat is, the challenge in identifying causal effects is that the untreated potential outcomes, $Y_{i,0}$, are never\nobserved for the treated group ($D_i= 1$). The \"second\" term in the following equation:\n\nATET = $\\frac{1}{N} (E[Y_{i,1}|D_i=1] - E[Y_{i,0}|D_i=1])$\n\n\nWe need an empirical design to **\"observe\"** what we do not really observe (i.e., the counterfactual). \n\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nMany options:\n\n- Matching/Balancing\n- Difference-in-differences (DiD)\n- Regression discontinuity design (RDD)\n- Synthetic control (Synth)\n- Instrumental variables\n\n\n\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nThe process of finding units that are comparable is called **matching**.\n\n. . .\n\n**Before we continue...**\n\n**We will match on observables. We cannot match on unobservables.**\n\nThus, you may want to write in your article \"selection bias due to observables\".\n\n. . .\n\n**Cunningham:**\n\n*Propensity score matching has not seen as wide adoption among economists as in other nonexperimental methods like regression discontinuity or difference-in-differences. The most common reason given for this is that economists are oftentimes skeptical that CIA can be achieved in any dataset almost as an article of faith. This is because for many applications, economists as a group are usually more concerned about selection on unobservables than they are selection on observables, and as such, they reach for matching methods less often.*\n\nCIA = CMI\n\n\n\n\n\n\n\n\n# Matching  {.smaller background=\"#e0cafc\"}\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\n**Matching** aims to compare the outcomes between observations that have the same values of all control variables, except that one unit is treated and the other is not. \n\n. . .\n\nIn this literature, the control variables used to matched are often called **covariates**.\n\nThat is, for each treated unit, the researcher finds an untreated unit that is similar in all covariates.\n\nThe implication is that the researcher can argue that \"*units are comparable after matching*\". \n\n\n\n\n\n\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\nThe easiest to see is **exact matching**: *it matches observations that have the exact same values*. \n\n- It might be doable if you have only one covariate. \n\n- Naturally, if you have only one covariate, you might still be left with some selection bias.\n\n  - In the previous example, health history is one important covariate that makes John and Mary different. \n  \n  - But what about life style? Nutrition? Etc. \n  \n\nAs the number of covariates grow, you cannot pursue exact matching. That is the job of PSM.\n\n\n\n\n\n\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\n**In exact matching, the causal effect estimator (ATET) is:**\n\n$$ATET = \\frac{1}{N} \\sum (E[Y_{i}] - E[Y_{j(i)}] | D_i=1)$$\n\nWhere $Y_{j(i)}$ is the j-th unit matched to the i-th unit based on the j-th being “closest to” the i-th unit for some  covariate. \n\nFor instance, let’s say that a unit in the treatment group has a covariate with a value of 2 and we find another unit in the control group (exactly one unit) with a covariate value of 2. \n\nThen we will impute the treatment unit’s missing counterfactual with the matched unit’s, and take a difference.\n\n\n\n\n\n\n\n## Matching {.smaller background=\"#e0cafc\"}\n\nConsider the following dataset from Cunningham:\n\n![](figs/scott.png)\n\n\n\n\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\n::: panel-tabset\n### R Averages\n\nAverage ages are very different. The salary of a 24 yrs old person is quite different than the salary of a 32 yrs person.\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(knitr)\nlibrary(kableExtra)\n\nread_data <- function(df)\n{\n  full_path <- paste(\"https://github.com/scunning1975/mixtape/raw/master/\",df, sep = \"\")\n  df <- read_dta(full_path)\n  return(df)\n}\ntraining_example <- read_data(\"training_example.dta\") %>% slice(1:20)\nsummary(training_example$age_treat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  18.00   20.25   23.00   24.30   28.50   33.00      10 \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nsummary(training_example$age_control)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   23.50   31.50   31.95   39.00   51.00 \n```\n\n\n:::\n:::\n\n\n\n### R Treated\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(knitr)\nlibrary(kableExtra)\n\nread_data <- function(df)\n{\n  full_path <- paste(\"https://github.com/scunning1975/mixtape/raw/master/\",df, sep = \"\")\n  df <- read_dta(full_path)\n  return(df)\n}\n\ntraining_example <- read_data(\"training_example.dta\") %>% slice(1:20)\n\nggplot(training_example, aes(x=age_treat)) +\n  stat_bin(bins = 10, na.rm = TRUE)\n```\n\n::: {.cell-output-display}\n![](part_6_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### R Control\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(knitr)\nlibrary(kableExtra)\n\nread_data <- function(df)\n{\n  full_path <- paste(\"https://github.com/scunning1975/mixtape/raw/master/\",df, sep = \"\")\n  df <- read_dta(full_path)\n  return(df)\n}\n\ntraining_example <- read_data(\"training_example.dta\") %>% slice(1:20)\n\nggplot(training_example, aes(x=age_control)) +\n  stat_bin(bins = 10, na.rm = TRUE)\n```\n\n::: {.cell-output-display}\n![](part_6_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n### R Matched\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(knitr)\nlibrary(kableExtra)\n\nread_data <- function(df)\n{\n  full_path <- paste(\"https://github.com/scunning1975/mixtape/raw/master/\",df, sep = \"\")\n  df <- read_dta(full_path)\n  return(df)\n}\n\ntraining_example <- read_data(\"training_example.dta\") %>% slice(1:20)\n\nggplot(training_example, aes(x=age_matched)) +\n  stat_bin(bins = 10, na.rm = TRUE)\n```\n\n::: {.cell-output-display}\n![](part_6_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n\n\n\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\nIn this example, you are literally finding the units in the control group that have the same age as the units in the treatment group.\n\nYou are exact matching 1-by-1 in this example.\n\nYou have only one covariate, i.e., age.\n\n\n\n\n\n\n\n\n\n\n\n# Distance Matching  {.smaller background=\"#c6f7ec\"}\n\n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\nThe last example was simple because you could *exact match*.\n\nIf you cannot find one exact match, you need an approximate match. \n\n. . .\n\nIn order to do that, you have to use distance matching.\n\n**Distance matching** minimizes the distance (i.e., how far the covariates are from each other) between the treatment and control groups.\n\n\n\n\n\n\n\n\n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\n**Euclidean distance** = $|X_i-X_j|=\\sqrt{(X_i-X_j)'(X_i-X_j)}=\\sqrt{\\sum_{n=1}^k(X_{n,i}-X_{n,j})^2}$\n\n![](figs/euclidian.png)\n\n\n\n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\n**Normalized Euclidean distance** = $|X_i-X_j|=\\sqrt{(X_i-X_j)'\\hat{V}^{-1}(X_i-X_j)}=\\sqrt{\\sum_{n=1}^k\\frac{(X_{n,i}-X_{n,j})}{\\sigma^2_n}}$\n\nThe problem with this metric of distance is that the distance measure itself depends on the **scale of the variables themselves**. \n\nFor this reason, researchers typically will use some modification of the Euclidean distance, such as the **normalized Euclidean distance**, or they’ll use a wholly different alternative distance. \n\nThe normalized Euclidean distance is a commonly used distance, and what makes it different is that the distance of each variable is scaled by the variable’s variance. \n\n\n \n \n \n \n \n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\n**Mahalanobis  distance** = $|X_i-X_j|=\\sqrt{(X_i-X_j)'\\hat{\\sum_x}^{-1}(X_i-X_j)}$\n\nWhere $\\hat{\\sum_x}$ is the sample covariance matrix of X.\n\n. . . \n\n![](figs/malahanobis_king_nielsen.png)\n\n\n\n\n\n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\nDistance matching only goes so far...\n\n... **the larger the dimensionality, the harder is to use distance matching**.\n\nAs sample size increases, for a given N of covariates, the matching discrepancies tend to zero.\n\nBut, the more covariates, the longer it takes.\n\n. . . \n\nAt the end of the day, it is preferable to have many covariates, but it  makes distance matching harder.\n\n\n::: {.callout-note}\n**At the end of the day, it is preferable to have many covariates, but it  makes distance matching harder.**\n:::\n\n\n\n\n\n# Coarsened Exact Matching (CER)  {.smaller background=\"#fce0cc\"}\n\n## Coarsened Exact Matching (CER)  {.smaller background=\"#fce0cc\"}\n\nIn coarsened exact matching, something only counts as a match if it exactly matches on each matching variable. \n\n**The “coarsened” part comes in because, if you have any continuous variables to match on, you need to “coarsen” them first by putting them into bins, rather than matching on exact values.**\n\nCoarsening means creating bins. Fewer bins makes exact matches more likely. \n\n. . .\n\nCER is not used much in empirical research in finance. It is used more in the big data realm when you have many variables to match. \n\n\n\n\n\n\n\n# Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n**PSM is one way to matching using many covariates.** \n\n**PSM aggregates all covariates into one score (propensity-score), which is the likelihood of receiving the treatment.**\n\nThe idea is to match units that, based on observables, have the same probability (called propensity-score) of being treated. \n\n. . .\n\nThe idea is to estimate a probit (default in stata) or logit model (fist stage):\n\n$$P(D=1|X)$$\n\n**The propensity-score is the predicted probability of a unit being treated given all covariates X**. The p-score is just a single number.\n\n\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nConsiderations in PSM.\n\n1) How many neighbors to match?\n\n   - Nearest neighbor, radius or kernel?\n\n2) With or without replacement?\n\n3) With or without common support?\n\n   - *Common support*: imposes a common support by dropping treatment observations whose pscore is higher than the maximum or less than the minimum pscore of the controls.\n\n4) It is expected that, after PSM, you show the overlap of propensity-scores.\n\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**The y-axis is the propensity-score**.\n\n![](figs/ani_katchova1.png)\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**Nearest matching:** Find the observation closest to ($min|p_i-p_j|$)\n\n![](figs/ani_katchova3.png)\n\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**Kernel matching:** Each treated observation i is matched with several control observations, with weights inversely proportional to the distance between treated and control observations.\n\n![](figs/ani_katchova2.png)\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**Radius matching**: Each treated observation i is matched with control observations j that fall within a specified radius.\n\n$$|p_i-p_j| <r$$\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**Common support:** Restrict matching only based on the common range of propensity scores.\n\n![](figs/ani_katchova5.png)\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nSeems good overlap, but \"good\" is arbitrary.\n\n![](figs/psm1.png)\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nSeems bad overlap\n\n![](figs/psm2.png)\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nSeems good overlap, but \"good\" is arbitrary.\n\n![](figs/psm_graph1.png)\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nSeems bad overlap\n\n![](figs/psm_graph2.png)\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n![](figs/psm_bias.png)\n\n\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n![](figs/psm_ttest1.png)\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n![](figs/psm_ttest2.png)\n\n\n\n\n\n\n\n\n\n\n# Example  {.smaller background=\"#dff5ce\"}\n\n## Example  {.smaller background=\"#dff5ce\"}\n\nLet's practice with an example. 185 treated units vs 15,992 control units. \n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# Load necessary packages\n# Load necessary libraries\nlibrary(haven)\nlibrary(psych)\ndata <- read_dta(\"files/cps1re74.dta\")\nsummary_stats <- by(data, data$treat, FUN = function(group) {\n  c(\n    mean = mean(group$age, na.rm = TRUE),\n    variance = var(group$age, na.rm = TRUE),\n    skewness = skew(group$age, na.rm = TRUE),\n    count = length(group$age)\n  )\n})\nsummary_df <- as.data.frame(do.call(rbind, summary_stats))\ncolnames(summary_df) <- c(\"mean\", \"variance\", \"skewness\", \"count\")\nprint(summary_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      mean variance  skewness count\n0 33.22524 121.9968 0.3477684 15992\n1 25.81622  51.1943 1.1063375   185\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nfrom scipy.stats import skew\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/cps1re74.dta\")\ngrouped_data = data.groupby('treat')['age'].agg(['mean', 'var', lambda x: skew(x, nan_policy='omit'), 'count']).reset_index()\ngrouped_data.columns = ['treat', 'mean', 'variance', 'skewness', 'count']\nprint(grouped_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   treat       mean    variance  skewness  count\n0      0  33.225238  121.996792  0.347801  15992\n1      1  25.816216   51.194301  1.115369    185\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse files/cps1re74.dta, clear\nqui estpost tabstat age black educ , by(treat) c(s) s(me v sk n) nototal\nesttab . \t,varwidth(20) cells(\"mean(fmt(3)) variance(fmt(3)) skewness(fmt(3)) count(fmt(0))\") noobs nonumber compress \n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(DW Subset of LaLonde Data)\n\n\n\n------------------------------------------------------------\n                                                            \n                          mean  variance  skewness     count\n------------------------------------------------------------\n0                                                           \nage                     33.225   121.997     0.348     15992\nblack                    0.074     0.068     3.268     15992\neduc                    12.028     8.242    -0.423     15992\n------------------------------------------------------------\n1                                                           \nage                     25.816    51.194     1.115       185\nblack                    0.843     0.133    -1.888       185\neduc                    10.346     4.043    -0.721       185\n------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n## Example  {.smaller background=\"#dff5ce\"}\n\nClearly, the treated group is younger, mainly black, and less educated.\n\nAlso note that the **variance and skewness** of the two subsamples are **different**.\n\nIf we were to use these two subsamples in any econometric analysis **without preprocessing to make them comparable**, we would likely have coefficients biased by **selection bias**.\n\nTherefore, it is important to perform some matching method.\n\nLet's start with Propensity Score Matching (PSM). We will use the simplest matching, that is, without using any additional functions.\n\n\n\n\n\n\n\n\n\n\n## Example  {.smaller background=\"#dff5ce\"}\n\n**Nearest with noreplacement.**\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# install.packages(\"MatchIt\")\nlibrary(haven)\nlibrary(psych)\nlibrary(MatchIt)\ndata <- read_dta(\"files/cps1re74.dta\")\nmodel <- matchit(treat ~ age + black + educ, data = data, method = \"nearest\")\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nmatchit(formula = treat ~ age + black + educ, data = data, method = \"nearest\")\n\nSummary of Balance for All Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.1445        0.0099          1.3615     7.5662    0.4987\nage            25.8162       33.2252         -1.0355     0.4196    0.1863\nblack           0.8432        0.0735          2.1171          .    0.7697\neduc           10.3459       12.0275         -0.8363     0.4905    0.0908\n         eCDF Max\ndistance   0.7741\nage        0.3427\nblack      0.7697\neduc       0.4123\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.1445        0.1443          0.0020     1.0039    0.0002\nage            25.8162       25.7081          0.0151     0.9244    0.0073\nblack           0.8432        0.8432          0.0000          .    0.0000\neduc           10.3459       10.4054         -0.0296     0.7190    0.0117\n         eCDF Max Std. Pair Dist.\ndistance   0.0162          0.0029\nage        0.0270          0.1481\nblack      0.0000          0.0000\neduc       0.0432          0.2554\n\nSample Sizes:\n          Control Treated\nAll         15992     185\nMatched       185     185\nUnmatched   15807       0\nDiscarded       0       0\n```\n\n\n:::\n:::\n\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse files/cps1re74.dta, clear\npsmatch2 treat age black educ , n(1) noreplacement\nsum _weight , d\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(DW Subset of LaLonde Data)\n\n\nProbit regression                                       Number of obs = 16,177\n                                                        LR chi2(3)    = 752.47\n                                                        Prob > chi2   = 0.0000\nLog likelihood = -634.83401                             Pseudo R2     = 0.3721\n\n------------------------------------------------------------------------------\n       treat | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |  -.0379829   .0042976    -8.84   0.000     -.046406   -.0295598\n       black |   1.700675   .0793898    21.42   0.000     1.545074    1.856277\n        educ |  -.0806962   .0142167    -5.68   0.000    -.1085604    -.052832\n       _cons |  -.9031083   .2123282    -4.25   0.000    -1.319264   -.4869526\n------------------------------------------------------------------------------\n\n            psmatch2: weight of matched controls\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%            1              1\n 5%            1              1\n10%            1              1       Obs                 370\n25%            1              1       Sum of wgt.         370\n\n50%            1                      Mean                  1\n                        Largest       Std. dev.             0\n75%            1              1\n90%            1              1       Variance              0\n95%            1              1       Skewness              .\n99%            1              1       Kurtosis              .\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n## Example  {.smaller background=\"#dff5ce\"}\n\n**Notice that we are creating weights now**\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# install.packages(\"MatchIt\")\nlibrary(haven)\nlibrary(MatchIt)\ndata <- read_dta(\"files/cps1re74.dta\")\nmodel <- matchit(treat ~ age + black + educ, data = data, method = \"exact\")\nsummary(model$weights)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.1457  0.0000 52.7952 \n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse files/cps1re74.dta, clear\nqui psmatch2 treat age black educ , kernel\nsum _weight , d\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(DW Subset of LaLonde Data)\n\n            psmatch2: weight of matched controls\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%     .0024355       .0007862\n 5%     .0024375       .0024348\n10%       .00244       .0024348       Obs              16,177\n25%     .0024517       .0024348       Sum of wgt.      16,177\n\n50%     .0024919                      Mean            .022872\n                        Largest       Std. dev.      .1130791\n75%     .0026476              1\n90%     .0038379              1       Variance       .0127869\n95%     .0876547              1       Skewness       7.604874\n99%            1              1       Kurtosis       64.26276\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n## Example  {.smaller background=\"#dff5ce\"}\n\n**Now, the descriptive statistics are much closer**\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(haven)\nlibrary(MatchIt)\n#install.packages(\"e1071\")\nlibrary(e1071)\ndata <- read_dta(\"files/cps1re74.dta\")\nmodel <- matchit(treat ~ age + black + educ, data = data, method = \"exact\")\nmatched_data <- match.data(model)\nsummary_stats <- by(matched_data, matched_data$treat, function(x) {\n  c(mean(x$age), var(x$age), skewness(x$age), length(x$age))\n})\n\nresult_df <- data.frame(\n  Treatment = c(\"Control\", \"Treated\"),\n  Mean_Age = sapply(summary_stats, function(x) x[1]),\n  Variance_Age = sapply(summary_stats, function(x) x[2]),\n  Skewness_Age = sapply(summary_stats, function(x) x[3]),\n  Count = sapply(summary_stats, function(x) x[4])\n)\nprint(result_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Treatment Mean_Age Variance_Age Skewness_Age Count\n0   Control 25.58786     39.16294    0.7826941  2191\n1   Treated 25.51807     51.59664    1.1298334   166\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse files/cps1re74.dta, clear\nqui psmatch2 treat age black educ , kernel\nqui estpost tabstat age black educ [aweight = _weight], by(treat) c(s) s(me v sk n) nototal\nesttab . \t,varwidth(20) cells(\"mean(fmt(3)) variance(fmt(3)) skewness(fmt(3)) count(fmt(0))\") noobs  nonumber compress \n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(DW Subset of LaLonde Data)\n\n\n\n\n------------------------------------------------------------\n                                                            \n                          mean  variance  skewness     count\n------------------------------------------------------------\n0                                                           \nage                     27.033    85.548     1.077     15992\nblack                    0.791     0.165    -1.434     15992\neduc                    10.710     8.146    -0.883     15992\n------------------------------------------------------------\n1                                                           \nage                     25.816    51.194     1.115       185\nblack                    0.843     0.133    -1.888       185\neduc                    10.346     4.043    -0.721       185\n------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Entropy Balancing  {.smaller background=\"#fccad9\"}\n\n## Entropy Balancing  {.smaller background=\"#fccad9\"}\n\n**Here, instead of matching units, we reweight the observations such that the moments of the distributions (mean, variance, skewness) are similar.**\n\n- The ebalance function implements a reweighting scheme. The user starts by choosing the covariates that should be included in the reweighting. \n\n- For each covariate, the user then specifies a set of balance constraints (in Equation 5) to equate the moments of the covariate distribution between the treatment and the reweighted control group. \n\n- The moment constraints may include the mean (first moment), the variance (second moment), and the skewness (third moment).\n\n**The outcome is a vector containing the weights to weight the observations, such that the weighted average, weighted variance, and weighted skewness of the covariates in control group are similar to those in the treatment group**\n\n\n\n\n\n\n\n\n\n\n## Entropy Balancing  {.smaller background=\"#fccad9\"}\n\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(haven)\n#install.packages(\"ebal\")\nlibrary(ebal)\ndata <- read_dta(\"files/cps1re74.dta\")\ntreatment <-cbind(data$treat)\nvars <-cbind(data$age, data$educ, data$black)\neb <- ebalance(treatment, vars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConverged within tolerance \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# means in treatment group data\napply(vars[treatment==1,],2,mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 25.8162162 10.3459459  0.8432432\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# means in reweighted control group data\napply(vars[treatment==0,],2,weighted.mean,w=eb$w)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 25.8163688 10.3460391  0.8431526\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# means in raw data control group data\napply(vars[treatment==0,],2,mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 33.22523762 12.02751376  0.07353677\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse files/cps1re74.dta, clear\nebalance treat age black educ, targets(3)\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(DW Subset of LaLonde Data)\n\n\n\nData Setup\nTreatment variable:   treat\nCovariate adjustment: age black educ (1st order). age black educ (2nd order). a\n> ge black educ (3rd order).\n\n\nOptimizing...\nIteration 1: Max Difference = 580799.347\nIteration 2: Max Difference = 213665.688\nIteration 3: Max Difference = 78604.7628\nIteration 4: Max Difference = 28918.6249\nIteration 5: Max Difference = 10640.1108\nIteration 6: Max Difference = 3915.82197\nIteration 7: Max Difference = 1442.09731\nIteration 8: Max Difference = 532.07826\nIteration 9: Max Difference = 197.376777\nIteration 10: Max Difference = 74.6380533\nIteration 11: Max Difference = 29.9524313\nIteration 12: Max Difference = 11.4337344\nIteration 13: Max Difference = 4.43722698\nIteration 14: Max Difference = 1.76899046\nIteration 15: Max Difference = .420548538\nIteration 16: Max Difference = .037814194\nIteration 17: Max Difference = .001164231\nmaximum difference smaller than the tolerance level; convergence achieved\n\n\nTreated units: 185     total of weights: 185\nControl units: 15992   total of weights: 185\n\n\nBefore: without weighting\n\n             |              Treat              |        Control       \n             |      mean   variance   skewness |      mean   variance \n-------------+---------------------------------+----------------------\n         age |     25.82      51.19      1.115 |     33.23        122 \n       black |     .8432      .1329     -1.888 |    .07354     .06813 \n        educ |     10.35      4.043     -.7212 |     12.03      8.242 \n\n             |  Control  \n             |  skewness \n-------------+-----------\n         age |     .3478 \n       black |     3.268 \n        educ |    -.4233 \n\n\nAfter:  _webal as the weighting variable\n\n             |              Treat              |        Control       \n             |      mean   variance   skewness |      mean   variance \n-------------+---------------------------------+----------------------\n         age |     25.82      51.19      1.115 |      25.8      51.17 \n       black |     .8432      .1329     -1.888 |     .8421       .133 \n        educ |     10.35      4.043     -.7212 |     10.34       4.04 \n\n             |  Control  \n             |  skewness \n-------------+-----------\n         age |     1.122 \n       black |    -1.877 \n        educ |    -.7121 \n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n\n\n\n## Entropy Balancing  {.smaller background=\"#fccad9\"}\n\n\n::: panel-tabset\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse files/cps1re74.dta, clear\nqui ebalance treat age black educ, targets(3)\nqui estpost tabstat age black educ [aweight = _webal], by(treat) c(s) s(me v sk n) nototal\nesttab . \t,varwidth(20) cells(\"mean(fmt(3)) variance(fmt(3)) skewness(fmt(3)) count(fmt(0))\") noobs  nonumber compress \n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(DW Subset of LaLonde Data)\n\n\n\n\n------------------------------------------------------------\n                                                            \n                          mean  variance  skewness     count\n------------------------------------------------------------\n0                                                           \nage                     25.801    51.167     1.122     15992\nblack                    0.842     0.133    -1.877     15992\neduc                    10.340     4.040    -0.712     15992\n------------------------------------------------------------\n1                                                           \nage                     25.816    51.194     1.115       185\nblack                    0.843     0.133    -1.888       185\neduc                    10.346     4.043    -0.721       185\n------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n## 🙋‍♂️ **Any Questions?**{ .smaller  background=\"#fdf6e3\"}\n\n::: columns\n::: {.column width=\"50%\"}\n### Thank You!\n\n![](figs/qa2.png){width=\"110%\" style=\"box-shadow: none;\"}\n\n:::\n\n::: {.column width=\"50%\"}\n<div style=\"text-align:right;\">\n  <img src=\"figs/avatar.jpg\" width=\"120px\" style=\"border-radius:50%; box-shadow:0 4px 12px rgba(0,0,0,.25);\" />\n</div>\n\n### **Henrique C. Martins**\n\n- 🌐 [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)  \n- 💼 [LinkedIn](https://www.linkedin.com/in/henriquecastror/)  \n- 🧠 [Google Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)  \n- 📄 [Lattes CV](http://lattes.cnpq.br/6076997472159785)  \n- 🏠 [Personal Website](https://henriquemartins.net/)  \n:::\n:::\n\n",
    "supporting": [
      "part_6_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}