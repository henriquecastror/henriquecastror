{
  "hash": "955e9d01a6a5387a22f039d53674d963",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Econometria Aplicada a Finanças'\nsubtitle: 'Part 6'\nauthor: 'Henrique C. Martins'\nformat:\n  revealjs: \n    slide-number: true\n    theme: simple\n    chalkboard: true\n    preview-links: auto\n    logo: figs/background2.png\n    css: styles.css\n    footer: <https://eaesp.fgv.br/>\n    multiplex: true\n---\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Multivariable models {.smaller background=\"#bdc7c9\"}\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\nIt is quite uncommon that you will have a model with only one independent variable.\n\nThe most frequent type of model in research is multivariate.\n\n\n$$y_i = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + . . . + \\beta_k x_k+ \\mu$$\n\n. . . \n\nThe estimation of $\\alpha$ refers to the predicted value of Y when all X's are zero (it might not make much sense if the variables cannot assume the value of zero). \n\n\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\nUsually, we think of $\\beta_k$ as having partial effects interpretations.\n\n- Meaning that we think $\\beta$ as the change in Y ($\\Delta y$) given a change in x ($\\Delta x_1$), ceteribus paribus \n\n    - i.e., holding all other changes as zero ($\\Delta x_2 = \\Delta x_3 = . . . = \\Delta x_k = 0$)\n\n- Thus, the \"effect\" or the \"association\" is $\\beta_1$, holding all else constant.\n\n\n\n\n. . . \n\nWe can predict the value of $y$ just like before.\n\n$$\\hat{y_i} = \\hat{\\alpha} + \\hat{\\beta_1} x_1 + \\hat{\\beta_2} x_2 + \\hat{\\beta_3} x_3 + . . . + \\hat{\\beta_k} x_k $$\n\n\n\n\n\n\n\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\nLike before, we will need some assumptions.\n\n$$E(\\mu | x_1,x_2, ... , x_k) = 0$$\n\nImplying no correlation between $\\mu$ and the X's.\n\n\n\n\n\n\n\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\n::: panel-tabset\n### R\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(sandwich)\nlibrary(foreign) \nlibrary(stargazer)\ndata <- read.dta(\"files/CEOSAL1.DTA\")\nmodel1 <- lm(salary ~ roe , data = data)\nmodel2 <- lm(salary ~ roe + lsales, data = data)\nstargazer(model1, model2 ,title = \"Regression Results\", type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression Results\n===============================================================\n                                Dependent variable:            \n                    -------------------------------------------\n                                      salary                   \n                            (1)                   (2)          \n---------------------------------------------------------------\nroe                       18.501*               22.674**       \n                          (11.123)              (10.982)       \n                                                               \nlsales                                         286.265***      \n                                                (92.332)       \n                                                               \nConstant                 963.191***           -1,482.294*      \n                         (213.240)             (815.971)       \n                                                               \n---------------------------------------------------------------\nObservations                209                   209          \nR2                         0.013                 0.057         \nAdjusted R2                0.008                 0.048         \nResidual Std. Error 1,366.555 (df = 207)  1,338.984 (df = 206) \nF Statistic         2.767* (df = 1; 207) 6.247*** (df = 2; 206)\n===============================================================\nNote:                               *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n### Python\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.iolib.summary2 import summary_col\n\ndata = pd.read_stata(\"files/CEOSAL1.DTA\")\nmodel1 = sm.OLS.from_formula(\"salary ~ roe\", data=data).fit()\nmodel2 = sm.OLS.from_formula(\"salary ~ roe + lsales\", data=data).fit()\nsummary = summary_col([model1, model2], stars=True)\nprint(summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n======================================\n                 salary I   salary II \n--------------------------------------\nIntercept      963.1913*** -1482.2944*\n               (213.2403)  (815.9709) \nR-squared      0.0132      0.0572     \nR-squared Adj. 0.0084      0.0480     \nlsales                     286.2647***\n                           (92.3318)  \nroe            18.5012*    22.6738**  \n               (11.1233)   (10.9816)  \n======================================\nStandard errors in parentheses.\n* p<.1, ** p<.05, ***p<.01\n```\n\n\n:::\n:::\n\n\n\n### Stata\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse \"files/CEOSAL1.DTA\" , replace\neststo: qui reg salary roe \neststo: qui reg salary roe lsales\nesttab\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(est1 stored)\n\n(est2 stored)\n\n\n--------------------------------------------\n                      (1)             (2)   \n                   salary          salary   \n--------------------------------------------\nroe                 18.50           22.67*  \n                   (1.66)          (2.06)   \n\nlsales                              286.3** \n                                   (3.10)   \n\n_cons               963.2***      -1482.3   \n                   (4.52)         (-1.82)   \n--------------------------------------------\nN                     209             209   \n--------------------------------------------\nt statistics in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\n**Goodness-of-fit**\n\nAs defined before\n\n$$R^2 = \\frac{SSE}{SST} = 1-\\frac{SSR}{SST}$$\n\nOne consequence of measuring $R^2$ this way is that it never goes down when you include more variables. \n\n- It is intuitive, if you are including more variables, you are taking stuff from the residual, increasing $R^2$.\n\n. . .\n\nIf you have multivariate models, that could be a problem, especially if you want to compare the $R^2$ of different models.\n\nWe often use:\n\n$$Adj\\;R^2 = 1-(1-R^2)\\frac{(n-1)}{n-k-1}$$\n\nWhere n is the number of observations and k is the number of independent variables, excluding the constant.\n\nAdj-R^2 can go up, but it can actually go down as well. \n\n\n\n\n# Multicollinearity {.smaller background=\"#7e96d6\"}\n\n## Multicollinearity {.smaller background=\"#7e96d6\"}\n\nWhen control variables show high correlation, the model may present **Multicollinearity**.\n\n- Highly collinear variables can inflate SEs\n\n- But, multicollinearity does not create a bias.\n\n. . .\n\nConsider a model such as:\n\n$$y_i= \\alpha + \\beta_1x1+\\beta_2x2+\\beta_3x3+ \\mu$$\n\n$x_2$ and $x3$ might be highly collinear. \n\n- This makes $var(\\beta_2)$ and  $var(\\beta_3)$ increase. \n\n- But that changes nothing on  $var(\\beta_1)$\n\n\n\n## Multicollinearity {.smaller background=\"#7e96d6\"}\n\nAt the end of the day, **multicollinearity is a non-problem**. It is very rare that I need to test it in my own research!\n\n\n- The tip is to not include controls that are too collinear with the variable of interest (i.e., the treatment).\n\n- Of course, if you need them for stablish causality, you need to include them, ignoring multicollinearity\n\n\n\n\n# Scaling {.smaller background=\"#f0e299\" }\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\n**Multiplying or dividing by a constant does not change your inference.**\n\nLet's say you multiply the Y by 1.000. What will change?\n\n. . . \n\nIn the example from before:\n\n- $\\alpha=963.19$\n- $\\beta=18.50$\n\nIf you multiply the Y (earnings) by 1.000, the new coefficients will be:\n\n- $\\alpha=963,190$\n- $\\beta=18,500$\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nScaling y by a constant c just causes all the estimates to be scaled by the same constant\n\n\n$$y=\\alpha + \\beta x + \\mu$$\n\n$$c.y = c.\\alpha + c.\\beta x + c.\\mu$$\n\n- new alpha: $c.\\alpha$\n- new beta: $c.\\beta$\n\n\n\n\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\n**The scaling has no effect on the relationship between X and Y.**\n\n::: panel-tabset\n### R\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(sandwich)\nlibrary(foreign) \nlibrary(lmtest)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\ndata$salary = data$salary * 1000\nmodel <- lm(salary ~ roe, data = data)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = salary ~ roe, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1160168  -526023  -253964   138797 13499886 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   963191     213240   4.517 1.05e-05 ***\nroe            18501      11123   1.663   0.0978 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1367000 on 207 degrees of freedom\nMultiple R-squared:  0.01319,\tAdjusted R-squared:  0.008421 \nF-statistic: 2.767 on 1 and 207 DF,  p-value: 0.09777\n```\n\n\n:::\n:::\n\n\n\n### Python\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\n\nmydata = pd.read_stata(\"files/CEOSAL1.DTA\")\narray1 = np.array([1000])\nmydata['salary'] = np.multiply(mydata['salary'], array1)\nX = sm.add_constant(mydata['roe'])  # Adding a constant (intercept) term\ny = mydata['salary'] \nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 salary   R-squared:                       0.013\nModel:                            OLS   Adj. R-squared:                  0.008\nMethod:                 Least Squares   F-statistic:                     2.767\nDate:                qua, 31 jan 2024   Prob (F-statistic):             0.0978\nTime:                        10:22:30   Log-Likelihood:                -3248.3\nNo. Observations:                 209   AIC:                             6501.\nDf Residuals:                     207   BIC:                             6507.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       9.632e+05   2.13e+05      4.517      0.000    5.43e+05    1.38e+06\nroe          1.85e+04   1.11e+04      1.663      0.098   -3428.196    4.04e+04\n==============================================================================\nOmnibus:                      311.096   Durbin-Watson:                   2.105\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            31120.902\nSkew:                           6.915   Prob(JB):                         0.00\nKurtosis:                      61.158   Cond. No.                         43.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n\n### Stata\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse \"files/CEOSAL1.DTA\" , replace\nreplace salary = salary * 1000\nqui reg salary roe \nesttab\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nvariable salary was int now long\n(209 real changes made)\n\n\n\n----------------------------\n                      (1)   \n                   salary   \n----------------------------\nroe               18501.2   \n                   (1.66)   \n\n_cons            963191.3***\n                   (4.52)   \n----------------------------\nN                     209   \n----------------------------\nt statistics in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nWhat if, instead, we multiply the x (ROE) by a constant 1000.\n\n$$y=\\alpha + \\beta x + \\mu$$\n\n$$y = \\alpha + \\frac{\\beta}{1.000} (1.000 x)+ \\mu$$\n\n\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\n**Only $\\beta$ changes.**\n\n::: panel-tabset\n### R\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(sandwich)\nlibrary(foreign) \nlibrary(lmtest)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\ndata$roe = data$roe * 1000\nmodel <- lm(salary ~ roe, data = data)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = salary ~ roe, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1160.2  -526.0  -254.0   138.8 13499.9 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 963.19134  213.24026   4.517 1.05e-05 ***\nroe           0.01850    0.01112   1.663   0.0978 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1367 on 207 degrees of freedom\nMultiple R-squared:  0.01319,\tAdjusted R-squared:  0.008421 \nF-statistic: 2.767 on 1 and 207 DF,  p-value: 0.09777\n```\n\n\n:::\n:::\n\n\n\n### Python\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\n\nmydata = pd.read_stata(\"files/CEOSAL1.DTA\")\narray1 = np.array([1000])\nmydata['roe'] = np.multiply(mydata['roe'], array1)\nX = sm.add_constant(mydata['roe'])  # Adding a constant (intercept) term\ny = mydata['salary'] \nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 salary   R-squared:                       0.013\nModel:                            OLS   Adj. R-squared:                  0.008\nMethod:                 Least Squares   F-statistic:                     2.767\nDate:                qua, 31 jan 2024   Prob (F-statistic):             0.0978\nTime:                        10:22:32   Log-Likelihood:                -1804.5\nNo. Observations:                 209   AIC:                             3613.\nDf Residuals:                     207   BIC:                             3620.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        963.1913    213.240      4.517      0.000     542.790    1383.592\nroe            0.0185      0.011      1.663      0.098      -0.003       0.040\n==============================================================================\nOmnibus:                      311.096   Durbin-Watson:                   2.105\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            31120.902\nSkew:                           6.915   Prob(JB):                         0.00\nKurtosis:                      61.158   Cond. No.                     4.32e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.32e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n\n\n:::\n:::\n\n\n\n### Stata\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse \"files/CEOSAL1.DTA\" , replace\nreplace roe = roe * 1000\nqui reg salary roe \nesttab\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(209 real changes made)\n\n\n\n----------------------------\n                      (1)   \n                   salary   \n----------------------------\nroe                0.0185   \n                   (1.66)   \n\n_cons               963.2***\n                   (4.52)   \n----------------------------\nN                     209   \n----------------------------\nt statistics in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nScaling is useful when we estimate very large or very small coefficients.\n\n- 0.000000185\n- 185000000.00\n\nSuch coefficients are hard to read.\n\n**Scaling affect the $\\beta$ and S.E. but do not affect the t-stat.**\n\n\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nTo write a better story in your article in terms of  magnitudes, it could be helpful to scale the variables by their sample standard deviation.\n\n- Let's say that $\\sigma_x$ and  $\\sigma_y$ are the s.d. of x and y, respectively. \n\n- Let's say that you divide X by $\\sigma_x$ ($k=\\frac{1}{\\sigma_x}$) and y by $\\sigma_y$ ($c=\\frac{1}{\\sigma_y}$).\n\n- Now, **units of x and y are standard deviations.**\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nYou would have:\n\n- $\\alpha$ scaled by ($c=\\frac{1}{\\sigma_x}$)\n\n- $\\beta$ scaled by ($\\frac{c=\\frac{1}{\\sigma_x}}{k=\\frac{1}{\\sigma_y}}$)\n\n$$c  y = c \\alpha + \\frac{c \\beta}{k} (k x)+ c \\mu$$\n\n\n$$\\frac{1}{\\sigma_y}  y = \\frac{1}{\\sigma_y} \\alpha + \\frac{\\sigma_x}{\\sigma_y} \\beta (\\frac{x}{\\sigma_x} )+ \\frac{1}{\\sigma_y} \\mu$$\n\n. . .\n\n\nSo, if you estimate a $\\beta$ of 0.2, it means that a 1 s.d. increase in x leads to a 0.2 s.d. increase in y.\n\n\n\n\n\n\n\n\n\n## Functional form of relationships {.smaller background=\"#abc8f7\"}\n\nIn many cases, you want to use the logarithm of a variable. This changes the interpretation of the coefficients you estimate.\n\n. . . \n\n\n- **log-log regression**: both Y and X are in log values $ln(Y) = \\alpha + \\beta \\times ln(X) + \\epsilon$. The interpretation of $\\beta$ in this case is: **one percent increase of $x$** leads to **$\\beta$ percent increase in $y$**.   \n\n. . . \n\n- **log-level regression**: both Y and X are in log values $ln(Y) = \\alpha + \\beta \\times X + \\epsilon$. The interpretation of $\\beta$ in this case is: **one unit  increase of $x$** leads to **$\\beta$ percent increase in $y$**.   \n\n\n. . . \n\n- **level-log regression**: both Y and X are in log values $Y = \\alpha + \\beta \\times ln(X) + \\epsilon$. The interpretation of $\\beta$ in this case is: **one percent increase of $x$** leads to **$\\frac{\\beta}{100}$ units increase in $y$**.   \n\n\n**Important note:** the misspecification of x’s is similar to the omitted variable bias (OVB).\n\n\n\n\n\n\n\n\n\n# Winsorization {.smaller background=\"#89faa7\"}\n\n\n## Winsorization {.smaller background=\"#89faa7\"}\n\nIn real research, one very common problem is when you have outliers.\n\nOutliers are observations very far from the mean. For instance, companies that have 800% of leverage ($\\frac{Debt}{TA}$). Clearly, situations like this are typing errors in the original dataset. And this is more common that one should expect.\n\nResearchers avoid excluding such variables. We only exclude when it is totally necessary.\n\nTo avoid using these weird values, we winsorize.\n\nUsually, 1% at both tails.\n\n\n\n\n\n## Winsorization {.smaller background=\"#89faa7\"}\n\n**Look at the following dispersion graphs.** Something weird?\n\n::: panel-tabset\n### R\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(ggplot2)\nlibrary(foreign) \nmydata <- read.dta(\"files/CEOSAL1.DTA\")\noptions(repr.plot.width=6, repr.plot.height=4) \nggplot(mydata, aes(x = roe, y = salary)) +\n  geom_point() +\n  labs(title = \"Salary vs. ROE\", x = \"ROE\", y = \"Salary\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](part_6_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n### Python\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nmydata = pd.read_stata(\"files/CEOSAL1.DTA\")\nplt.figure(figsize=(6, 4))  \nsns.scatterplot(x = \"roe\", y = \"salary\", data=mydata)\nsns.despine(trim=True)\nplt.title(\"Salary vs. ROE\")\nplt.xlabel(\"ROE\")\nplt.ylabel(\"Salary\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](part_6_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n### Stata\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse \"files/CEOSAL1.DTA\" , replace\ntwoway scatter salary roe\nqui graph export \"files/graph6_1.svg\", replace\n```\n:::\n\n\n\n![](files/graph6_1.svg) \n\n:::\n\n\n\n\n\n\n\n\n\n\n## Winsorization {.smaller background=\"#89faa7\"}\n\n**Take a look on the extreme values now.**\n\n::: panel-tabset\n### R\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(ggplot2)\nlibrary(foreign) \nlibrary(DescTools)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\ndata$w_salary     <- Winsorize(data$salary   , probs = c(0.05, 0.95) , na.rm = TRUE) \ndata$w_roe     <- Winsorize(data$roe   , probs = c(0.05, 0.95) , na.rm = TRUE) \noptions(repr.plot.width=6, repr.plot.height=4) \nggplot(data, aes(y = w_salary, x = w_roe)) +\n  geom_point() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](part_6_files/figure-html/unnamed-chunk-13-3.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n### Python\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport numpy as np\nimport pandas as pd\n\nmydata = pd.read_stata(\"files/CEOSAL1.DTA\")\nquantiles = [0.05, 0.95]\nmydata['w_salary'] = mydata['salary'].clip(np.percentile(mydata['salary'], quantiles[0]), np.percentile(mydata['salary'], quantiles[1]))\nmydata['w_roe'] = mydata['roe'].clip(np.percentile(mydata['roe'], quantiles[0]), np.percentile(mydata['roe'], quantiles[1]))\nplt.figure(figsize=(6, 4))  \nplt.scatter(mydata['w_roe'], mydata['w_salary'])\nplt.xlabel('Winsorized ROE')\nplt.ylabel('Winsorized Salary')\nplt.title('Scatter Plot of Winsorized Salary vs. Winsorized ROE')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](part_6_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n### Stata\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse \"files/CEOSAL1.DTA\" , replace\nwinsor salary , gen(w_salary) p(0.05)\nwinsor roe , gen(w_roe) p(0.05)\ntwoway scatter w_salary w_roe\nqui graph export \"files/graph6_2.svg\", replace\n```\n:::\n\n\n\n![](files/graph6_2.svg) \n\n:::\n\n\n\n\n\n## Winsorization {.smaller background=\"#89faa7\"}\n\n**Finally, take a look at the statistics.**\n\n::: panel-tabset\n### R\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(ggplot2)\nlibrary(foreign) \nlibrary(DescTools)\nlibrary(haven)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\ndata$w_salary     <- Winsorize(data$salary   , probs = c(0.05, 0.95) , na.rm = TRUE) \ndata$w_roe     <- Winsorize(data$roe   , probs = c(0.05, 0.95) , na.rm = TRUE) \nsummary_stats <- summary(data[c(\"salary\", \"w_salary\", \"roe\", \"w_roe\")])\nprint(summary_stats)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     salary         w_salary         roe            w_roe      \n Min.   :  223   Min.   : 450   Min.   : 0.50   Min.   : 6.92  \n 1st Qu.:  736   1st Qu.: 736   1st Qu.:12.40   1st Qu.:12.40  \n Median : 1039   Median :1039   Median :15.50   Median :15.50  \n Mean   : 1281   Mean   :1127   Mean   :17.18   Mean   :16.86  \n 3rd Qu.: 1407   3rd Qu.:1407   3rd Qu.:20.00   3rd Qu.:20.00  \n Max.   :14822   Max.   :2295   Max.   :56.30   Max.   :34.38  \n```\n\n\n:::\n:::\n\n\n\n### Python\n\n**Python for some reason is no good.**\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport numpy as np\n\ndata = pd.read_stata(\"files/CEOSAL1.DTA\")\nquantiles = [0.005, 0.095]\ndata['w_salary'] = data['salary'].clip(np.percentile(data['salary'], quantiles[0]), np.percentile(data['salary'], quantiles[1]))\ndata['w_roe'] = data['roe'].clip(np.percentile(data['roe'], quantiles[0]), np.percentile(data['roe'], quantiles[1]))\nsummary_stats = data[[\"salary\", \"w_salary\", \"roe\", \"w_roe\"]].describe()\nprint(summary_stats)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             salary    w_salary         roe       w_roe\ncount    209.000000  209.000000  209.000000  209.000000\nmean    1281.119617  229.491242   17.184210    0.775386\nstd     1372.345308    0.427314    8.518509    0.018128\nmin      223.000000  223.343200    0.500000    0.514560\n25%      736.000000  229.520800   12.400000    0.776640\n50%     1039.000000  229.520800   15.500000    0.776640\n75%     1407.000000  229.520800   20.000000    0.776640\nmax    14822.000000  229.520800   56.299999    0.776640\n```\n\n\n:::\n:::\n\n\n\n### Stata\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse \"files/CEOSAL1.DTA\" , replace\nwinsor salary , gen(w_salary) p(0.05)\nwinsor roe , gen(w_roe) p(0.05)\nestpost tabstat salary w_salary roe w_roe , s(min, mean, max, sd, count) c(s)\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSummary statistics: min mean max sd count\n     for variables: salary w_salary roe w_roe\n\n             |    e(min)    e(mean)     e(max)      e(sd)   e(count) \n-------------+-------------------------------------------------------\n      salary |       223    1281.12      14822   1372.345        209 \n    w_salary |       448   1128.306       2327    510.432        209 \n         roe |        .5   17.18421       56.3   8.518509        209 \n       w_roe |       6.8   16.89474       35.1   7.014505        209 \n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n# Models with binary variables {.smaller background=\"#ed95d5\" }\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\nA binary variable is quite simple to understand: it takes the value of 0 for one group, and 1 for the other.\n\n- 0 for men\n- 1 for women\n\nWe can explore many interesting types of binary variables in most cases of corporate finance. \n\nFor instance, whether the firm is included in \"Novo Mercado\", if the firm has high levels of ESG, etc. \n\n\n\n\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\nThe interpretation is a bit trickier. \n\nLet's think about the example 7.1 of Wooldridge. He estimates the following equation:\n\n$$wage = \\beta_0 + \\delta_1 female + \\beta_1 educ + \\mu$$\n\n- In model (7.1), only two observed factors affect wage: gender and education. \n\n- Because $female = 1$ when the person is female, and $female = 0 $ when the person is male, the parameter $\\delta_1$ has the following interpretation: \n\n\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\n- $\\delta_1$  is the difference in hourly wage between females and males, given the same amount of education (and the same error term u). \n\nThus, the coefficient $\\delta_1$  determines whether there is discrimination against women: \n\n- if $\\delta_1<0$, then, for the same level of other factors, women earn less than men on average.\n\n. . .\n\nIn terms of expectations, if we assume the zero conditional mean assumption E($\\mu$ | female,educ) = 0, then \n\n- $\\delta_1 = E(wage | female = 1, educ) - E(wage | female = 0, educ)$, or\n\n- $\\delta_1 = E(wage | female, educ) - E(wage | male, educ)$ \n\n- The key here is that the level of education is the same in both expectations; the difference, $\\delta_1$ , is due to gender only.\n\n\n\n\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\nThe visual interpretation is as follows. The situation can be depicted graphically as an intercept shift between males and females. The interpretation relies on $\\delta_1$. We can observe that $delta_1 < 0$; this is an argument for existence of a gender gap in wage.\n \n\n![](figs/wooldridge_7_1B.png)\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\n\n![](figs/wooldridge_7_1.png)\n\n\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\n\n::: panel-tabset\n### R\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\n\ndata <- read_dta(\"files/wage1.dta\")\nmodel <- lm(wage ~ female +  educ + exper + tenure , data)\nstargazer(model,title = \"Regression Results\", type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression Results\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                               wage            \n-----------------------------------------------\nfemale                       -1.811***         \n                              (0.265)          \n                                               \neduc                         0.572***          \n                              (0.049)          \n                                               \nexper                         0.025**          \n                              (0.012)          \n                                               \ntenure                       0.141***          \n                              (0.021)          \n                                               \nConstant                     -1.568**          \n                              (0.725)          \n                                               \n-----------------------------------------------\nObservations                    526            \nR2                             0.364           \nAdjusted R2                    0.359           \nResidual Std. Error      2.958 (df = 521)      \nF Statistic           74.398*** (df = 4; 521)  \n===============================================\nNote:               *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n### Python\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/wage1.dta\")\nX = data[['female', 'educ', 'exper', 'tenure']]\ny = data['wage']\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   wage   R-squared:                       0.364\nModel:                            OLS   Adj. R-squared:                  0.359\nMethod:                 Least Squares   F-statistic:                     74.40\nDate:                qua, 31 jan 2024   Prob (F-statistic):           7.30e-50\nTime:                        10:22:48   Log-Likelihood:                -1314.2\nNo. Observations:                 526   AIC:                             2638.\nDf Residuals:                     521   BIC:                             2660.\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -1.5679      0.725     -2.164      0.031      -2.991      -0.145\nfemale        -1.8109      0.265     -6.838      0.000      -2.331      -1.291\neduc           0.5715      0.049     11.584      0.000       0.475       0.668\nexper          0.0254      0.012      2.195      0.029       0.003       0.048\ntenure         0.1410      0.021      6.663      0.000       0.099       0.183\n==============================================================================\nOmnibus:                      185.864   Durbin-Watson:                   1.794\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              715.580\nSkew:                           1.589   Prob(JB):                    4.11e-156\nKurtosis:                       7.749   Cond. No.                         141.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n\n### Stata\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse \"files/wage1.dta\" , replace\neststo: qui reg wage female  educ exper  tenure\nesttab\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(est1 stored)\n\n\n----------------------------\n                      (1)   \n                     wage   \n----------------------------\nfemale             -1.811***\n                  (-6.84)   \n\neduc                0.572***\n                  (11.58)   \n\nexper              0.0254*  \n                   (2.20)   \n\ntenure              0.141***\n                   (6.66)   \n\n_cons              -1.568*  \n                  (-2.16)   \n----------------------------\nN                     526   \n----------------------------\nt statistics in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n\n# Models with quadratic terms {.smaller background=\"#32a89d\"  }\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\n\nLet’s say you have a variable that should not show a clear linear relationship with another variable.\n\nFor instance, consider **ownership concentration and firm value**.There is a case to be made the relationship between these variable is not linear. \n\n. . .\n\n- In low levels of ownership concentration (let’s say 5% of shares), a small increase in it might lead to an increase in firm value. The argument is that, in such levels, an increase in ownership concentration will lead the shareholder to monitor more the management maximizing the likelihood of value increasing decisions.\n\n. . . \n\n- But consider now the case where the shareholder has 60% or more of the firm’s outstanding shares. If you increase further the concentration it might signals the market that this shareholder is too powerful that might start using the firm to personal benefits (which will not be shared with minorities).\n\n. . .\n\n**If this story is true, the relationship is (inverse) u-shaped**. That is, at first the relationship is positive, then becomes negative.\n\n\n\n\n\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\nTheoretically, I could make an argument for a non-linear relationship between several variables of interest in finance. Let’s say size and leverage. Small firms might not be able to issue too much debt as middle size firms. At the same time, huge firms might not need debt. The empirical relationship might be non-linear.\n\n. . . \n\nAs noted before, misspecifying the functional form of a model can create biases. \n\nBut, in this specific case, the problem seems minor since we have the data to fix it.\n\n\n. . . \n\nThe model is:\n\n$$y_i = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon$$\n\n\n\n\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\n**How to include quadratic terms?** Create the variable and include as control. \n\n::: panel-tabset\n### R\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(foreign) \nlibrary(stargazer)\n\ndata <- read.dta(\"files/CEOSAL1.dta\")\n\ndata$roe_sq = data$roe * data$roe\nmodel1 <- lm(salary ~ roe, data=data)\nmodel2 <- lm(salary ~ roe + roe_sq, data=data)\n\nstargazer(model1, model2, title = \"Regression Results\", type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression Results\n=============================================================\n                               Dependent variable:           \n                    -----------------------------------------\n                                     salary                  \n                            (1)                  (2)         \n-------------------------------------------------------------\nroe                       18.501*               33.474       \n                          (11.123)             (36.439)      \n                                                             \nroe_sq                                          -0.314       \n                                               (0.728)       \n                                                             \nConstant                 963.191***           821.423**      \n                         (213.240)            (391.861)      \n                                                             \n-------------------------------------------------------------\nObservations                209                  209         \nR2                         0.013                0.014        \nAdjusted R2                0.008                0.005        \nResidual Std. Error 1,366.555 (df = 207) 1,369.249 (df = 206)\nF Statistic         2.767* (df = 1; 207) 1.471 (df = 2; 206) \n=============================================================\nNote:                             *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n### Python\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.iolib.summary2 import summary_col\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\ndata['roe_sq'] = data['roe'] ** 2\n# OLS model\nmodel1 = sm.OLS(data['salary'], sm.add_constant(data['roe'])).fit()\nmodel2 = sm.OLS(data['salary'], sm.add_constant(data[['roe', 'roe_sq']])).fit()\nsummary = summary_col([model1, model2], stars=True)\nprint(summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=====================================\n                 salary I  salary II \n-------------------------------------\nR-squared      0.0132      0.0141    \nR-squared Adj. 0.0084      0.0045    \nconst          963.1913*** 821.4228**\n               (213.2403)  (391.8609)\nroe            18.5012*    33.4737   \n               (11.1233)   (36.4387) \nroe_sq                     -0.3143   \n                           (0.7283)  \n=====================================\nStandard errors in parentheses.\n* p<.1, ** p<.05, ***p<.01\n```\n\n\n:::\n:::\n\n\n\n### Stata\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse \"files/CEOSAL1.DTA\" , replace\ngen roe_sq = roe * roe\neststo: qui reg salary roe \neststo: qui reg salary roe roe_sq\nesttab\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(est1 stored)\n\n(est2 stored)\n\n\n--------------------------------------------\n                      (1)             (2)   \n                   salary          salary   \n--------------------------------------------\nroe                 18.50           33.47   \n                   (1.66)          (0.92)   \n\nroe_sq                             -0.314   \n                                  (-0.43)   \n\n_cons               963.2***        821.4*  \n                   (4.52)          (2.10)   \n--------------------------------------------\nN                     209             209   \n--------------------------------------------\nt statistics in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\nIn the previous example, the quadratic term is not significant, suggesting the relationship is not quadratic.\n\nAlso, notice that the linear term is also not significant anymore.\n\n\n\n\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\n**Here, the association is non-linear? What does it mean?**\n\n\n\n::: panel-tabset\n### R\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\n\ndata <- read_dta(\"files/wage1.dta\")\nmodel <- lm(lwage ~ female +  educ + exper + expersq + tenure + tenursq, data)\nstargazer(model,title = \"Regression Results\", type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression Results\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                               lwage           \n-----------------------------------------------\nfemale                       -0.297***         \n                              (0.036)          \n                                               \neduc                         0.080***          \n                              (0.007)          \n                                               \nexper                        0.029***          \n                              (0.005)          \n                                               \nexpersq                      -0.001***         \n                             (0.0001)          \n                                               \ntenure                       0.032***          \n                              (0.007)          \n                                               \ntenursq                      -0.001**          \n                             (0.0002)          \n                                               \nConstant                     0.417***          \n                              (0.099)          \n                                               \n-----------------------------------------------\nObservations                    526            \nR2                             0.441           \nAdjusted R2                    0.434           \nResidual Std. Error      0.400 (df = 519)      \nF Statistic           68.177*** (df = 6; 519)  \n===============================================\nNote:               *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n### Python\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/wage1.dta\")\nX = data[['female', 'educ', 'exper', 'expersq', 'tenure', 'tenursq']]\ny = data['lwage']\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  lwage   R-squared:                       0.441\nModel:                            OLS   Adj. R-squared:                  0.434\nMethod:                 Least Squares   F-statistic:                     68.18\nDate:                qua, 31 jan 2024   Prob (F-statistic):           2.11e-62\nTime:                        10:22:52   Log-Likelihood:                -260.59\nNo. Observations:                 526   AIC:                             535.2\nDf Residuals:                     519   BIC:                             565.0\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.4167      0.099      4.212      0.000       0.222       0.611\nfemale        -0.2965      0.036     -8.281      0.000      -0.367      -0.226\neduc           0.0802      0.007     11.868      0.000       0.067       0.093\nexper          0.0294      0.005      5.916      0.000       0.020       0.039\nexpersq       -0.0006      0.000     -5.431      0.000      -0.001      -0.000\ntenure         0.0317      0.007      4.633      0.000       0.018       0.045\ntenursq       -0.0006      0.000     -2.493      0.013      -0.001      -0.000\n==============================================================================\nOmnibus:                       13.111   Durbin-Watson:                   1.796\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               24.212\nSkew:                          -0.085   Prob(JB):                     5.53e-06\nKurtosis:                       4.037   Cond. No.                     4.49e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.49e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n\n\n:::\n:::\n\n\n\n### Stata\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse \"files/wage1.dta\" , replace\neststo: qui reg lwage female  educ exper expersq tenure tenursq\nesttab\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(est1 stored)\n\n\n----------------------------\n                      (1)   \n                    lwage   \n----------------------------\nfemale             -0.297***\n                  (-8.28)   \n\neduc               0.0802***\n                  (11.87)   \n\nexper              0.0294***\n                   (5.92)   \n\nexpersq         -0.000583***\n                  (-5.43)   \n\ntenure             0.0317***\n                   (4.63)   \n\ntenursq         -0.000585*  \n                  (-2.49)   \n\n_cons               0.417***\n                   (4.21)   \n----------------------------\nN                     526   \n----------------------------\nt statistics in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n\n# Models with Interactions {.smaller background=\"#e3b174\"}\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\nIn some specific cases, you want to interact variables to test if the interacted effect is significant. \n\nFor instance, you might believe that, using Wooldridge very traditional example 7.4., women that are married are yet more discriminated in the job market than single women. \n\nSo, you may prefer to estimate the following equation to follow your intuition.\n\n$$wage = \\beta_0 + \\beta_1 female + \\beta_2 married + \\beta_3 female.married + \\mu$$\n\nWhere $maried$ is a binary variable marking all married people with 1, and 0 otherwise. $female$ marks 1 to women and 0 otherwise. \n\n\n\n\n\n\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\nIn this setting\n\n- The group of single men is the base case and is represented by $\\beta_0$. That is, both female and married are 0.\n\n- The group of single women is represented by $\\beta_0 + \\beta_1$. That is, female is 1 but married is 0.\n\n- The group of married men is represented by $\\beta_0 + \\beta_2$. That is, female is 0 but married is 1.\n\n- Finally, the group of married women is represented by $\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3$. That is, female and married are 1.\n\n\n\n\n\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\nUsing a random sample taken from the U.S. Current Population Survey for the year 1976, Wooldridge estimates that \n\n- $female<0$ \n\n- $married>0$\n\n- $female.married<0$\n\nThis result makes sense for the 70s. \n\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\n![](figs/wooldridge_7_4.png)\n\n\n\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\n\n\n::: panel-tabset\n### R\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\n\ndata <- read_dta(\"files/wage1.dta\")\ndata$fem_mar <- data$female * data$married\nmodel <- lm(lwage ~ female + married + fem_mar + educ + exper + expersq + tenure + tenursq, data)\nstargazer(model,title = \"Regression Results\", type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression Results\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                               lwage           \n-----------------------------------------------\nfemale                       -0.110**          \n                              (0.056)          \n                                               \nmarried                      0.213***          \n                              (0.055)          \n                                               \nfem_mar                      -0.301***         \n                              (0.072)          \n                                               \neduc                         0.079***          \n                              (0.007)          \n                                               \nexper                        0.027***          \n                              (0.005)          \n                                               \nexpersq                      -0.001***         \n                             (0.0001)          \n                                               \ntenure                       0.029***          \n                              (0.007)          \n                                               \ntenursq                      -0.001**          \n                             (0.0002)          \n                                               \nConstant                     0.321***          \n                              (0.100)          \n                                               \n-----------------------------------------------\nObservations                    526            \nR2                             0.461           \nAdjusted R2                    0.453           \nResidual Std. Error      0.393 (df = 517)      \nF Statistic           55.246*** (df = 8; 517)  \n===============================================\nNote:               *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n### Python\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/wage1.dta\")\ndata['fem_mar'] = data['female'] * data['married']\nX = data[['female', 'married', 'fem_mar', 'educ', 'exper', 'expersq', 'tenure', 'tenursq']]\ny = data['lwage']\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  lwage   R-squared:                       0.461\nModel:                            OLS   Adj. R-squared:                  0.453\nMethod:                 Least Squares   F-statistic:                     55.25\nDate:                qua, 31 jan 2024   Prob (F-statistic):           1.28e-64\nTime:                        10:22:55   Log-Likelihood:                -250.96\nNo. Observations:                 526   AIC:                             519.9\nDf Residuals:                     517   BIC:                             558.3\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.3214      0.100      3.213      0.001       0.125       0.518\nfemale        -0.1104      0.056     -1.980      0.048      -0.220      -0.001\nmarried        0.2127      0.055      3.842      0.000       0.104       0.321\nfem_mar       -0.3006      0.072     -4.188      0.000      -0.442      -0.160\neduc           0.0789      0.007     11.787      0.000       0.066       0.092\nexper          0.0268      0.005      5.112      0.000       0.017       0.037\nexpersq       -0.0005      0.000     -4.847      0.000      -0.001      -0.000\ntenure         0.0291      0.007      4.302      0.000       0.016       0.042\ntenursq       -0.0005      0.000     -2.306      0.022      -0.001   -7.89e-05\n==============================================================================\nOmnibus:                       15.526   Durbin-Watson:                   1.785\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               32.182\nSkew:                          -0.062   Prob(JB):                     1.03e-07\nKurtosis:                       4.205   Cond. No.                     5.06e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.06e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n\n\n:::\n:::\n\n\n\n### Stata\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse \"files/wage1.dta\" , replace\ngen fem_mar = female * married\neststo: qui reg lwage female married fem_mar educ exper expersq tenure tenursq\nesttab\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(est1 stored)\n\n\n----------------------------\n                      (1)   \n                    lwage   \n----------------------------\nfemale             -0.110*  \n                  (-1.98)   \n\nmarried             0.213***\n                   (3.84)   \n\nfem_mar            -0.301***\n                  (-4.19)   \n\neduc               0.0789***\n                  (11.79)   \n\nexper              0.0268***\n                   (5.11)   \n\nexpersq         -0.000535***\n                  (-4.85)   \n\ntenure             0.0291***\n                   (4.30)   \n\ntenursq         -0.000533*  \n                  (-2.31)   \n\n_cons               0.321** \n                   (3.21)   \n----------------------------\nN                     526   \n----------------------------\nt statistics in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n\n# Linear probability model {.smaller background=\"#7ae7eb\"}\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\nWhen the dependent variable is binary we cannot rely on linear models as those discussed so far. \n\nWe need a **linear probability model**. \n\nIn such models, we are interested in how the probability of the occurrence of an event depends on the values of x. That is, we want to know $P[y=1|x]$.\n\n\n. . .\n\n\nImagine that $y$ is employment status, 0 for unemployed, 1 for employed. \n\nImagine that we are interested in estimating the probability that a person start working after a training program. \n\nFor these types of problem, we need a linear probability model.\n\n$$P[y=1|x] = \\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_kx_k + \\epsilon$$\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\nThe mechanics of estimating these model is similar to before, except that $Y$ is binary.\n\nThe interpretation of coefficients change. That is, **a unit change in $x$ changes the probability of y = 1**. \n\nSo, let's say that $\\beta_1$ is 0.05. It means that changing $x_1$ by one unit will change the probability of $y = 1$ (i.e., getting a job) in 5%, ceteris paribus. \n\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\n\nUsing Wooldridge's example 7.29:\n\n\n![](figs/wooldridge_7_29.png)\n\n\nwhere:\n\n- $inlf$   =1 if in labor force, 1975\n\n\n\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\nThe relationship between the probability of labor force participation and $educ$ is plotted in the figure below. \n\nFixing the other independent variables at 50, 5, 30, 1 and 6, respectively, the predicted probability is negative until education equals 3.84 years. This is odd, since the model is predicting negative probability of employment given a set of specific values. \n\n\n![](figs/wooldridge_7_3.png)\n\n\n\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\n**Another example**\n\nThe model is predicting that *going from 0 to 4 kids less than 6 years old* reduces the probability of working by $4\\times 0.262 = 1.048$, which is impossible since it is higher than 1.\n\n. . .\n\n**The takeaway**\n\nThat is, one important caveat of a linear probability model is that probabilities might falls off of expected empirical values. \n\nIf this is problematic to us, we might need a different solution.\n\n\n\n\n\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\n::: panel-tabset\n### R\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\n\ndata <- read_dta(\"files/mroz.dta\")\nlpm <- lm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = data)\nstargazer(lpm,title = \"Regression Results\", type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression Results\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                               inlf            \n-----------------------------------------------\nnwifeinc                     -0.003**          \n                              (0.001)          \n                                               \neduc                         0.038***          \n                              (0.007)          \n                                               \nexper                        0.039***          \n                              (0.006)          \n                                               \nexpersq                      -0.001***         \n                             (0.0002)          \n                                               \nage                          -0.016***         \n                              (0.002)          \n                                               \nkidslt6                      -0.262***         \n                              (0.034)          \n                                               \nkidsge6                        0.013           \n                              (0.013)          \n                                               \nConstant                     0.586***          \n                              (0.154)          \n                                               \n-----------------------------------------------\nObservations                    753            \nR2                             0.264           \nAdjusted R2                    0.257           \nResidual Std. Error      0.427 (df = 745)      \nF Statistic           38.218*** (df = 7; 745)  \n===============================================\nNote:               *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n### Python\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\ndata = pd.read_stata(\"files/mroz.dta\")\nformula = \"inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6\"\nmodel = smf.ols(formula, data=data).fit()\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   inlf   R-squared:                       0.264\nModel:                            OLS   Adj. R-squared:                  0.257\nMethod:                 Least Squares   F-statistic:                     38.22\nDate:                qua, 31 jan 2024   Prob (F-statistic):           6.90e-46\nTime:                        10:22:57   Log-Likelihood:                -423.89\nNo. Observations:                 753   AIC:                             863.8\nDf Residuals:                     745   BIC:                             900.8\nDf Model:                           7                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.5855      0.154      3.798      0.000       0.283       0.888\nnwifeinc      -0.0034      0.001     -2.351      0.019      -0.006      -0.001\neduc           0.0380      0.007      5.151      0.000       0.024       0.052\nexper          0.0395      0.006      6.962      0.000       0.028       0.051\nexpersq       -0.0006      0.000     -3.227      0.001      -0.001      -0.000\nage           -0.0161      0.002     -6.476      0.000      -0.021      -0.011\nkidslt6       -0.2618      0.034     -7.814      0.000      -0.328      -0.196\nkidsge6        0.0130      0.013      0.986      0.324      -0.013       0.039\n==============================================================================\nOmnibus:                      169.137   Durbin-Watson:                   0.494\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               36.741\nSkew:                          -0.196   Prob(JB):                     1.05e-08\nKurtosis:                       1.991   Cond. No.                     3.06e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.06e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n\n\n:::\n:::\n\n\n\n### Stata\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse \"files/mroz.dta\" , replace\n\neststo: qui reg inlf nwifeinc educ exper expersq age kidslt6 kidsge6\nesttab\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(est1 stored)\n\n\n----------------------------\n                      (1)   \n                     inlf   \n----------------------------\nnwifeinc         -0.00341*  \n                  (-2.35)   \n\neduc               0.0380***\n                   (5.15)   \n\nexper              0.0395***\n                   (6.96)   \n\nexpersq         -0.000596** \n                  (-3.23)   \n\nage               -0.0161***\n                  (-6.48)   \n\nkidslt6            -0.262***\n                  (-7.81)   \n\nkidsge6            0.0130   \n                   (0.99)   \n\n_cons               0.586***\n                   (3.80)   \n----------------------------\nN                     753   \n----------------------------\nt statistics in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n# Logit and Probit {.smaller background=\"#ebe0ae\" }\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\nAlthough the linear probability model is simple to estimate and use, it has some limitations as discussed.\n\nIf that problem is important to us, we need a solution that addresses the problem of  negative or higher than 1 probability. \n\nThat is, **we need a binary response model**.\n\n. . .\n\nIn a binary response model, interest relies on the response probability.\n\n$$P(y =1 | x) = P(y=1| x_1,x_2,x_3,...)$$\n\nThat is, we have a group of X variables explaining Y, which is binary. In a LPM, we assume that the response probability is linear in the parameters $\\beta$. \n\n- This is the assumption that created the problem discussed above.\n\n\n\n\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\nWe can change that assumption to a different function. \n\n- A **logit** model assumes a logistic function ($G(Z)=\\frac{exp(z)}{[1+exp(z)]}$)\n- A **probit** model assumes a standard normal cumulative distribution function ($\\int_{-inf}^{+z}\\phi(v)dv$)\n\n. . . \n\nThe adjustment is something as follows.\n\n$$P(y =1 | x) = G(\\beta_0 + \\beta_1 x_1+ \\beta_2 x_2 + \\beta_3 x_3)$$\n\nWhere G is either the logistic (logit) or the normal (probit) function. \n\n\nWe don't need to memorize the functions, but we need to understand the adjustment that assuming a different function makes. \n\nBasically, we will not have predicted negative values anymore because the function adjusts at very low and very high values. \n\n\n\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\n![](figs/wooldridge_17_1.png)\n\n\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\n\n\n::: panel-tabset\n### R\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\ndata <- read_dta(\"files/mroz.dta\")\nlpm <- lm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data )\nlogit <- glm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data ,family = binomial)\nprobit <- glm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data , family = binomial(link = probit))\nstargazer(lpm , logit, probit,title = \"Regression Results\", type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression Results\n===============================================================\n                                Dependent variable:            \n                    -------------------------------------------\n                                       inlf                    \n                              OLS           logistic   probit  \n                              (1)              (2)       (3)   \n---------------------------------------------------------------\nnwifeinc                   -0.003**         -0.021**  -0.012** \n                            (0.001)          (0.008)   (0.005) \n                                                               \neduc                       0.038***         0.221***  0.131*** \n                            (0.007)          (0.043)   (0.025) \n                                                               \nexper                      0.039***         0.206***  0.123*** \n                            (0.006)          (0.032)   (0.019) \n                                                               \nexpersq                    -0.001***        -0.003*** -0.002***\n                           (0.0002)          (0.001)   (0.001) \n                                                               \nage                        -0.016***        -0.088*** -0.053***\n                            (0.002)          (0.015)   (0.008) \n                                                               \nkidslt6                    -0.262***        -1.443*** -0.868***\n                            (0.034)          (0.204)   (0.118) \n                                                               \nkidsge6                      0.013            0.060     0.036  \n                            (0.013)          (0.075)   (0.044) \n                                                               \nConstant                   0.586***           0.425     0.270  \n                            (0.154)          (0.860)   (0.508) \n                                                               \n---------------------------------------------------------------\nObservations                  753              753       753   \nR2                           0.264                             \nAdjusted R2                  0.257                             \nLog Likelihood                              -401.765  -401.302 \nAkaike Inf. Crit.                            819.530   818.604 \nResidual Std. Error    0.427 (df = 745)                        \nF Statistic         38.218*** (df = 7; 745)                    \n===============================================================\nNote:                               *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n### Python\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.iolib.summary2 import summary_col\n\ndata = pd.read_stata(\"files/mroz.dta\")\nlpm_model = smf.ols(\"inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6\", data=data).fit(disp=False)\nlogit_model = smf.logit(\"inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6\", data=data).fit(disp=False)\nprobit_model = smf.probit(\"inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6\", data=data).fit(disp=False)\nsummary = summary_col([lpm_model, logit_model,probit_model], stars=True)\nprint(summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n===============================================\n                 inlf I    inlf II    inlf III \n-----------------------------------------------\nIntercept      0.5855***  0.4255     0.2701    \n               (0.1542)   (0.8604)   (0.5086)  \nR-squared      0.2642                          \nR-squared Adj. 0.2573                          \nage            -0.0161*** -0.0880*** -0.0529***\n               (0.0025)   (0.0146)   (0.0085)  \neduc           0.0380***  0.2212***  0.1309*** \n               (0.0074)   (0.0434)   (0.0253)  \nexper          0.0395***  0.2059***  0.1233*** \n               (0.0057)   (0.0321)   (0.0187)  \nexpersq        -0.0006*** -0.0032*** -0.0019***\n               (0.0002)   (0.0010)   (0.0006)  \nkidsge6        0.0130     0.0601     0.0360    \n               (0.0132)   (0.0748)   (0.0435)  \nkidslt6        -0.2618*** -1.4434*** -0.8683***\n               (0.0335)   (0.2036)   (0.1185)  \nnwifeinc       -0.0034**  -0.0213**  -0.0120** \n               (0.0014)   (0.0084)   (0.0048)  \n===============================================\nStandard errors in parentheses.\n* p<.1, ** p<.05, ***p<.01\n```\n\n\n:::\n:::\n\n\n\n### Stata\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse \"files/mroz.dta\" , replace\neststo: qui regress inlf nwifeinc educ exper expersq age kidslt6 kidsge6\neststo: qui logit inlf nwifeinc educ exper expersq age kidslt6 kidsge6\neststo: qui probit inlf nwifeinc educ exper expersq age kidslt6 kidsge6\nesttab\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(est1 stored)\n\n(est2 stored)\n\n(est3 stored)\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                     inlf            inlf            inlf   \n------------------------------------------------------------\nmain                                                        \nnwifeinc         -0.00341*        -0.0213*        -0.0120*  \n                  (-2.35)         (-2.53)         (-2.48)   \n\neduc               0.0380***        0.221***        0.131***\n                   (5.15)          (5.09)          (5.18)   \n\nexper              0.0395***        0.206***        0.123***\n                   (6.96)          (6.42)          (6.59)   \n\nexpersq         -0.000596**      -0.00315**      -0.00189** \n                  (-3.23)         (-3.10)         (-3.15)   \n\nage               -0.0161***      -0.0880***      -0.0529***\n                  (-6.48)         (-6.04)         (-6.23)   \n\nkidslt6            -0.262***       -1.443***       -0.868***\n                  (-7.81)         (-7.09)         (-7.33)   \n\nkidsge6            0.0130          0.0601          0.0360   \n                   (0.99)          (0.80)          (0.83)   \n\n_cons               0.586***        0.425           0.270   \n                   (3.80)          (0.49)          (0.53)   \n------------------------------------------------------------\nN                     753             753             753   \n------------------------------------------------------------\nt statistics in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\nImportantly, in a LPM model, the coefficients have similar interpretations as usual. \n\nBut logit and probit models lead to harder to interpret coefficients. \n\nIn fact, often we do not make any interpretation of these coefficients. \n\nInstead, we usually transform them to arrive at an interpretation that is similar to what we have in LPM. \n\nTo make the magnitudes of probit and logit roughly comparable, we can multiply the probit coefficients by 1.6, or we can multiply the logit estimates by .625. \n\nAlso, the probit slope estimates can be divided by 2.5 to make them comparable to the LPM estimates.\n\nAfter these adjustments, the interpretation of the logit and probit outputs are similar to LPM's.  \n\n\n\n\n\n\n# Tobit {.smaller background=\"#ae83f2\" }\n\n\n\n## Tobit {.smaller background=\"#ae83f2\"}\n\nAnother problem in the dependent variable occurs when we have a **limited dependent variable** with a corner solution. \n\nThat is, a variable that ranges from zero to all positive values. \n\nFor instance, hours working. \n\n- Nobody works less than zero hours, but individuals in the population can work many number of positive hours. \n\nWhen we have such type of dependent variable, we need to estimate a **tobit** model.\n\n\n\n\n## Tobit {.smaller background=\"#ae83f2\"}\n\n**Tobit can make a huge difference to the LPM model.**\n\n::: panel-tabset\n### R\n\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\nlibrary(AER)\ndata <- read_dta(\"files/mroz.dta\")\nlpm <- lm(hours ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = data)\ntobit <- tobit(hours ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = data)\nstargazer(lpm , tobit ,title = \"Regression Results\", type = \"text\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression Results\n===============================================================\n                                Dependent variable:            \n                    -------------------------------------------\n                                       hours                   \n                              OLS                  Tobit       \n                              (1)                   (2)        \n---------------------------------------------------------------\nnwifeinc                    -3.447               -8.814**      \n                            (2.544)               (4.459)      \n                                                               \neduc                       28.761**              80.646***     \n                           (12.955)              (21.583)      \n                                                               \nexper                      65.673***            131.564***     \n                            (9.963)              (17.279)      \n                                                               \nexpersq                    -0.700**              -1.864***     \n                            (0.325)               (0.538)      \n                                                               \nage                       -30.512***            -54.405***     \n                            (4.364)               (7.419)      \n                                                               \nkidslt6                   -442.090***           -894.022***    \n                           (58.847)              (111.878)     \n                                                               \nkidsge6                     -32.779               -16.218      \n                           (23.176)              (38.641)      \n                                                               \nConstant                 1,330.482***            965.305**     \n                           (270.785)             (446.436)     \n                                                               \n---------------------------------------------------------------\nObservations                  753                   753        \nR2                           0.266                             \nAdjusted R2                  0.259                             \nLog Likelihood                                  -3,819.095     \nResidual Std. Error   750.179 (df = 745)                       \nF Statistic         38.495*** (df = 7; 745)                    \nWald Test                                   253.862*** (df = 7)\n===============================================================\nNote:                               *p<0.1; **p<0.05; ***p<0.01\n```\n\n\n:::\n:::\n\n\n\n\n\n### Python\n\n\n### Stata\n\n\n\n:::\n\n\n\n\n\n\n\n# Economic vs statistical significance {.smaller background=\"#a1f7e3\"}\n\n\n## Economic vs statistical significance {.smaller background=\"#a1f7e3\" }\n\n**Economic is not the same as statistical significance!**\n\n- coefficients may be significantly different from 0 from a statistical perspective. \n\n- but their economic significance may be very small. \n\n- Conversely, coefficients might be economically large but with no statistical significance. \n\n\n\n\n\n## Economic vs statistical significance {.smaller background=\"#a1f7e3\" }\n\nThere is considerable variation in this regard in empirical research.\n\n**But we should always try to show economic significance alongside the statistical significance.** \n\n- How large is the variation in $y$ given a unit variation in $x$? \n\n- Try to put that on economic figures. A thousand? A million?\n\n- Perhaps more importantly, are these figures realistic? \n\n  - e.g., How realistic is a predicted increase in salary of 1 Million?\n\n\n\n\n## THANK YOU!\n\n::: columns\n::: {.column width=\"30%\"}\n![](figs/fgv.png){fig-align=\"right\"}\n:::\n\n::: {.column width=\"70%\"}\n**Henrique Castro Martins**\n\n-   [henrique.martins\\@fgv.br](henrique.martins@fgv.br)\n-   <https://eaesp.fgv.br/en/people/henrique-castro-martins>\n-   [henriquemartins.net](https://henriquemartins.net/)\n-   <https://www.linkedin.com/in/henriquecastror/>\n:::\n:::\n",
    "supporting": [
      "part_6_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}