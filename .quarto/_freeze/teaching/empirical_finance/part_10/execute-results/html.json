{
  "hash": "79a747312ccdbf7e817bd42690ce8a87",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Empirical Methods in Finance'\nsubtitle: 'Part 10'\nauthor: 'Henrique C. Martins'\nformat:\n  revealjs: \n    slide-number: true\n    theme: simple\n    chalkboard: true\n    preview-links: auto\n    logo: figs/background8.png\n    css: logo.css\n    footer: '**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  '\n    multiplex: true\n    scrollable: true \ntitle-slide-attributes:\n    data-background-color: \"#b1cafa\"\ninclude-after: |\n  <script type=\"text/javascript\">\n    Reveal.on('ready', event => {\n      if (event.indexh === 0) {\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n      }\n    });\n    Reveal.addEventListener('slidechanged', (event) => {\n      if (event.indexh === 0) {\n        Reveal.configure({ slideNumber: null });\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n      }\n      if (event.indexh === 1) { \n        Reveal.configure({ slideNumber: 'c' });\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n      }\n    });\n  </script>\n\n---\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Introduction IV {.smaller background=\"#bcd3f7\"}\n\n## Introduction IV {.smaller background=\"#bcd3f7\"}\n\nImagine the following model\n\n$$Ln(wage)=\\alpha + \\beta_1 educ + \\epsilon$$\n\nWe can infer that *educ* is correlated with *ability*, but the latter is in the error term. \n\n*educ* in this case is \"endogenous\".\n\n$$Cov(educ, \\epsilon) \\neq 0$$\n\n\n\n## Introduction IV {.smaller background=\"#bcd3f7\"}\n\n**The setup of an IV is**\n\nSuppose that we have an observable variable z that satisfies these two assumptions: \n\n(1) z is uncorrelated with u:\n\n$$Cov(z, \\epsilon) = 0$$\n\n\n(2) z is correlated with x:\n\n$$Cov(z, x) \\neq 0$$\n\n\n\nThen, we call z an instrumental variable for x, or sometimes simply an instrument for x.\n\n\n\n\n\n## Introduction IV {.smaller background=\"#bcd3f7\"}\n\nBefore we continue,\n\nIV is not a model, it is an **estimation method** \n\nI'll call it a **Design**.\n\n\nDo not say, I estimated an IV model (more often than it should be).\n\n\n\n\n\n\n\n\n\n\n# Instrumental Variables {.smaller background=\"#8eadde\"}\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\nImagine that you have one independent variable that is \"endogenous\":\n\n- $Cov(x_k,\\mu)\\neq 0$\n\n- You may have many other independent variables not \"endogenous\"\n\n. . .\n\nIn this situation:\n\n- $B_k$ is biased\n\n- The other betas will likely be biased as well, since it is unlikely that all other Xs are not correlated with $x_k$\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n$x_k$ is the endogenous variable.\n\n- It has \"good\" variation: \n\n  - the part that varies that is not correlated with $\\mu$\n\n- It has \"bad\" variation: \n\n  - the part that varies that is  correlated with $\\mu$\n\n. . . \n\nLet's assume now that you can find an instrument $z$\n\n- The instrument $z$ is correlated with $X_k$, but only the \"good\" variation, not the \"bad\".\n\n- The instrument $z$ does not explain $y$ directly, only through $x_k$.\n\n  - **Only through** condition.\n\n\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n**Relevance Condition**\n\nThe instrument $z$ is correlated with $x_k$.\n\n- This assumption is easy to test. Simply run a regression of $x=f(z, all\\; Xs)$ and check the significance of the $\\beta_z$.\n\n- This is called the **first stage of an IV regression**\n\n  - **Tip**: Always show the beta coefficient and the R2 (even if low) of the first-stage.  \n\n. . . \n\n**Exclusion Condition**\n\nThe instrument $z$ is not correlated with $\\mu$.\n\n- That is $cov(z,\\mu) = 0$\n\n- As all correlations with $\\mu$, you cannot test this prediction. You have to rely on the theory, create a story about that.\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n**An example of IV** [Murray](https://doi.org/10.1257/jep.20.4.111).\n\n![](figs/iv2.png)\n\n\n\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n**An example of IV** [Murray](http://dx.doi.org/10.1016/j.jcorpfin.2013.12.013).\n\n![](figs/iv6.png)\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#good-instruments-should-feel-weird)\n\n**Good instruments should feel weird**\n\nParents with two same-gender kids are more likely to try a third kid than a diverse-gender pair of parents.\n\nSo, you may use the gender of the kids as instrument for the likelihood of the mother go back to the labor market.\n\n\n\n\n\n\n# Angrist's example {.smaller background=\"#d2e2fc\"}\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\nRemember the **Fuzzy RDD**.\n\n- There is the *treatment*\n- There is the *position* (before or after the cut) \n\nThe *position* is an indication of receiving or not the treatment, but it is not definitive.\n\nThus, we can use the *position* as an IV for the treatment.\n\n\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#the-problem-of-weak-instruments)\n\n*One of the more seminal papers in instrumental variables for the modern period is Angrist and Krueger (1991).* \n\n*Their idea is simple and clever; a quirk in the United States educational system is that a child enters a grade on the basis of his or her birthday.* \n\n*For a long time, that cutoff was late December. If children were born on or before December 31, then they were assigned to the first grade. But if their birthday was on or after January 1, they were assigned to kindergarten. *\n\n*Thus two people—one born on December 31 and one born on January 1—were exogenously assigned different grades.*\n\nEveryone is forced to leave school when 16.\n\n\n\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#the-problem-of-weak-instruments)\n\n![](figs/iv3.png)\n\n*Angrist and Krueger had the insight that that small quirk was exogenously assigning more schooling to people born later in the year.*\n\n*The person born in December would reach age 16 with more education than the person born in January*\n\n\n\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#the-problem-of-weak-instruments)\n\n![](figs/iv4.jpg)\n\n\n**What is the instrument here?**\n\n\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#the-problem-of-weak-instruments)\n\n\n**What is the instrument here?**\n\nThe instrument is the quarter of birth.\n\n- People born in the 3rd and 4th quarter receive more education than others due to **compulsory schooling**.\n\n\n\n\n\n\n\n\n# Two-stage least squares (2SLS) {.smaller background=\"#abc8f5\"}\n\n## Two-stage least squares  (2SLS){.smaller background=\"#abc8f5\"}\n\n\nOne of the more intuitive instrumental variables estimators is the 2SLS. \n\n\n\n**The first stage is**\n\n$$x_k = \\delta + \\delta_1 z + \\delta_2 x_1 + . . .+ \\delta_n x_n + \\mu$$\n\nThen, you predict $x_k$ using the first stage.\n\n- \"predict\" means that you are finding the \"response\" Y of the equation after estimating the coefficients\n\n $$\\hat{x_k} = \\hat{\\delta} + \\hat{\\delta_1} z + \\hat{\\delta_2} x_1 + . . . + \\hat{\\delta_n} x_n $$\n\n\n. . . \n\nThen, **the second stage** is:\n\n\n$$y = \\alpha + \\beta_1 \\hat{x_k} + \\beta_2 x_1 + . . .+  \\beta_n x_n + \\mu$$\n\n\nThe idea using $\\hat{x_k}$ is that it represents only the variation that is not correlated with $\\mu$.\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#abc8f5\"}\n\nWe can write that:\n\n$$\\beta_1= \\frac{Cov(z,y)}{Cov(z,x_k)}$$\n\n\n- It shows that $\\beta_1$ is the population covariance between z and y divided by the population covariance between z and x.\n\n- Notice how this fails if z and x are uncorrelated, that is, if $Cov(z, x) = 0$ \n\n\n\n\n. . . \n\n\n\n$$\\beta_1= \\frac{\\sum_{i=1}^n(z_i-\\bar{z})(y_i-\\bar{y})}{\\sum_{i=1}^n(z_i-\\bar{z})(x_i-\\bar{x})}$$\n\n\nNotice that if $z$ and $x$ are the same (i.e., perfect correlation), $\\beta_1$ above is the OLS $\\beta$\n\n\n$$\\beta_1= \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$$\n\n\n\n# Weak Instruments  {.smaller background=\"#8da6cc\"}\n\n## Weak Instruments {.smaller background=\"#8da6cc\"}\n\nI did not demonstrate here, but we can say that (see pp; 517-18 Wooldridge):\n\n- though IV is consistent when $z$ and $u$ are uncorrelated and $z$ and $x$ have any positive or negative correlation, IV estimates can have large standard errors, especially if $z$ and $x$ are only weakly correlated.\n\nThis gives rise to the week instruments problem.\n\nWe can write the probability limit of the IV estimator:\n\n$$plim \\hat{\\beta_{iv}} = \\beta_1 + \\frac{Corr(z,\\mu)}{Corr(z,x)} \\times \\frac{\\sigma_{\\mu}}{\\sigma_x}$$\n\nIt shows that, even if $Corr(z,\\mu)$ is small, the inconsistency in the IV estimator can be very large if $Corr(z,x)$ is also small.\n\n\n\n\n\n\n\n## Weak Instruments {.smaller background=\"#8da6cc\"}\n\nTwo tips:\n\n1) Look at the F-stat of the first-stage. It should not be low.\n\n2) The sign of the coefficient in the first stage should be as expected.\n\n3) Look at the S.E. When the instrument is week, the S.E., are even larger than it should be.\n\n. . .\n\nOne more tip,\n\n4) Avoid computing the first stage by hand, it would give wrong estimates of the S.E. in the second stage.\n\n\n\n\n\n\n\n\n\n\n\n# Example {.smaller background=\"#95a9c7\"}\n\n## Example  {.smaller background=\"#95a9c7\"}\n\nOLS\n \n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nlibrary(haven)  \ndata <- read_dta(\"files/mroz.dta\")\nmodel <- lm(lwage ~ educ, data = data)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = lwage ~ educ, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.10256 -0.31473  0.06434  0.40081  2.10029 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.1852     0.1852  -1.000    0.318    \neduc          0.1086     0.0144   7.545 2.76e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.68 on 426 degrees of freedom\n  (325 observations deleted due to missingness)\nMultiple R-squared:  0.1179,\tAdjusted R-squared:  0.1158 \nF-statistic: 56.93 on 1 and 426 DF,  p-value: 2.761e-13\n```\n\n\n:::\n:::\n\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse files/mroz.dta , clear\nreg lwage educ \n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Source |       SS           df       MS      Number of obs   =       428\n-------------+----------------------------------   F(1, 426)       =     56.93\n       Model |  26.3264237         1  26.3264237   Prob > F        =    0.0000\n    Residual |  197.001028       426  .462443727   R-squared       =    0.1179\n-------------+----------------------------------   Adj R-squared   =    0.1158\n       Total |  223.327451       427  .523015108   Root MSE        =    .68003\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .1086487   .0143998     7.55   0.000     .0803451    .1369523\n       _cons |  -.1851969   .1852259    -1.00   0.318    -.5492674    .1788735\n------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n\n\n\n\n\n## Example  {.smaller background=\"#95a9c7\"}\n\nThe IV estimate of the return to education is 5.9%, which is barely more than one-half of the  OLS estimate. This suggests that the OLS estimate is too high and is consistent with omitted ability bias.\n\n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nlibrary(haven)  \nlibrary(AER)    # For IV regression\ndata <- read_dta(\"files/mroz.dta\")\nmodel_iv <- ivreg(lwage ~ educ | fatheduc, data = data)\nsummary(model_iv, diagnostics = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nivreg(formula = lwage ~ educ | fatheduc, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0870 -0.3393  0.0525  0.4042  2.0677 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.44110    0.44610   0.989   0.3233  \neduc         0.05917    0.03514   1.684   0.0929 .\n\nDiagnostic tests:\n                 df1 df2 statistic p-value    \nWeak instruments   1 426     88.84  <2e-16 ***\nWu-Hausman         1 425      2.47   0.117    \nSargan             0  NA        NA      NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6894 on 426 degrees of freedom\nMultiple R-Squared: 0.09344,\tAdjusted R-squared: 0.09131 \nWald test: 2.835 on 1 and 426 DF,  p-value: 0.09294 \n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse files/mroz.dta , clear\nivreg lwage (educ =fatheduc ) , first\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFirst-stage regressions\n-----------------------\n\n      Source |       SS           df       MS      Number of obs   =       428\n-------------+----------------------------------   F(1, 426)       =     88.84\n       Model |  384.841983         1  384.841983   Prob > F        =    0.0000\n    Residual |  1845.35428       426  4.33181756   R-squared       =    0.1726\n-------------+----------------------------------   Adj R-squared   =    0.1706\n       Total |  2230.19626       427  5.22294206   Root MSE        =    2.0813\n\n------------------------------------------------------------------------------\n        educ | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    fatheduc |   .2694416   .0285863     9.43   0.000     .2132538    .3256295\n       _cons |   10.23705   .2759363    37.10   0.000     9.694685    10.77942\n------------------------------------------------------------------------------\n\n\nInstrumental variables 2SLS regression\n\n      Source |       SS           df       MS      Number of obs   =       428\n-------------+----------------------------------   F(1, 426)       =      2.84\n       Model |  20.8673618         1  20.8673618   Prob > F        =    0.0929\n    Residual |  202.460089       426  .475258426   R-squared       =    0.0934\n-------------+----------------------------------   Adj R-squared   =    0.0913\n       Total |  223.327451       427  .523015108   Root MSE        =    .68939\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0591735   .0351418     1.68   0.093    -.0098994    .1282463\n       _cons |   .4411035   .4461018     0.99   0.323    -.4357311    1.317938\n------------------------------------------------------------------------------\nInstrumented: educ\n Instruments: fatheduc\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Final Comments {.smaller background=\"#bcd3f7\"}\n\n## Final Comments {.smaller background=\"#bcd3f7\"}\n\nWhen you have only one Z, you can say that the model is **just identified**.\n\nBut you can have multiple IVs, in which case you will say that the model is **overidentified**.\n\n- You can implement IV design just as before\n\n- The relevance and exclusion assumptions are there as well.\n\n. . . \n\nAssuming that both conditions are satisfied, you will have more asymptotic efficiency in the IV estimates.\n\n\n\n\n\n\n## Final Comments {.smaller background=\"#bcd3f7\"}\n\n**It is rare to have multiple Zs. You should be happy if you have a good one!**\n\n. . . \n\n\nBut if you do have multiple IVs, you can test their quality...\n\n- If they are all valid, you should get consistent estimates...\n\n- ... even if you use only a subset of them.\n\n- So the test is about how similar the estimates are if you use subsets of IVs.\n\n\n**But this test does not give really an answer about whether the IVs are good.**\n\nThis always come from theory.\n\n\n\n\n## Final Comments {.smaller background=\"#bcd3f7\"}\n\n**Some more comments:**\n\n1) If you have an interaction term between $x_1$ and $x_2$, and $z$ is the instrument for $x_1$, you can \"create\" the instrument $zx_2$ for $x_2$.\n\n. . . \n\n2) GMM uses lagged variables as instruments. But, this is not a good decision if the variables are highly serially correlated.\n\n  - Lagged total assets is not a good instrument for total assets.\n  \n. . . \n\n3) Using the average-group of variable X is also problematic (i.e., the industry average own. concentration as IV of firm-level own. concentration)\n\n - This is no different than a group FE, making hard to believe in the exclusion restriction.\n\n\n\n\n\n## **THANK YOU!** {background=\"#b1cafa\"}\n\n::: columns\n::: {.column width=\"60%\"}\n**QUESTIONS?**\n\n![](figs/qa2.png){width=\"150%\" heigth=\"150%\"}\n:::\n\n::: {.column width=\"40%\"}\n**Henrique C. Martins**\n\n-   [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)\n-   [Personal Website](https://henriquemartins.net/)\n-   [LinkedIn](https://www.linkedin.com/in/henriquecastror/)\n-   [Lattes](http://lattes.cnpq.br/6076997472159785)\n-   [Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)\\\n:::\n:::\n\n::: footer\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}