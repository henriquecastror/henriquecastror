{
  "hash": "da699875fe66df5973cbb44636ea6eda",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Empirical Methods in Finance'\nsubtitle: 'Part 1'\nauthor: 'Henrique C. Martins'\nformat:\n  revealjs: \n    slide-number: true\n    theme: simple\n    chalkboard: true\n    preview-links: auto\n    logo: figs/background8.png\n    css: logo.css\n    footer: '**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  '\n    multiplex: true\n    scrollable: true \ntitle-slide-attributes:\n    data-background-color: \"#b1cafa\"\ninclude-after: |\n  <script type=\"text/javascript\">\n    Reveal.on('ready', event => {\n      if (event.indexh === 0) {\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n      }\n    });\n    Reveal.addEventListener('slidechanged', (event) => {\n      if (event.indexh === 0) {\n        Reveal.configure({ slideNumber: null });\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n      }\n      if (event.indexh === 1) { \n        Reveal.configure({ slideNumber: 'c' });\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n      }\n    });\n  </script>\n\n---\n\n\n\n\n\n# Agenda\n\n## Agenda {.smaller}\n\n- Apresenta√ß√£o do syllabus do curso\n  - Apresenta√ß√£o dos crit√©rios de avalia√ß√£o\n  \n. . .\n\n- Breve apresenta√ß√£o dos temas de pesquisa e discuss√£o inicial sobre a entrega final\n\n. . .\n\n- In√≠cio do conte√∫do\n  - Introdu√ß√£o a causalidade\n\n\n\n\n\n\n\n\n\n## Sobre a letter  {.smaller visibility=\"hidden\"}\n\n- Formato letter\n  - Entre 2k e 2.5k palavras a depender do journal.\n  \n*The objective of a letter is to facilitate the rapid dissemination of important research that contains an insight, new data, or discuss current important topic.*\n  \n- Ir√° requerer todas as etapas da pesquisa (com √™nfase na an√°lise dos dados, i.e., regress√µes).\n\n- Idealmente, ser√° submetida com o/a orientador/a. Leia-se, sua miss√£o √© \"convencer\" de que o trabalho final √© submet√≠vel a uma revista. \n\n\n\n\n\n\n\n\n\n## Sobre a letter  {.smaller visibility=\"hidden\"}\n\n- Op√ß√µes de revistas que aceitam letter (checar se refer√™ncias e tabelas fazem parte do word count):\n\n  - [Economic Letters](https://www.sciencedirect.com/journal/economics-letters) (ABS3): 2k palavras\n  - [Journal of Accounting and Public Policy](https://www.sciencedirect.com/journal/journal-of-accounting-and-public-policy) (ABS3): 3k palavras\n  - [Finance Research Letters](https://www.sciencedirect.com/journal/finance-research-letters) (ABS2): 2.5k palavras\n  - [Applied Economic Letters](https://www.tandfonline.com/journals/rael20) (ABS1): 2k palavras\n  - [Brazilian Review of Finance](https://periodicos.fgv.br/rbfin) (A4): [4k palavras](https://periodicos.fgv.br/rbfin/libraryFiles/downloadPublic/140) \n  \n* Voc√™ √© bem-vindo/a para propor outro journal que aceite letter, sob condi√ß√£o de valida√ß√£o junto ao instrutor. \n  \n\n\n\n\n\n## Stata {.smaller}\n\n**Providenciar instala√ß√£o para pr√≥ximo encontro**.\n\nPara instala√ß√£o do Stata, seguir instru√ß√µes da TI. \n\n\n\n\n\n\n\n## R {.smaller}\n\n**Providenciar instala√ß√£o para pr√≥ximo encontro**.\n\n\nInstall R [here Win](https://cran.r-project.org/bin/windows/base/)\n\nInstall R [here Mac](https://cran.r-project.org/bin/macosx/)\n\nInstall R Studio [here](https://posit.co/download/rstudio-desktop/)\n\n. . .\n\nPara instalar e carregar os pacotes voc√™ precisa rodar as duas linhas abaixo.\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"R\" code-line-numbers=\"true\"}\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n```\n:::\n\n\n\n\n## Python {.smaller}\n\n**I might show some code in python, but I cannot offer you support on it.**\n\n\n\n\n\n\n# Selection bias  {.smaller background=\"#fadea7\"} \n\n##  {.smaller background=\"#fadea7\"} \n\n\n![](figs/slides4-airplane.png)\n\n\n\n\n\n\n##  {.smaller background=\"#fadea7\"} \n\n![](figs/slides4-path1.jpg)\n\n\n**Voc√™ nunca sabe o resultado do caminho que n√£o toma.**\n\n\n\n\n\n\n\n\n\n\n\n\n## Quais as aplica√ß√µes do que vamos discutir? {.smaller background=\"#fadea7\"} \n\nH√° uma s√©rie de **quest√µes de pesquisa** que poderiam ser investigadas com as ferramentas que vamos discutir hoje.\n\n::: incremental\n\n1) Vale mais a pena estudar em escola particular ou p√∫blica?\n\n2) Qual o efeito de investimentos de marketing t√™m na lucratividade?\n\n3) Qual o efeito que jornadas de 4 dias semanais t√™m na produtividade?\n\n4) Qual efeito que educa√ß√£o tem na remunera√ß√£o futura?\n\n5) E diversas outras semelhantes...\n\n:::\n\n\n\n\n\n\n## Antes de come√ßar: Nossa agenda {.smaller background=\"#fadea7\"} \n\n\n::: incremental \n\n1) Introdu√ß√£o a **pesquisa quantitativa**\n\n2) Validade **Externa** vs. Validade **Interna**\n\n3) **Problemas** em pesquisa quantitativa inferencial\n\n4) **Rem√©dios**\n\n:::\n\n\n\n\n\n\n        \n\n\n## Introdu√ß√£o {.smaller background=\"#fadea7\"} \n\n**O que fazemos em pesquisa quantitiva?** Seguimos o m√©todo de pesquisa tradicional (com ajustes):\n\n::: incremental\n\n- Observa√ß√£o \n\n- Quest√£o de pesquisa \n\n- Modelo te√≥rico (abstrato)\n\n- Hip√≥teses\n\n- Modelo emp√≠rico\n\n- Coleta de dados \n\n- An√°lise do resultado do modelo (diferente de an√°lise de dados \"pura\")\n\n- Conclus√£o/desdobramentos/aprendizados\n  \n:::\n\n\n\n\n\n\n\n\n\n## Introdu√ß√£o {.smaller background=\"#fadea7\"} \n\n**O que fazemos em pesquisa quantitiva?** Seguimos o m√©todo de pesquisa tradicional (com ajustes):\n\n\n- Observa√ß√£o \n\n- Quest√£o de pesquisa \n\n- Modelo te√≥rico (abstrato): **Aqui √© onde a matem√°tica √© necess√°ria**\n\n- Hip√≥teses\n\n- Modelo emp√≠rico: **Estat√≠stica e econometria necess√°rias**\n\n- Coleta de dados: **Geralmente secund√°rios**\n\n- An√°lise do resultado do modelo (diferente de an√°lise de dados \"pura\")\n\n- Conclus√£o/desdobramentos/aprendizados\n\n\n\n\n\n\n\n. . .\n\n## Defini√ß√£o {.smaller background=\"#fadea7\"} \n\n**_Pesquisa quantitativa busca testar hip√≥teses..._**\n\n. . .\n\n**_...a partir da defini√ß√£o de modelos formais (abstratos)..._**\n\n. . .\n\n**_...de onde se estimam modelos emp√≠ricos utilizando a estat√≠stica e a econometria como mecanismos/instrumentos._**\n\n. . .\n\n\nNo fim do dia, buscamos **entender as rela√ß√µes** (que tenham **validade interna** e que ofere√ßam **validade externa**) entre diferentes **vari√°veis de interesse.**\n\n\n\n\n\n\n\n\n\n\n\n## Quais as vantagens? {.smaller background=\"#fadea7\"} \n\n1) **Validade externa:** \n\n. . .\n\n- Conceito de que, se a pesquisa tem validade externa, os seus **achados s√£o representativos**.\n\n. . .\n\n- I.e., s√£o **v√°lidos al√©m do seu modelo**. Resultados \"valem externamente\".\n\n. . .\n\n- Idealmente, buscamos resultados que valem externamente para **acumular conhecimento**...\n\n. . .\n\n- ...naturalmente, nem toda pesquisa quantitativa oferece validade externa. A pesquisa √≥tima sim. **A pesquisa excelente tem validade externa para al√©m do seu tempo**.\n\n. . .\n\n- Pesquisa qualitativa dificilmente oferece **validade externa**.\n\n\n\n\n\n\n\n\n\n\n## Quais as armadilhas? {.smaller background=\"#fadea7\"} \n\n\n2) **Validade interna:** \n\n. . .\n\n- Conceito de que a pesquisa precisa de validade interna para que seus **resultados sejam cr√≠veis**.\n\n. . .\n\n- I.e., os **resultados n√£o podem conter erros**, vieses, problemas de estima√ß√£o, problemas nos dados, etc..\n\n. . .\n\n- √â aqui que a gente separa a pesquisa ruim da pesquisa boa. Para ser levada a s√©rio, a pesquisa **PRECISA** ter validade interna.\n\n. . .\n\n- Mas isso, nem sempre √© trivial. Muitas pesquisas que vemos publicadas, mesmo em top journals, **n√£o t√™m validade interna** (seja por erro do pesquisador, por m√©todo incorreto, por falta de dados...)\n\n. . .\n\n- Mas cada vez mais, **avaliadores est√£o de olho** em problemas e em modelos  **Trash-in-Trash-out**\n\n\n\n\n\n\n\n\n\n\n\n## Como fazemos na pr√°tica? {.smaller background=\"#fadea7\"} \n\nExemplo de modelo emp√≠rico:\n\n$Y_{i} = Œ± + ùú∑_{1} √ó X_i + Controls + error$\n\n. . .\n\n<img src=\"figs/slides4-ols.jpg\" width=\"30%\" align=\"right\" />\n\n. . .\n\nUma vez que estimemos esse modelo, temos o **valor**, o **sinal** e a **signific√¢ncia** do $ùú∑$.\n\n. . .\n\nSe o Beta for **significativamente diferente de zero** e **positivo** --> X e Y est√£o positivamente correlacionados.\n\n. . .\n\n**O problema?** Os pacotes estat√≠sticos que utilizamos **sempre \"cospem\" um beta**. Seja ele com ou sem vi√©s.\n\n. . .\n\nCabe ao pesquisador ter um **design emp√≠rico** que garanta que o beta estimado tenha validade interna.\n\n\n\n\n\n## Como fazemos na pr√°tica? {.smaller background=\"#fadea7\"} \n\n\n<img src=\"figs/slides4-table.png\" width=\"110%\" align=\"center\" />\n\nA decis√£o final √© baseada na signific√¢ncia do Beta estimado. Se **significativo**, as vari√°veis s√£o relacionadas e fazemos infer√™ncias em cima disso.\n\nContudo, **sem um design emp√≠rico inteligente**, o beta encontrado pode ter literalmente qualquer sinal e signific√¢ncia.\n\n\n\n\n\n\n\n\n\n\n\n\n## Exemplo desses problemas {.smaller background=\"#fadea7\"} \n\nVeja esse [site](http://www.tylervigen.com/spurious-correlations).\n\n<img src=\"figs/slides4-spurius1.png\" width=\"100%\" align=\"center\" />\n\n\n\n\n\n## Exemplo desses problemas {.smaller background=\"#fadea7\"} \n\nVeja esse [site](http://www.tylervigen.com/spurious-correlations).\n\n<img src=\"figs/slides4-spurius2.png\" width=\"110%\" align=\"center\" />\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Selection bias - We see I {.smaller background=\"#fadea7\"} \n\n\n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(data.table)\nlibrary(ggplot2)\n# Generate Data\nn = 10000\nset.seed(100)\nx <- rnorm(n)\ny <- rnorm(n)\ndata1 <- 1/(1+exp( 2 - x  -  y))\ngroup  <- rbinom(n, 1, data1)\n\n# Data Together\ndata_we_see     <- subset(data.table(x, y, group), group==1)\ndata_all        <- data.table(x, y, group)\n\n# Graphs\nggplot(data_we_see, aes(x = x, y = y)) + \n      geom_point(aes(colour = factor(-group)), size = 1) +\n      geom_smooth(method=lm, se=FALSE, fullrange=FALSE)+\n      labs( y = \"\", x=\"\", title = \"The observations we see\")+\n      xlim(-3,4)+ ylim(-3,4)+ \n      theme(plot.title = element_text(color=\"black\", size=30, face=\"bold\"),\n            panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n            axis.text.y = element_text(face=\"bold\", color=\"black\", size = 18),\n            axis.text.x = element_text(face=\"bold\", color=\"black\", size = 18),\n            legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nn = 10000\nnp.random.seed(100)\nx = np.random.normal(size=n)\ny = np.random.normal(size=n)\ndata1 = 1 / (1 + np.exp(2 - x - y))\ngroup = np.random.binomial(1, data1, n)\n\ndata_we_see = pd.DataFrame({'x': x[group == 1], 'y': y[group == 1], 'group': group[group == 1]})\ndata_all = pd.DataFrame({'x': x, 'y': y, 'group': group})\n\nsns.set(style='whitegrid')\nplt.figure(figsize=(7, 5))\nplt.scatter(data_we_see['x'], data_we_see['y'], c=-data_we_see['group'], cmap='viridis', s=20)\nsns.regplot(x='x', y='y', data=data_we_see, scatter=False, ci=None, line_kws={'color': 'blue'})\nplt.title(\"The observations we see\", fontsize=18)\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nclear all\nset seed 100\nset obs 10000\ngen x = rnormal(0,1)\ngen y = rnormal(0,1)\ngen data1 = 1 / (1 + exp(2 - x - y))\ngen group = rbinomial(1, data1)\ntwoway (scatter x y if group == 1, mcolor(black) msize(small))    (lfit y x if group == 1, color(blue)),title(\"The observations we see\", size(large) ) xtitle(\"\") ytitle(\"\")\nquietly graph export figs/graph1.svg, replace\n```\n:::\n\n\n![](figs/graph1.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n## Selection bias - We see II  {.smaller background=\"#fadea7\"} \n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# Fit a linear regression model\nmodel <- lm(y ~ x, data = data_we_see)\n# Print the summary of the regression model\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, data = data_we_see)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.05878 -0.63754 -0.00276  0.62056  3.11374 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.72820    0.02660   27.37  < 2e-16 ***\nx           -0.14773    0.02327   -6.35 2.75e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9113 on 1747 degrees of freedom\nMultiple R-squared:  0.02256,\tAdjusted R-squared:  0.022 \nF-statistic: 40.32 on 1 and 1747 DF,  p-value: 2.746e-10\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport statsmodels.api as sm\nimport pandas as pd\nn = 10000\nnp.random.seed(100)\nx = np.random.normal(size=n)\ny = np.random.normal(size=n)\ndata1 = 1 / (1 + np.exp(2 - x - y))\ngroup = np.random.binomial(1, data1, n)\n\ndata_we_see = pd.DataFrame({'x': x[group == 1], 'y': y[group == 1], 'group': group[group == 1]})\ndata_all = pd.DataFrame({'x': x, 'y': y, 'group': group})\n\nX = data_we_see['x']  \nX = sm.add_constant(X)\ny = data_we_see['y']  \nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.018\nModel:                            OLS   Adj. R-squared:                  0.018\nMethod:                 Least Squares   F-statistic:                     33.84\nDate:                qua, 28 ago 2024   Prob (F-statistic):           7.06e-09\nTime:                        21:28:21   Log-Likelihood:                -2411.1\nNo. Observations:                1809   AIC:                             4826.\nDf Residuals:                    1807   BIC:                             4837.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.7037      0.026     26.826      0.000       0.652       0.755\nx             -0.1339      0.023     -5.817      0.000      -0.179      -0.089\n==============================================================================\nOmnibus:                        4.656   Durbin-Watson:                   1.973\nProb(Omnibus):                  0.097   Jarque-Bera (JB):                5.264\nSkew:                          -0.038   Prob(JB):                       0.0720\nKurtosis:                       3.253   Cond. No.                         1.93\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nclear all\nset seed 100\nset obs 10000\ngen x = rnormal(0,1)\ngen y = rnormal(0,1)\ngen data1 = 1 / (1 + exp(2 - x - y))\ngen group = rbinomial(1, data1)\nreg y x if group ==1\n\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of observations (_N) was 0, now 10,000.\n\n      Source |       SS           df       MS      Number of obs   =     1,872\n-------------+----------------------------------   F(1, 1870)      =     48.62\n       Model |  40.9398907         1  40.9398907   Prob > F        =    0.0000\n    Residual |  1574.57172     1,870  .842016963   R-squared       =    0.0253\n-------------+----------------------------------   Adj R-squared   =    0.0248\n       Total |  1615.51161     1,871  .863448215   Root MSE        =    .91761\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |  -.1579538   .0226526    -6.97   0.000    -.2023808   -.1135269\n       _cons |   .7202285   .0257215    28.00   0.000     .6697827    .7706744\n------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Selection bias - All I  {.smaller background=\"#fadea7\"} \n\n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nggplot(data_all, aes(x = x, y = y,  colour=group)) + \n  geom_point(aes(colour = factor(-group)), size = 1) +\n  geom_smooth(method=lm, se=FALSE, fullrange=FALSE)+\n  labs( y = \"\", x=\"\", title = \"All observations\")+\n  xlim(-3,4)+ ylim(-3,4)+ \n  theme(plot.title = element_text(color=\"black\", size=30, face=\"bold\"),\n      panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n      axis.text.y = element_text(face=\"bold\", color=\"black\", size = 18),\n      axis.text.x = element_text(face=\"bold\", color=\"black\", size = 18),\n      legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style='whitegrid')\nplt.figure(figsize=(6, 4))\nsns.scatterplot(data=data_all, x='x', y='y', hue='group', palette=['blue', 'red'], s=20)\nsns.regplot(data=data_all, x='x', y='y', scatter=False, ci=None, line_kws={'color': 'blue'})\nplt.title(\"All observations\", fontsize=18)\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.legend(title=\"Group\", labels=[\"0\", \"1\"], loc=\"upper left\")\n\nplt.gca().get_legend().remove()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of observations (_N) was 0, now 10,000.\n```\n\n\n:::\n:::\n\n\n![](figs/graph2.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n\n## Selection bias - All I {.smaller background=\"#fadea7\"} \n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nmodel2 <- lm(y ~ x, data = data_all)\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, data = data_all)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9515 -0.6716  0.0087  0.6698  3.9878 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept) -0.011825   0.009994  -1.183    0.237\nx           -0.003681   0.010048  -0.366    0.714\n\nResidual standard error: 0.9994 on 9998 degrees of freedom\nMultiple R-squared:  1.342e-05,\tAdjusted R-squared:  -8.66e-05 \nF-statistic: 0.1342 on 1 and 9998 DF,  p-value: 0.7141\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport statsmodels.api as sm\nimport pandas as pd\nn = 10000\nnp.random.seed(100)\nx = np.random.normal(size=n)\ny = np.random.normal(size=n)\ndata1 = 1 / (1 + np.exp(2 - x - y))\ngroup = np.random.binomial(1, data1, n)\n\ndata_we_see = pd.DataFrame({'x': x[group == 1], 'y': y[group == 1], 'group': group[group == 1]})\ndata_all = pd.DataFrame({'x': x, 'y': y, 'group': group})\n\nX = data_all['x']  \nX = sm.add_constant(X)\ny = data_all['y']  \nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     1.281\nDate:                qua, 28 ago 2024   Prob (F-statistic):              0.258\nTime:                        21:28:33   Log-Likelihood:                -14157.\nNo. Observations:               10000   AIC:                         2.832e+04\nDf Residuals:                    9998   BIC:                         2.833e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0003      0.010     -0.034      0.973      -0.020       0.019\nx             -0.0112      0.010     -1.132      0.258      -0.031       0.008\n==============================================================================\nOmnibus:                        0.267   Durbin-Watson:                   2.009\nProb(Omnibus):                  0.875   Jarque-Bera (JB):                0.242\nSkew:                           0.009   Prob(JB):                        0.886\nKurtosis:                       3.017   Cond. No.                         1.01\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nclear all\nset seed 100\nset obs 10000\ngen x = rnormal(0,1)\ngen y = rnormal(0,1)\ngen data1 = 1 / (1 + exp(2 - x - y))\ngen group = rbinomial(1, data1)\nreg y x \n\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of observations (_N) was 0, now 10,000.\n\n      Source |       SS           df       MS      Number of obs   =    10,000\n-------------+----------------------------------   F(1, 9998)      =      0.28\n       Model |  .284496142         1  .284496142   Prob > F        =    0.5938\n    Residual |  9999.04347     9,998  1.00010437   R-squared       =    0.0000\n-------------+----------------------------------   Adj R-squared   =   -0.0001\n       Total |  9999.32797     9,999   1.0000328   Root MSE        =    1.0001\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |  -.0053101    .009956    -0.53   0.594    -.0248259    .0142057\n       _cons |   .0006182   .0100006     0.06   0.951    -.0189849    .0202213\n------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n## Selection bias {.smaller background=\"#fadea7\"} \n\nSelection bias n√£o √© o √∫nico dos nossos problemas, mas √© um **importante**.\n\nVeja que suas conclus√µes mudaram significativamente.\n\nN√£o seria dif√≠cil criar um exemplo em que o **coeficiente verdadeiro** fosse positivo.\n\n\n\n\n\n\n\n\n\n\n\n## Exemplo desses problemas {.smaller background=\"#fadea7\"} \n\n![](figs/slides4-path2b.png) \n\n\nSource: [Angrist](https://www.youtube.com/watch?v=iPBV3BlV7jk)\n\n**N√£o podemos pegar dois caminhos.**\n\n\n\n\n\n\n\n\n\n\n## Exemplo desses problemas {.smaller background=\"#fadea7\"} \n\n![](figs/slides4-matching.png) \n\n\nSource: [Angrist](https://www.youtube.com/watch?v=6YrIDhaUQOE)\n\n**N√£o podemos comparar pessoas que n√£o s√£o compar√°veis.**\n\n\n\n\n\n\n\n\n## O que precisamos fazer? {.smaller background=\"#fadea7\"} \n\n. . .\n\nDefinir um bom **_Design emp√≠rico_**\n\n. . .\n\nNo mundo ideal: ter√≠amos **universos paralelos.** Ter√≠amos **dois clones**, em que cada um escolhe um caminho. Todo o resto √© igual.\n\n- Obviamente, isso n√£o existe.\n\n. . .\n\nSegunda melhor solu√ß√£o: **experimentos**\n\n. . .\n\n**Mas o que √© um experimento?**\n\n- Grupo de tratamento vs. Grupo de controle\n\n- Igualdade entre os grupos (i.e., aleatoriedade no sampling)\n\n    - Nada diferencia os grupos a n√£o ser o fato de que um indiv√≠duo recebe tratamento e o outro n√£o\n    - Estamos comparando ma√ßas com ma√ßas e laranjas com laranjas\n      \n- Testes placebo/falsifica√ß√£o.\n\n\n\n\n\n\n\n\n\n\n\n\n\n# The challenge {.smaller background=\"#b0aeae\"}\n\n## Correlation & Causality {.smaller background=\"#b0aeae\"}\n\n\nIt is very common these days to hear someone say ‚Äú*correlation does not mean causality*.‚Äù \n\nIn essence, that is true.\n\n- *The killer struck during daylight. Had the sun not been out that day, the victim would have been safe.*\n\n. . .\n\n- There is a correlation, but it is clear there is no causation.\n\n\n\n\n\n\n\n## Correlation & Causality  {.smaller background=\"#b0aeae\"}\n\nSometimes, there is causality even when we do not observe correlation.\n\n*The sailor is adjusting the rudder on a windy day to align the boat with the wind, but the boat is not changing direction.* ([Source: The Mixtape](https://mixtape.scunning.com/01-introduction#do-not-confuse-correlation-with-causality))\n\n\n![](figs/scottboat.jpg)\n\n\n\n\n\n::: {.callout-note}\n\nIn this example, the sailor is *endogenously* adjusting the course to balance the unobserved wind.\n\n:::\n\n\n\n\n\n\n\n## The challenge  {.smaller background=\"#b0aeae\"}\n\n- I will discuss some issues in using plain OLS models in Finance Research (mainly with panel data).\n\n. . .\n\n- I will avoid the word ‚Äúendogeneity‚Äù as much as I can\n\n. . .\n\n- I will also avoid the word ‚Äúidentification‚Äù because identification does not guarantee causality and vice-versa (Kahn and Whited 2017)\n\n. . .\n\n- The discussion is based on [Atanasov and Black (2016)](https://www.nowpublishers.com/article/Details/CFR-0036)\n\n![](figs/slides1-empiricalissues-paper.png)\n\n\n\n\n\n\n\n\n## The challenge  {.smaller background=\"#b0aeae\"}\n\n- Imagine that you want to investigate the effect of Governance on Q\n\n    - You may have more covariates explaining Q (omitted  from slides)\n  \n $ùë∏_{i} = Œ± + ùú∑_{i} √ó Gov + Controls + error$\n\n. . . \n\n All the issues in the next slides will make it not possible to infer that __changing Gov will _CAUSE_ a change in Q__ \n \n That is, cannot infer causality\n \n![](figs/slides1-empiricalissues-wrong.jpg)\n\n\n\n\n\n\n\n\n## 1) Reverse causation   {.smaller background=\"#b0aeae\"}\n\n_One source of bias is: reverse causation_\n\n- Perhaps it is Q that causes Gov\n\n- OLS based methods do not tell the difference between these two betas:\n\n$ùëÑ_{i} = Œ± + ùú∑_{i} √ó Gov + Controls + error$\n\n$Gov_{i} = Œ± + ùú∑_{i} √ó Q + Controls + error$\n\n- If one Beta is significant, the other will most likely be significant too\n\n- You need a sound theory!\n\n\n\n\n\n\n\n\n\n\n\n\n## 2) Omitted variable bias (OVB)  {.smaller background=\"#b0aeae\"}\n\n_The second source of bias is: OVB_\n\n- Imagine that you do not include an important ‚Äútrue‚Äù predictor of Q\n\n- Let's say, long is:  $ùë∏_{i} = ùú∂_{long} + ùú∑_{long}* gov_{i} + Œ¥ * omitted + error$\n\n- But you estimate short:  $ùë∏_{i} = ùú∂_{short} + ùú∑_{short}* gov_{i} + error$\n\n- $ùú∑_{short}$ will be: \n\n    - $ùú∑_{short} = ùú∑_{long}$ +  bias\n\n    - $ùú∑_{short} = ùú∑_{long}$ +  relationship between omitted (omitted) and included (Gov) * effect of omitted in long (Œ¥)\n\n        - Where: relationship between omitted (omitted) and included (Gov) is: $Omitted = ùú∂ + œï *gov_{i} + u$\n\n- Thus, OVB is: $ùú∑_{short} ‚Äì ùú∑_{long} = œï * Œ¥$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## 3) Specification error  {.smaller background=\"#b0aeae\"}\n\n_The third source of bias is: Specification error_\n\n- Even if we could perfectly measure gov and all relevant covariates, we would not know for sure the functional form through which each influences q\n\n    - Functional form: linear? Quadratic? Log-log? Semi-log?\n\n- Misspecification of x‚Äôs is similar to OVB\n\n\n\n\n\n\n\n\n## 4) Signaling   {.smaller background=\"#b0aeae\"}\n\n_The fourth source of bias is: Signaling_\n\n- Perhaps, some individuals are signaling the existence of an X without truly having it:\n\n    - For instance: firms signaling they have good governance without having it\n\n- This is similar to the OVB because you cannot observe the full story\n\n\n\n\n\n\n\n\n\n## 5) Simultaneity  {.smaller background=\"#b0aeae\"}\n\n_The fifth source of bias is: Simultaneity_\n\n- Perhaps gov and some other variable x are determined simultaneously\n\n- Perhaps there is bidirectional causation, with q causing gov and gov also causing q \n\n- In both cases, OLS regression will provide a biased estimate of the effect\n\n- Also, the sign might be wrong\n\n\n\n\n\n\n\n\n\n\n## 6) Heterogeneous effects   {.smaller background=\"#b0aeae\"}\n\n_The sixth source of bias is: Heterogeneous effects_\n\n- Maybe the causal effect of gov on q depends on observed and unobserved firm characteristics:\n\n    - Let's assume that firms seek to maximize q\n    - Different firms have different optimal gov\n    - Firms know their optimal gov\n    - If we observed all factors that affect q, each firm would be at its own optimum and OLS regression would give a non-significant coefficient\n\n- In such case, we may find a positive or negative relationship.\n\n- Neither is the true causal relationship\n\n\n\n\n\n## 7) Construct validity  {.smaller background=\"#b0aeae\"}\n\n_The seventh source of bias is: Construct validity_\n\n- Some constructs (e.g. Corporate governance) are complex, and sometimes have conflicting mechanisms\n\n- We usually don‚Äôt know for sure what ‚Äúgood‚Äù governance is, for instance\n\n- It is common that we use imperfect proxies\n\n- They may poorly fit the underlying concept\n\n\n\n\n\n\n\n## 8) Measurement error   {.smaller background=\"#b0aeae\"}\n\n_The eighth source of bias is: Measurement error_\n\n- \"Classical\" random measurement error for the outcome will inflate standard errors but will not lead to biased coefficients. \n\n    - $y^{*} = y + \\sigma_{1}$\n    - If you estimante $y^{*} = f(x)$, you have $y + \\sigma_{1} = x + \\epsilon$ \n    - $y = x + u$ \n        - where $u = \\epsilon + \\sigma_{1}$ \n\n- \"Classical\" random measurement error in x‚Äôs will bias coefficient estimates toward zero\n\n    - $x^{*} = x + \\sigma_{2}$\n    - Imagine that $x^{*}$ is a bunch of noise\n    - It would not explain anything\n    - Thus, your results are biased toward zero\n\n\n<!-- https://web.stanford.edu/class/polisci100a/regress5.pdf  --> \n\n\n\n\n\n\n\n\n\n## 9) Observation bias   {.smaller background=\"#b0aeae\"}\n\n_The ninth source of bias is: Observation bias_\n\n- This is analogous to the Hawthorne effect, in which observed subjects behave differently because they are observed\n\n- Firms which change gov may behave differently because their managers or employees think the change in gov matters, when in fact it has no direct effect\n\n\n\n\n\n\n\n\n\n\n## 10) Interdependent effects   {.smaller background=\"#b0aeae\"}\n\n_The tenth source of bias is: Interdependent effects_\n\n- Imagine that a governance reform that will not affect share prices for a single firm might be effective if several firms adopt\n\n- Conversely, a reform that improves efficiency for a single firm might not improve profitability if adopted widely because the gains will be competed away\n\n- \"One swallow doesn't make a summer\" \n\n\n\n\n\n\n## 11) Selection bias   {.smaller background=\"#b0aeae\"}\n\n_The eleventh source of bias is: Selection bias_\n\n- If you run a regression with two types of companies\n\n    - High gov (let's say they are the treated group)\n    - Low gov (let's say they are the control group)\n\n    \n- Without any matching method, these companies are likely not comparable\n\n- Thus, the estimated beta will contain selection bias\n\n- The bias can be either be positive or negative\n\n- It is similar to OVB\n\n\n  \n\n## 12) Self-Selection  {.smaller background=\"#b0aeae\"}\n\n_The twelfth source of bias is: Self-Selection_\n\n- Self-selection is a type of selection bias\n\n- Usually, firms decide which level of governance they adopt\n\n- There are reasons why firms adopt high governance\n\n    - If observable, you need to control for\n    - If unobservable, you have a problem\n\n- It is like they \"self-select\" into the treatment.\n\n    - Units decide whether they receive the treatment of not\n\n- Your coefficients will be biased.\n\n\n\n\n\n\n## 13) Collider Bias (endog. selection bias) {.smaller background=\"#b0aeae\"}\n\n\n**Let's assume an arbitrary population.**\n\n- Two variables describe the population: IQ and luck. \n\n- These variables are random and normally distributed.\n\n- Let's say that after you reach a certain level of IQ and Luck, you become successful (i.e., upper-right quadrant).\n\n- Because you are interested in successful people, you only investigate such subsample. \n\n\n\n\n## 13) Collider Bias (endog. selection bias) {.smaller background=\"#b0aeae\"}\n\n**A representation of the population**\n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(data.table)\nlibrary(ggplot2)\nset.seed(100)\nluck <- rnorm(1000, 100, 15)\niq   <- rnorm(1000, 100, 15)\npop <- data.frame(luck, iq)\n\nggplot(pop, aes(x = iq, y = luck)) + \n      geom_point() +\n      labs(title = \"The general population\")  + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnp.random.seed(100)\n\nluck = np.random.normal(100, 15, 1000)\niq = np.random.normal(100, 15, 1000)\npop = pd.DataFrame({'luck': luck, 'iq': iq})\n\nsns.set(style=\"whitegrid\")  \nplt.figure(figsize=(8, 6))\nplt.scatter(pop['iq'], pop['luck'])\nplt.title(\"The general population\")\nplt.xlabel(\"IQ\")\nplt.ylabel(\"Luck\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nset seed 100\nset obs 1000\ngen luck = rnormal(100, 15)\ngen iq = rnormal(100, 15)\ntwoway (scatter luck iq), title(\"The general population\") \nquietly graph export figs/collider1.svg, replace\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of observations (_N) was 0, now 1,000.\n```\n\n\n:::\n:::\n\n\n![](figs/collider1.svg)\n\n:::\n\n\n\n\n\n## 13) Collider Bias (endog. selection bias) {.smaller background=\"#b0aeae\"}\n\n::: {.callout-important}\n**Analyzing only successful people will suggest a negative correlation between luck and IQ.**\n\n- \"Success\" is a collider in this example. It \"collides\" with luck and IQ.\n::: \n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(data.table)\nlibrary(ggplot2)\nset.seed(100)\nluck <- rnorm(1000, 100, 15)\niq   <- rnorm(1000, 100, 15)\npop <- data.frame(luck, iq)\n\npop$comb <- pop$luck + pop$iq \nsuccessfull <- pop[pop$comb > 240, ] \n\nggplot() + \n  geom_point(data = pop, aes(x = iq, y = luck)) +  \n  geom_point(data = successfull, aes(x = iq, y = luck), color = \"red\") +  \n  labs(title = \"The general population & successful subpopulation\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-17-3.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nnp.random.seed(100)\n\nluck = np.random.normal(100, 15, 1000)\niq = np.random.normal(100, 15, 1000)\npop = pd.DataFrame({'luck': luck, 'iq': iq})\n\npop['comb'] = pop['luck'] + pop['iq']\n\nsuccessful = pop[pop['comb'] > 240]\n\nsns.set(style=\"whitegrid\")  # Minimalistic theme similar to theme_minimal in ggplot2\nplt.figure(figsize=(8, 6))\nplt.scatter(pop['iq'], pop['luck'], label=\"General Population\", alpha=0.5)\nplt.scatter(successful['iq'], successful['luck'], color='red', label=\"Successful Subpopulation\", alpha=0.7)\nplt.title(\"The general population & successful subpopulation\")\nplt.xlabel(\"IQ\")\nplt.ylabel(\"Luck\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nset seed 100\nset obs 1000\ngen luck = rnormal(100, 15)\ngen iq = rnormal(100, 15)\ngen comb = luck + iq\ngen successful = comb > 240\ntwoway (scatter luck iq if successful == 0)  (scatter luck iq if successful == 1, mcolor(red)) , title(\"The general population & successful subpopulation\") \n\nquietly graph export figs/collider2.svg, replace\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of observations (_N) was 0, now 1,000.\n```\n\n\n:::\n:::\n\n\n![](figs/collider2.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Conclus√£o  {.smaller background=\"#b0aeae\"}\n\n**Pesquisa quantitativa tem a parte _quanti (m√©todos, modelos, etc.)_...**\n\n**... Mas talvez a parte mais importante seja o desenho da pesquisa (design emp√≠rico)!**\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Preocupa√ß√µes recentes em pesquisa   {.smaller background=\"#b0aeae\"}\n\n**P-Hacking**\n\n![](figs/slides4-phacking.png) \n\nArtigo original [aqui](https://doi.org/10.1111/jofi.12530).\n\n\n\n\n\n\n\n\n\n\n## Preocupa√ß√µes recentes em pesquisa  {.smaller background=\"#b0aeae\"}\n\n**Publication bias**\n\n![](figs/slides4-Harvey-2017.png) \n\n\nArtigo original [aqui](https://doi.org/10.1111/jofi.12530).\n\n\n\n\n\n\n\n \n\n## Preocupa√ß√µes recentes em pesquisa   {.smaller background=\"#b0aeae\"}\n\n**Crise de replica√ß√£o**\n\n\n![](figs/slides4-aguinis.png) \n\n\nArtigo original [aqui](https://link.springer.com/article/10.1057/s41267-017-0081-0).\n\n\n\n\n\n## Some fun stuff  {.smaller background=\"#b0aeae\"}\n\n![](figs/selection bias.png) \n\n\n## Some fun stuff  {.smaller background=\"#b0aeae\"}\n\n![](figs/fig1.jpg) \n\n\n\n\n\n## Some fun stuff  {.smaller background=\"#b0aeae\"}\n\n\n![](figs/hypothesis2.png) \n\n\n\n\n\n\n## Some fun stuff {.smaller background=\"#b0aeae\"}\n\n\n![](figs/confounding variables.png) \n\n\n\n\n\n\n\n\n\n## Some fun stuff {.smaller background=\"#b0aeae\"}\n\n![](figs/proxy variable.png) \n\n\n\n\n\n\n\n\n\n\n\n\n\n## **THANK YOU!** {background=\"#b1cafa\"}\n\n::: columns\n::: {.column width=\"60%\"}\n**QUESTIONS?**\n\n![](figs/qa2.png){width=\"150%\" heigth=\"150%\"}\n:::\n\n::: {.column width=\"40%\"}\n**Henrique C. Martins**\n\n-   [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)\n-   [Personal Website](https://henriquemartins.net/)\n-   [LinkedIn](https://www.linkedin.com/in/henriquecastror/)\n-   [Lattes](http://lattes.cnpq.br/6076997472159785)\n-   [Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)\\\n:::\n:::\n\n::: footer\n:::\n",
    "supporting": [
      "part_1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}