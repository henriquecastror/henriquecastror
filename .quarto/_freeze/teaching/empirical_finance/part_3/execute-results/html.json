{
  "hash": "abb17e34fdff772f0dbe55f9c25f2c86",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Empirical Methods in Finance'\nsubtitle: 'Part 3'\nauthor: 'Henrique C. Martins'\nformat:\n  revealjs: \n    slide-number: true\n    theme: simple\n    chalkboard: true\n    preview-links: auto\n    logo: figs/background8.png\n    css: logo.css\n    footer: '**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  '\n    multiplex: true\n    scrollable: true \ntitle-slide-attributes:\n    data-background-color: \"#b1cafa\"\ninclude-after: |\n  <script type=\"text/javascript\">\n    Reveal.on('ready', event => {\n      if (event.indexh === 0) {\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n      }\n    });\n    Reveal.addEventListener('slidechanged', (event) => {\n      if (event.indexh === 0) {\n        Reveal.configure({ slideNumber: null });\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n      }\n      if (event.indexh === 1) { \n        Reveal.configure({ slideNumber: 'c' });\n        document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n      }\n    });\n  </script>\n\n---\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Conterfactuals {.smaller background=\"#b3eafc\"}\n\n## Conterfactuals {.smaller background=\"#b3eafc\"}\n\n-   Imagine that John and Mary are moving to the north of Canada.\n\n-   John has a history of respiratory disease and decide to buy insurance.\n\n-   Mary does not have a history of respiratory disease and decide not to buy insurance.\n\n-   What is the causal effect of buying insurance?\n\n| Default                     | John   |   Mary |\n|-----------------------------|:-------|-------:|\n| State of insurance          | 1      |      0 |\n| Situation without insurance | `n.o.` |      5 |\n| Situation with insurance    | 4      | `n.o.` |\n| Observed                    | 4      |      5 |\n| Effect                      | ?      |      ? |\n\n[Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845)\n\n\n\n\n\n\n\n\n## Conterfactuals {.smaller background=\"#b3eafc\"}\n\n**Na√Øve calculation: comparing John com Mary**\n\n$$Y_{john} - Y_{Mary} = 4 - 5 = -1$$\n\nConclusion: buying insurance has a negative effect on health.\n\n. . .\n\n**This is wrong!**\n\n[Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845)\n\n## Conterfactuals {.smaller background=\"#b3eafc\"}\n\n| Default                     | John | Mary |\n|-----------------------------|:-----|-----:|\n| State of insurance          | 1    |    0 |\n| Situation without insurance | `3`  |    5 |\n| Situation with insurance    | 4    |  `5` |\n| Observed                    | 4    |    5 |\n| Effect                      | ?    |    ? |\n\n$$(Y_{1,john} - Y_{0,john}) + (Y_{1,Mary}- Y_{0,Mary}) = 4 - 3 + 5 - 5 = 0.5$$\n\n**Conclusion:** buying insurance has a positive effect of 1 in John's health and average effect of 0.5 in the sample's health (i.e. averages conditional on insurance status).\n\n[Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Regressions {.smaller background=\"#dfe3f7\"}\n\n## Regression [Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n**Let's see how a regression could solve the problem.** Imagine that you have the following data on students' application. (**Decisions in bold**)\n\n| Student | Private   | Private   | Private   | Public    | Public    | Public    | Earnings |\n|---------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n|         | Ivy       | Leafy     | Smart     | State     | Tall      | Altered   | 110,000  |\n| 1       |           | Reject    | **Admit** |           | Admit     |           | 110,000  |\n| 2       |           | Reject    | **Admit** |           | Admit     |           | 100,000  |\n| 3       |           | Reject    | Admit     |           | **Admit** |           | 110,000  |\n| 4       | **Admit** |           | Admit     |           | Admit     | Admit     | 60,000   |\n| 5       | Admit     |           | Admit     |           | Admit     | **Admit** | 30,000   |\n| 6       |           | **Admit** |           |           |           |           | 115,000  |\n| 7       |           | **Admit** |           |           |           |           | 75,000   |\n| 8       | Reject    |           |           | **Admit** | Admit     |           | 90,000   |\n| 9       | Reject    |           |           | Admit     | **Admit** |           | 60,000   |\n\n\n\n\n\n\n\n\n## Regression [Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n**We can see from the table that:**\n\n-   Some students earn high salary, in both situations\n\n-   Some students earn low salary, in both situations\n\n-   There are clusters of students that applied for the same universities\n\n    -   How likely are they to be similar? Can we benefit from the fact they believe they are similar?\n\n. . .\n\n-   If we compare earnings from the first three individuals:\n\n    -   ((110 + 100)/ 2 - 11000) = -5.000\n\n-   If we compare earnings from individuals 4 and 5:\n\n    -   (60 - 30) = 30.000\n\n-   The average is:\n\n    -   25.000/2 = 12.500\n\n\n\n\n\n\n\n\n\n\n\n## Regression [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\nLet's create a dataframe to run regressions with the previous student's data.\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# Create the data frame\ndata <- data.frame(\n  id = 1:9,\n  earnings = c(110000, 100000, 110000, 60000, 30000, 115000, 75000, 90000, 60000),\n  school = c(\"private\", \"private\", \"public\", \"private\", \"public\", \"private\", \"private\", \"public\", \"public\"),\n  private = c(1, 1, 0, 1, 0, 1, 1, 0, 0),\n  group = c(1, 1, 1, 2, 2, 3, 3, 4, 4)\n)\nprint(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  id earnings  school private group\n1  1   110000 private       1     1\n2  2   100000 private       1     1\n3  3   110000  public       0     1\n4  4    60000 private       1     2\n5  5    30000  public       0     2\n6  6   115000 private       1     3\n7  7    75000 private       1     3\n8  8    90000  public       0     4\n9  9    60000  public       0     4\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\ndata = pd.DataFrame({\n    'id': range(1, 10),\n    'earnings': [110000, 100000, 110000, 60000, 30000, 115000, 75000, 90000, 60000],\n    'school': [\"private\", \"private\", \"public\", \"private\", \"public\", \"private\", \"private\", \"public\", \"public\"],\n    'private': [1, 1, 0, 1, 0, 1, 1, 0, 0],\n    'group': [1, 1, 1, 2, 2, 3, 3, 4, 4]\n})\nprint(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   id  earnings   school  private  group\n0   1    110000  private        1      1\n1   2    100000  private        1      1\n2   3    110000   public        0      1\n3   4     60000  private        1      2\n4   5     30000   public        0      2\n5   6    115000  private        1      3\n6   7     75000  private        1      3\n7   8     90000   public        0      4\n8   9     60000   public        0      4\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\ninput id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\nlist\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            id   earnings     school    private      group\n  1. 1 110000 \"private\" 1 1\n  2. 2 100000 \"private\" 1 1\n  3. 3 110000 \"public\" 0 1\n  4. 4 60000 \"private\" 1 2\n  5. 5 30000 \"public\" 0 2\n  6. 6 115000 \"private\" 1 3\n  7. 7 75000 \"private\" 1 3\n  8. 8 90000 \"public\" 0 4\n  9. 9 60000 \"public\" 0 4\n 10. end\n\n     +-------------------------------------------+\n     | id   earnings    school   private   group |\n     |-------------------------------------------|\n  1. |  1     110000   private         1       1 |\n  2. |  2     100000   private         1       1 |\n  3. |  3     110000    public         0       1 |\n  4. |  4      60000   private         1       2 |\n  5. |  5      30000    public         0       2 |\n     |-------------------------------------------|\n  6. |  6     115000   private         1       3 |\n  7. |  7      75000   private         1       3 |\n  8. |  8      90000    public         0       4 |\n  9. |  9      60000    public         0       4 |\n     +-------------------------------------------+\n```\n\n\n:::\n:::\n\n:::\n\n\n\n\n\n\n\n\n## \"Naive\" regression all students [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\epsilon$$ **What is the benefit of private education here?**\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# Create the data frame\nmodel <- lm(earnings ~ private, data = data)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = earnings ~ private, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-42500 -17000   8000  18000  37500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    72500      14522   4.992  0.00158 **\nprivate        19500      19484   1.001  0.35023   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29040 on 7 degrees of freedom\nMultiple R-squared:  0.1252,\tAdjusted R-squared:  0.0002116 \nF-statistic: 1.002 on 1 and 7 DF,  p-value: 0.3502\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\n#pip install numpy scikit-learn statsmodels\nimport statsmodels.api as sm\nX = sm.add_constant(data['private'])  \ny = data['earnings']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.125\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     1.002\nDate:                sex, 29 ago 2025   Prob (F-statistic):              0.350\nTime:                        13:51:29   Log-Likelihood:                -104.13\nNo. Observations:                   9   AIC:                             212.3\nDf Residuals:                       7   BIC:                             212.7\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        7.25e+04   1.45e+04      4.992      0.002    3.82e+04    1.07e+05\nprivate      1.95e+04   1.95e+04      1.001      0.350   -2.66e+04    6.56e+04\n==============================================================================\nOmnibus:                        1.011   Durbin-Watson:                   2.352\nProb(Omnibus):                  0.603   Jarque-Bera (JB):                0.666\nSkew:                          -0.263   Prob(JB):                        0.717\nKurtosis:                       1.776   Cond. No.                         2.77\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nquiet input id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\n\nreg earnings private \n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Source |       SS           df       MS      Number of obs   =         9\n-------------+----------------------------------   F(1, 7)         =      1.00\n       Model |   845000000         1   845000000   Prob > F        =    0.3502\n    Residual |  5.9050e+09         7   843571429   R-squared       =    0.1252\n-------------+----------------------------------   Adj R-squared   =    0.0002\n       Total |  6.7500e+09         8   843750000   Root MSE        =     29044\n\n------------------------------------------------------------------------------\n    earnings | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     private |      19500   19483.51     1.00   0.350    -26571.18    65571.18\n       _cons |      72500   14522.15     4.99   0.002     38160.57    106839.4\n------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n:::\n\n\n\n\n\n\n\n\n## \"Naive\" regression all students [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\epsilon$$ **What is the benefit of private education here?**\n\nThe coefficient of `private` is 19500, meaning that those that have private education earn 19500 more.\n\n. . . \n\nThe problem with this design is that 1) we are including all students, even those that do not bring any \"information\", and 2) we are not controlling for the differences in students' profiles. \n\n\nLet's fix the first problem first. \n\n**What students should we not include in the model?**\n\n\n\n\n\n## Students id\\<=5 [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\epsilon \\;,\\; if\\; i <=5$$ **What is the benefit of private education here?**\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nmodel2 <- lm(earnings ~ private , data = subset(data,id<=5))\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = earnings ~ private, data = subset(data, id <= 5))\n\nResiduals:\n     1      2      3      4      5 \n 20000  10000  40000 -30000 -40000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    70000      27689   2.528   0.0856 .\nprivate        20000      35746   0.560   0.6149  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39160 on 3 degrees of freedom\nMultiple R-squared:  0.09449,\tAdjusted R-squared:  -0.2073 \nF-statistic: 0.313 on 1 and 3 DF,  p-value: 0.6149\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\n#pip install numpy scikit-learn statsmodels\n\nsubset_data = data[data['id'] <= 5]\nX = sm.add_constant(subset_data['private']) \ny = subset_data['earnings']\nmodel2 = sm.OLS(y, X).fit()\nprint(model2.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.094\nModel:                            OLS   Adj. R-squared:                 -0.207\nMethod:                 Least Squares   F-statistic:                    0.3130\nDate:                sex, 29 ago 2025   Prob (F-statistic):              0.615\nTime:                        13:51:31   Log-Likelihood:                -58.694\nNo. Observations:                   5   AIC:                             121.4\nDf Residuals:                       3   BIC:                             120.6\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst           7e+04   2.77e+04      2.528      0.086   -1.81e+04    1.58e+05\nprivate         2e+04   3.57e+04      0.560      0.615   -9.38e+04    1.34e+05\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   1.304\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.520\nSkew:                          -0.129   Prob(JB):                        0.771\nKurtosis:                       1.441   Cond. No.                         2.92\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nquiet input id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\nreg earnings private if id<=5\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Source |       SS           df       MS      Number of obs   =         5\n-------------+----------------------------------   F(1, 3)         =      0.31\n       Model |   480000000         1   480000000   Prob > F        =    0.6149\n    Residual |  4.6000e+09         3  1.5333e+09   R-squared       =    0.0945\n-------------+----------------------------------   Adj R-squared   =   -0.2073\n       Total |  5.0800e+09         4  1.2700e+09   Root MSE        =     39158\n\n------------------------------------------------------------------------------\n    earnings | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     private |      20000   35746.02     0.56   0.615    -93759.78    133759.8\n       _cons |      70000   27688.75     2.53   0.086    -18117.95    158117.9\n------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n:::\n\n\n\n\n\n## Students id\\<=5 [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\epsilon \\;,\\; if\\; i <=5$$ **What is the benefit of private education here?**\n\nStudents 6 and 7 only applied to Private, while students 8 and 9 did not really had a choice. So we should exclude them.\n\n. . .\n\nThe benefit of private is now 20000.\n\nThe coefficient did not change much, but the design improved partially.\n\n. . .\n\nWe still have an uncontrolled \"heterogeneity\" in the groups of students. **Students 1 to 3 seem to earn more no matter their decisions**.\n\n\n\n\n\n\n\n## Apples-to-Apples [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\beta_2 Group+ \\epsilon \\;,\\; if\\; i <=5$$ **This is the best we can do.**\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\ndata$dummy <- ifelse(data$group == 1, 1, 0)\ndata$dummy[data$group == 2] <- 0\nmodel3 <- lm(earnings ~ private + dummy, data = subset(data,id<=5))\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = earnings ~ private + dummy, data = subset(data, \n    id <= 5))\n\nResiduals:\n         1          2          3          4          5 \n 1.182e-11 -1.000e+04  1.000e+04  1.000e+04 -1.000e+04 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)    40000      11952   3.347   0.0789 .\nprivate        10000      13093   0.764   0.5248  \ndummy          60000      13093   4.583   0.0445 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14140 on 2 degrees of freedom\nMultiple R-squared:  0.9213,\tAdjusted R-squared:  0.8425 \nF-statistic:  11.7 on 2 and 2 DF,  p-value: 0.07874\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\n#pip install numpy scikit-learn statsmodels\n\ndata['dummy'] = 1\ndata.loc[data['group'] == 2, 'dummy'] = 0\nsubset_data = data[data['id'] <= 5]\nX = sm.add_constant(subset_data[['private', 'dummy']])\ny = subset_data['earnings']\nmodel3 = sm.OLS(y, X).fit()\nprint(model3.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               earnings   R-squared:                       0.921\nModel:                            OLS   Adj. R-squared:                  0.843\nMethod:                 Least Squares   F-statistic:                     11.70\nDate:                sex, 29 ago 2025   Prob (F-statistic):             0.0787\nTime:                        13:51:33   Log-Likelihood:                -52.589\nNo. Observations:                   5   AIC:                             111.2\nDf Residuals:                       2   BIC:                             110.0\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst           4e+04    1.2e+04      3.347      0.079   -1.14e+04    9.14e+04\nprivate         1e+04   1.31e+04      0.764      0.525   -4.63e+04    6.63e+04\ndummy           6e+04   1.31e+04      4.583      0.044    3665.052    1.16e+05\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   2.250\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.638\nSkew:                           0.000   Prob(JB):                        0.727\nKurtosis:                       1.250   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nquiet input id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\ngen \tdummy = 1 if group == 1\nreplace dummy = 0 if group == 2\nreg earnings private dummy if id<=5 \n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(6 missing values generated)\n\n(2 real changes made)\n\n      Source |       SS           df       MS      Number of obs   =         5\n-------------+----------------------------------   F(2, 2)         =     11.70\n       Model |  4.6800e+09         2  2.3400e+09   Prob > F        =    0.0787\n    Residual |   400000000         2   200000000   R-squared       =    0.9213\n-------------+----------------------------------   Adj R-squared   =    0.8425\n       Total |  5.0800e+09         4  1.2700e+09   Root MSE        =     14142\n\n------------------------------------------------------------------------------\n    earnings | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     private |      10000   13093.07     0.76   0.525    -46334.95    66334.95\n       dummy |      60000   13093.07     4.58   0.044     3665.052    116334.9\n       _cons |      40000   11952.29     3.35   0.079    -11426.54    91426.54\n------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n:::\n\n\n\n\n## Regression {.smaller background=\"#dfe3f7\"}\n\nThe previous regression assumes that students 1 to 3 are different that students 4 and 5. \n\nWe will find many instances like that in empirical research. E.g., industry. \n\n. . .\n\nThe private school coefficient, in this case 10,000, implies a private-public earnings differential of this value.\n\n. . .\n\n::: callout-important\nThe Y above is used in monetary values.\n\nUsing a logged y, ln(Y) or ln(earnings), allows estimates to be interpreted as a percent change.\n\nFor instance if $\\beta=0.05$, it means that the earnings differential is 5% for those studying in private schools (conditional on the controls included in the model). \n:::\n\n\n\n\n\n\n\n\n\n\n\n# OVB again {.smaller background=\"#f5caae\"}\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n\n\nRegression is a way to make other things equal (ceteris paribus), but equality  is generated only for variables included in the model as controls on the right-hand sided of the model.\n\nFailure to include enough controls of the right controls still leave us with selection bias.\n\nThe regression version of the selection bias generated by the inadequate controls is called **Omitted Variable Bias (OVB)**. \n\nThe inclusion of a control that should not be included is called \"**Bad Controls**\" problem.\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n**How could we calculate the OVB in this example?**\n\n\n$$earnings_i = 70.000 + 20.000\\times Private_i  \\epsilon $$\n\n$$earnings_i = 40.000 + 10.000 \\times Private_i + 60.000 \\times Group+ \\epsilon$$ \n\n\n- $\\beta$ (1st regression) - $\\beta$ (second regression).\n- The OVB here is 20.000 - 10.000 = 10.000.\n- Meaning that the $\\beta$ (1st regression) is 10.000 higher than what it should be.\n\n\n\n\n\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n**How could we calculate the OVB in this example?**\n\n\nWe could calculate the bias by estimating:\n\n$$Private=\\alpha + \\beta_{omitted} \\times Group + \\epsilon$$\n\nThen,\n\n$$\\beta_{omitted} \\times \\beta_{missing} = 0.1667 * 60.000 = 10.000$$\n\nThe OVB is 10.000, meaning that the first model (the one with the omitted variable) estimates a Beta that is 10.000 higher than it should be. \n\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nmodel4 <- lm(private ~ dummy , data = subset(data,id<=5))\nsummary(model4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = private ~ dummy, data = subset(data, id <= 5))\n\nResiduals:\n      1       2       3       4       5 \n 0.3333  0.3333 -0.6667  0.5000 -0.5000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   0.5000     0.4410   1.134    0.339\ndummy         0.1667     0.5693   0.293    0.789\n\nResidual standard error: 0.6236 on 3 degrees of freedom\nMultiple R-squared:  0.02778,\tAdjusted R-squared:  -0.2963 \nF-statistic: 0.08571 on 1 and 3 DF,  p-value: 0.7888\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nmatrix2<- summary(model4)$coefficients\nsum(0.1667 * 60000 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10002\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nsubset_data = data[data['id'] <= 5]\nmodel4 = sm.OLS(subset_data['private'], sm.add_constant(subset_data[['dummy']])).fit()\nprint(model4.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                private   R-squared:                       0.028\nModel:                            OLS   Adj. R-squared:                 -0.296\nMethod:                 Least Squares   F-statistic:                   0.08571\nDate:                sex, 29 ago 2025   Prob (F-statistic):              0.789\nTime:                        13:51:35   Log-Likelihood:                -3.4565\nNo. Observations:                   5   AIC:                             10.91\nDf Residuals:                       3   BIC:                             10.13\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.5000      0.441      1.134      0.339      -0.903       1.903\ndummy          0.1667      0.569      0.293      0.789      -1.645       1.978\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   2.881\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.749\nSkew:                          -0.394   Prob(JB):                        0.688\nKurtosis:                       1.276   Cond. No.                         2.92\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nbias = 0.1667 * 60000\nprint(bias)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n10002.0\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nquiet input id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\ngen \tdummy = 1 if group == 1\nreplace dummy = 0 if group == 2\nreg private dummy if id<=5\ndi .1666667 *  60000 \n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(6 missing values generated)\n\n(2 real changes made)\n\n      Source |       SS           df       MS      Number of obs   =         5\n-------------+----------------------------------   F(1, 3)         =      0.09\n       Model |  .033333333         1  .033333333   Prob > F        =    0.7888\n    Residual |  1.16666667         3  .388888889   R-squared       =    0.0278\n-------------+----------------------------------   Adj R-squared   =   -0.2963\n       Total |         1.2         4          .3   Root MSE        =    .62361\n\n------------------------------------------------------------------------------\n     private | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       dummy |   .1666667    .569275     0.29   0.789    -1.645021    1.978354\n       _cons |         .5   .4409586     1.13   0.339    -.9033269    1.903327\n------------------------------------------------------------------------------\n\n10000.002\n```\n\n\n:::\n:::\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n**So what?**\n\n- Anticipating the effect of the omitted variable on the non-omitted variable can tell you the sign of the bias.\n\n- Then you can know if the bias is attenuating or increasing the effect you are investigating.\n\n- If attenuating, the problem is smaller than if it is increasing\n\n\n\n\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n**Regressions**\n\n-   The previous examples show that we can run **regressions and find correlations** ...\n\n-   ... And we can run regressions and find **causal effects**.\n\n-   But we need to control for all relevant variables, otherwise we have the *OVB problem*.\n\n-   Should you not look careful to your data, you'd miss the inclusion of the variable `group`.\n\n-   The results show that you may estimate a spurious coefficient twice the size of the \"true\" coefficient.\n\n\n\n\n\n\n\n# Bad Controls Problem {.smaller background=\"#dff2c7\"}\n\n\n## Bad Controls Problem {.smaller background=\"#dff2c7\"}\n\n**Bad controls** are variables that are **also outcome of the treatment** being studied.\n\nA **Bad control** could very well be a **dependent variable** of the treatment as well. \n\n**Good controls** are variables that **you can think as being fixed** at the time of the treatment. \n\n. . .\n\nLet's return to the model.\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\beta_2 Group+ \\epsilon \\;,\\; if\\; i <=5$$ \n\n\nAssuming you also have the occupation of the students at the time of earnings. Should you include `occupation` in the model?\n\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\beta_2 Group + \\beta_3 Occupation + \\epsilon \\;,\\; if\\; i <=5$$ \n\nReasoning: \"*We should use occupation as control because it would be wise to look at the effect of education on earnings only for those within an occupation*\".\n\nWhat is the problem with this reasoning?\n\n\n\n\n\n\n\n\n## Bad Controls Problem {.smaller background=\"#dff2c7\"}\n\nThe problem is that studying in private would increase the chances of getting a white-collar occupation, i.e., *private education (treatment) affects the occupation (bad control)*.\n\nIn this case, should you include occupation as control, the coefficient of interest no longer has a causal interpretation.\n\n\n. . .\n\n**This is a very common problem in empirical research**.\n\nIt is not hard to come up with stories of why a control is a bad control.\n\n\n\n\n\n\n# Randomization {.smaller background=\"#ff9c6b\"}\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n**Now I want to discuss the idea of randomization**\n\nSuppose you have developed a treatment (e.g., a program) that you believe will increase the 'motivation' of employees of a factory.\n\nYou have 100 employees to use in an experiment to test your claim that the treatment will increase motivation.\n\n. . .\n\n- You randomly allocate 50 employees to receive the treatment. The other 50 are part of the control group.\n\n. . .\n\n- You treat all employees in the same manner, except for the treatment.\n\n\n\n. . .\n\nUsing the data available, this is the **difference in motivation between the treatment and control groups (next slide):**\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(dplyr)\ndata  <- read_excel(\"files/part_3_data.xlsx\", range = \"A1:C101\")\n# Box plot control vs treatment groups\nggplot(data, aes(y=motivation, fill=group)) +   \n  geom_boxplot()+\n  theme(plot.title = element_text(color=\"black\", size=30, face=\"bold\"),\n        panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n        axis.text.y = element_text(face=\"bold\", color=\"black\", size = 18),\n        axis.text.x = element_blank(),\n        legend.title = element_blank(),\n        legend.key.size = unit(3, \"cm\"))\n```\n\n::: {.cell-output-display}\n![](part_3_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Read data from Excel file\ndata = pd.read_excel(\"files/part_3_data.xlsx\")\n\n# Create a box plot of control vs treatment groups using seaborn\nplt.figure(figsize=(7, 5))\nsns.set(style='whitegrid')\nsns.boxplot(x='group', y='motivation', data=data, palette='Set2')\nplt.title(\"Box Plot of Control vs Treatment Groups\", fontsize=18)\nplt.xlabel(\"Group\", fontsize=14)\nplt.ylabel(\"Motivation\", fontsize=14)\nplt.show()\n```\n\n::: {.cell-output-display}\n![](part_3_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nimport excel \"files/part_3_data.xlsx\", cellrange(A1:C101) firstrow clear\ngraph box motivation , over(group) box(1, color(black)) \tytitle(\"Motivation\")  \n\nquietly graph export \"files/graph3_5.svg\", replace\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(3 vars, 100 obs)\n```\n\n\n:::\n:::\n\n\n![](files/graph3_5.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\nThe calculated means are below. And they are statistically different.\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\ndata  <- read_excel(\"files/part_3_data.xlsx\", range = \"A1:C101\")\ntapply(data$motivation, data$group, summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Control\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  16.70   19.70   20.70   20.80   22.27   24.60 \n\n$Treatment\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  17.60   20.52   22.50   22.27   23.77   26.50 \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nt.test(motivation ~ group, data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  motivation by group\nt = -3.7301, df = 94.879, p-value = 0.0003258\nalternative hypothesis: true difference in means between group Control and group Treatment is not equal to 0\n95 percent confidence interval:\n -2.2493176 -0.6866824\nsample estimates:\n  mean in group Control mean in group Treatment \n                 20.800                  22.268 \n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\ndata = pd.read_excel(\"files/part_3_data.xlsx\")\ngroup_summary = data.groupby('group')['motivation'].describe()\nprint(group_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           count    mean       std   min     25%   50%     75%   max\ngroup                                                               \nControl     50.0  20.800  1.780392  16.7  19.700  20.7  22.275  24.6\nTreatment   50.0  22.268  2.138800  17.6  20.525  22.5  23.775  26.5\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nimport excel \"files/part_3_data.xlsx\", cellrange(A1:C101) firstrow clear\nbys group  : sum motivation\nestpost ttest motivation , by(group)\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(3 vars, 100 obs)\n\n\n-------------------------------------------------------------------------------\n-> group = Control\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n  motivation |         50        20.8    1.780392       16.7       24.6\n\n-------------------------------------------------------------------------------\n-> group = Treatment\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n  motivation |         50      22.268      2.1388       17.6       26.5\n\n             |      e(b)   e(count)      e(se)       e(t)    e(df_t) \n-------------+-------------------------------------------------------\n  motivation |    -1.468        100   .3935546  -3.730105         98 \n\n             |    e(p_l)       e(p)     e(p_u)     e(N_1)    e(mu_1) \n-------------+-------------------------------------------------------\n  motivation |  .0001604   .0003208   .9998396         50       20.8 \n\n             |    e(N_2)    e(mu_2) \n-------------+----------------------\n  motivation |        50     22.268 \n```\n\n\n:::\n:::\n\n:::\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n**Is there evidence that the program has increased motivation?**\n\n. . .\n\n- well, if you randomly split a group of 100 people into two groups of 50, you certainly wouldn't get the same mean motivation in both groups even if you treated them exactly alike. \n\n- Maybe the difference that we see is just such a difference?\n\n**How can we test this hypothesis?**\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n**Solution**: \n\n- Suppose the treatment had no effect, and the employees developed their motivation  independently of the treatment. \n\n- What is the chance that the 50 employees randomly assigned to the treatment group would have an average at least 1.47 (22.27 - 20.80)  points higher than the average motivation of the employees randomly assigned to the control group?\n\n\n\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n**Steps**\n\n1) Randomly split the 100 employees that we observed in this experiment into two groups of 50.\n\n2) Note the difference in the mean motivation  between the two groups.\n\n3) Repeat 1 and 2 a total of 10,000 times.\n\n4) Note the proportion of times the difference is at least 1.47 (22.27 - 20.80).\n\n\n\n\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# Load necessary libraries\ndata <- read_excel(\"files/part_3_data.xlsx\", range = \"A1:C101\")\ncomb <- 10000\ndf <- data.frame(matrix(ncol = 2, nrow = comb))\ncolnames(df) <- c(\"order\" ,\"diff\")\n# Create the loop for randomization:\nfor (i in seq(from = 1, to = comb)) {\n  set.seed(i)                               \n  data$temp <- runif(100, min = 0, max = 1)  # Creating 100 random numbers 0 to 1\n  data <- data[order(data$temp),]            # Sorting data by the random numbers generated in the previous row\n  data$rank <- rank(data$temp)               # Ranking by the random numbers\n# The row below defines the treatment group based on the random numbers generated. This is where we guarantee randomization\ndata$status_rank <- case_when(data$rank <= 50 ~ \"Control_rand\", data$rank > 50 ~ \"Treated_rand\")\n# Calculate the new means of the new groups. Need to transpose data.\nmeans <- t(as.data.frame(tapply(data$motivation, data$status_rank, mean)))\n# Moving the new means to df. Each row is the difference of means\ndf[i, 1] <- i\ndf[i, 2] <- means[1, 2] - means[1, 1]\nrm(means) # Deleting value\ndata = subset(data, select = -c(temp, rank, status_rank)) # Deleting variables\n}\n# Calculate a suitable binwidth for the histogram\nbinwidth <- (max(df$diff) - min(df$diff)) / sqrt(length(df$diff))\n# Create a histogram of the differences with the calculated binwidth\nggplot(df, aes(x = diff)) +\n  geom_histogram(binwidth = binwidth, fill = \"blue\", color = \"black\") +\n  labs(title = \"Distribution of Differences\", x = \"Difference\", y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](part_3_files/figure-revealjs/unnamed-chunk-22-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nimport excel \"files/part_3_data.xlsx\", cellrange(A1:C101) firstrow clear\nset seed 472195 \t\t\nsort group\t\t \nset obs 10000 \t\t\t\t \negen fin_order = seq() \nsort fin_order \t\t\t\t \nsummarize \t\t\t\t\t\ngen av_diff=.\n\nlocal i = 1\nwhile `i'<=10000 {\n\n\tsort fin_order\n\tgen rand_num`i' = uniform() if !missing(motivation)\n\tegen ordering`i' = rank(rand_num`i')\n\tsort ordering`i'\n\n\tgen group`i' = \"\"\n\treplace group`i' = \"T\" if ordering <= 50\n\treplace group`i' = \"C\" if ordering > 50 & ordering<=100\n\t\n\tqui summ motivation if group`i'==\"T\"\n\tscalar avT = `r(mean)'\n\tqui summ motivation if group`i'==\"C\"\n\tscalar avC = `r(mean)'\n\t\n\tsort fin_order\n\treplace av_diff = avT-avC in `i'\n\t\n\tdrop rand_num`i' ordering`i' group`i'\n\tlocal i = `i' + 1\n}\nhistogram av_diff, frequency kdensity  \ngraph export \"files/graph3_6.png\" , replace\n\n```\n:::\n\n\n![](files/graph3_6.png){ width=800px height=450px }\n:::\n\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\nThe mean difference was as far from 0 as 1.5 for only a few out of the 10,000 random divisions of the data into two groups of 50.\n\n- Thus, **the difference between the mean motivation would almost always be less than the observed difference of 1.47 (22.27 - 20.80) if the treatment had no effect.**\n\n- It seems reasonable to believe that the treatment caused the difference in motivation.\n\n\n\n\n\n\n\n\n\n# Measurement Error problem  {.smaller background=\"#f2e9b6\"}\n\n\n## Measurement Error problem  {.smaller background=\"#f2e9b6\"}\n\nThe measurement error problem has a similar statistical structure to the omitted variable bias (OVB).\n\n- \"Classical\" random measurement error for the $y$ will inflate standard errors but will not lead to biased coefficients. \n\n    - $y^{*} = y + \\sigma_{1}$\n    - If you estimante $y^{*} = f(x)$, you have $y + \\sigma_{1} = x + \\epsilon$ \n    - $y = x + u$ \n        - where $u = \\epsilon - \\sigma_{1}$ \n\n\n\n\n## Measurement Error problem  {.smaller background=\"#f2e9b6\"}\n\n- \"Classical‚Äù random measurement error in x‚Äôs will bias coefficient estimates toward zero.\n\n- $x^*=x+\\sigma_2$\n\n- Imagine that $x^*$ is a bunch of noise. It would not explain anything. Thus, your results are biased toward zero.\n\n\n\n\n\n## Measurement Error problem  {.smaller background=\"#f2e9b6\"}\n\nA example using one of the Wooldridge's datasets.\n\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(foreign) \nlibrary(jtools)\ndata <- read.dta(\"files/CEOSAL1.dta\")\nset.seed(2)\ndata$salary_noise <- data$salary + runif(length((data$salary)), min=-100, max= 100)\ndata$roe_noise <- data$roe + runif(length((data$roe)), min=-100, max= 100)\n# OLS model \nmodel1 <- lm(data$salary ~ data$roe)\nmodel2 <- lm(data$salary ~ data$roe_noise)\nmodel3 <- lm(data$salary_noise ~ data$roe)\n#summary(model1)\n#summary(model2)\n#summary(model3)\nexport_summs(model1, model2, model3, digits = 3 , model.names = c(\"Roe\", \"Roe (X) with noise\", \"Salary (y) with noise\") )\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<table class=\"huxtable\" style=\"border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  \" id=\"tab:unnamed-chunk-24\">\n<col><col><col><col><tr>\n<th style=\"vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.8pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\"></th><th style=\"vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.8pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">Roe</th><th style=\"vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.8pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">Roe (X) with noise</th><th style=\"vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.8pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">Salary (y) with noise</th></tr>\n<tr>\n<th style=\"vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">(Intercept)</th><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">963.191 ***</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">1269.739 ***</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">964.058 ***</td></tr>\n<tr>\n<th style=\"vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\"></th><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">(213.240)&nbsp;&nbsp;&nbsp;</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">(97.356)&nbsp;&nbsp;&nbsp;</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">(214.588)&nbsp;&nbsp;&nbsp;</td></tr>\n<tr>\n<th style=\"vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">data$roe</th><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">18.501&nbsp;&nbsp;&nbsp;&nbsp;</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">18.318&nbsp;&nbsp;&nbsp;&nbsp;</td></tr>\n<tr>\n<th style=\"vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\"></th><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">(11.123)&nbsp;&nbsp;&nbsp;</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">(11.194)&nbsp;&nbsp;&nbsp;</td></tr>\n<tr>\n<th style=\"vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">data$roe_noise</th><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">0.868&nbsp;&nbsp;&nbsp;&nbsp;</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td></tr>\n<tr>\n<th style=\"vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\"></th><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">(1.593)&nbsp;&nbsp;&nbsp;</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td></tr>\n<tr>\n<th style=\"vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">N</th><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">209&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">209&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">209&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td></tr>\n<tr>\n<th style=\"vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.8pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">R2</th><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.8pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">0.013&nbsp;&nbsp;&nbsp;&nbsp;</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.8pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">0.001&nbsp;&nbsp;&nbsp;&nbsp;</td><td style=\"vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.8pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\">0.013&nbsp;&nbsp;&nbsp;&nbsp;</td></tr>\n<tr>\n<th colspan=\"4\" style=\"vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.8pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal;\"> *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.</th></tr>\n</table>\n\n```\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.iolib.summary2 import summary_col\n\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\nnp.random.seed(2)\n# Add noise to the 'salary' and 'roe' columns\ndata['salary_noise'] = data['salary'] + np.random.uniform(-100, 100, len(data))\ndata['roe_noise'] = data['roe'] + np.random.uniform(-100, 100, len(data))\n# OLS model\nmodel1 = smf.ols(formula='salary ~ roe', data=data).fit()\nmodel2 = smf.ols(formula='salary ~ roe_noise', data=data).fit()\nmodel3 = smf.ols(formula='salary_noise ~ roe', data=data).fit()\n# Create a summary table for all regressions\nresults = summary_col([model1, model2, model3], \n                      model_names=['Reg 1', 'Reg 2', 'Reg 3'],\n                      stars=True,\n                      float_format='%0.2f')\n# Print the summary table\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=============================================\n                 Reg 1     Reg 2      Reg 3  \n---------------------------------------------\nIntercept      963.19*** 1262.12*** 966.97***\n               (213.24)  (98.19)    (213.37) \nR-squared      0.01      0.00       0.01     \nR-squared Adj. 0.01      -0.00      0.01     \nroe            18.50*               17.92    \n               (11.12)              (11.13)  \nroe_noise                1.25                \n                         (1.63)              \n=============================================\nStandard errors in parentheses.\n* p<.1, ** p<.05, ***p<.01\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nuse \"files/CEOSAL1.dta\", clear\nset seed 2\ngen salary_noise = salary + runiform() * 200 - 100\ngen roe_noise = roe + runiform() * 200 - 100\neststo: qui reg salary roe\neststo: qui reg salary roe_noise\neststo: qui reg salary_noise roe\nesttab\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(est1 stored)\n\n(est2 stored)\n\n(est3 stored)\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                   salary          salary    salary_noise   \n------------------------------------------------------------\nroe                 18.50                           18.14   \n                   (1.66)                          (1.63)   \n\nroe_noise                          0.0336                   \n                                   (0.02)                   \n\n_cons               963.2***       1280.4***        966.1***\n                   (4.52)         (12.71)          (4.53)   \n------------------------------------------------------------\nN                     209             209             209   \n------------------------------------------------------------\nt statistics in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## üôã‚Äç‚ôÇÔ∏è **Any Questions?**{ .smaller  background=\"#fdf6e3\"}\n\n::: columns\n::: {.column width=\"50%\"}\n### Thank You!\n\n![](figs/qa2.png){width=\"110%\" style=\"box-shadow: none;\"}\n\n:::\n\n::: {.column width=\"50%\"}\n<div style=\"text-align:right;\">\n  <img src=\"figs/avatar.jpg\" width=\"120px\" style=\"border-radius:50%; box-shadow:0 4px 12px rgba(0,0,0,.25);\" />\n</div>\n\n### **Henrique C. Martins**\n\n- üåê [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)  \n- üíº [LinkedIn](https://www.linkedin.com/in/henriquecastror/)  \n- üß† [Google Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)  \n- üìÑ [Lattes CV](http://lattes.cnpq.br/6076997472159785)  \n- üè† [Personal Website](https://henriquemartins.net/)  \n:::\n:::\n\n",
    "supporting": [
      "part_3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}