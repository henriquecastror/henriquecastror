{"title":"Empirical Methods in Finance","markdown":{"yaml":{"title":"Empirical Methods in Finance","subtitle":"Part 4","author":"Henrique C. Martins","format":{"revealjs":{"slide-number":true,"theme":"simple","chalkboard":true,"preview-links":"auto","logo":"figs/background8.png","css":"logo.css","footer":"**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  ","multiplex":true,"scrollable":true}},"title-slide-attributes":{"data-background-color":"#b1cafa"},"include-after":"<script type=\"text/javascript\">\n  Reveal.on('ready', event => {\n    if (event.indexh === 0) {\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n  });\n  Reveal.addEventListener('slidechanged', (event) => {\n    if (event.indexh === 0) {\n      Reveal.configure({ slideNumber: null });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n    if (event.indexh === 1) {\n      Reveal.configure({ slideNumber: 'c' });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n    }\n  });\n</script>\n"},"headingText":"library(reticulate)","containsRefs":false,"markdown":"\n\n\n\n```{r setup}\n#| include: false\n#| warning: false\n\n\n# use_python(\"C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe\")\nlibrary(reticulate)\nlibrary(Statamarkdown)\n#reticulate::py_install(\"matplotlib\")\n#reticulate::py_install(\"seaborn\")\n#reticulate::py_install(\"pyfinance\")\n#reticulate::py_install(\"xlrd\")\n#reticulate::py_install(\"quandl\")\n\n```\n\n\n\n\n\n\n# More about Regressions {.smaller background=\"#454343\"}\n\n## Regression {.smaller background=\"#454343\"}\n\nLinear regressions are the workhorse tool in econometrics\n\n-   **Simplicity**: straightforward to understand, implement, and visualize.\n\n. . .\n\n-   **Interpretability**: coefficients have clear interpretations.\n    -   represents the change in the Y for a one-unit change in the X, holding all other variables constant.\n\n. . .\n\n-   **Versatility**: simple linear regression or *multiple linear regression*.\n\n. . .\n\n-   **Assumptions**: linearity, independence of errors, homoscedasticity, and normality of errors.\n\n. . .\n\n-   **Baseline Model**: You can compare the performance of more advanced models to linear regression.\n\n. . .\n\n-   **Estimation**: provides clear estimates of the coefficients' confidence intervals and hypothesis testing.\n\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\nIn this setting, the variables $y$ and $x$ can have several names.\n\n| Y                  | X                    |\n|--------------------|----------------------|\n| Dependent variable | Independent variable |\n| Explained variable | Explanatory variable |\n| Response variable  | Control variable     |\n| Predicted variable | Predictor variable   |\n| Regressand         | Regressor            |\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\nBroadly, we are interested in how y is explained by x?\n\n-   $y_i = \\alpha + \\beta_1 x_i + \\epsilon$\n\n. . .\n\nPerhaps $\\epsilon$ is the most important part of a regression.\n\nThe interpretation is \"*everything that is not explained by X and that explains Y*\".\n\n. . .\n\nA comment\n\n-   Usually, the literature uses $\\epsilon$ for the \"estimated\" residual.\n\n-   And $\\mu$ for the \"true\" residual, which necessarily implies that the assumptions hold.\n\n-   At the end od the day, you don't need to worry to much with the notation of this term because we are always in the \"estimated world\", and almost never in the \"true world\".\n\n-   The \"true world\" implies that you are studying the population or that you have a true random sample of the population\n\n    -   $y_i = \\alpha + \\beta_1 x_i + \\mu$\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\n**Remember**\n\n- $y, x$, and $\\mu$ are random variables\n- $y and x$ are observable\n- $\\mu$ and $\\beta$ are unobservable\n- $\\mu$ captures everything that determines y after accounting for x \n\nOur goal is to estimate Œ≤\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\nThere are some assumptions/requirements about $\\mu$ in a OLS\n\n**First assumption**\n\n-   E($\\mu$) = 0\n\n    -   This is a simple assumption, not strong at all.\n    -   It simply assumes that the average of $\\mu$ is zero in the population.\n    -   Basically, any non-zero mean is absorbed by the intercept\n        -   Say that E($\\mu$) = k\n        -   We could rewrite $\\mu = k + w$, where E(w)=0\n        -   Then, model becomes $y=(\\alpha +ùëò) + \\betaùë•+ùë§$\n        -   Intercept is now just (Œ± + k), and error, w, is mean zero\n\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\n**Second assumption**\n\n-   E($\\mu$\\|x) = E($\\mu$) for all values of x\n    -   It says that the average value of $\\mu$ does not depend on the value of x (i.e., the slice of the population you are looking at).\n    -   We say that $\\mu$ is mean-independent of x.\n    -   This is true if the X and the $\\mu$ are independent to each other.\n    -   Implies that x and $\\mu$ are *uncorrelated*.\n    -   **Conditional Mean Independence (CMI)**.\n    -   This is one of the keys assumption to *causal inference*.\n\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\n**Second assumption**\n\n**Example**\n\nLet's say the model is:\n\n$$wage = \\alpha + \\beta Schooling_{years} + \\epsilon$$\n\n-   where $\\epsilon$ represents *unobserved ability*.\n\nDoes CMI hold?\n\nThat is E(ability\\|x=8)=E(ability\\|x=10)=E(ability\\|x=12)?\n\n. . .\n\n**Probably not**, because the unobserved ability should depend on the years of schooling.\n\nThe solution (not trivial) would be to include ability as a new X.\n\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\n**Another example**\n\nConsider the following model (with only one X)\n\n$$Leverage_i = \\alpha + \\beta_1 Profitability_i + \\mu_i$$\n\n-   CMI says that, to every firm *i*, $\\mu$ is the same, even when firms have different profitability.\n\n-   Can you think on examples when this assumption may not hold in this model?\n\n. . .\n\n1)  unprofitable firms have higher bankruptcy risk, which should make them to have lower leverage (tradeoff theory).\n\n2)  unprofitable firms have low cash, which should make them to have more leverage (pecking order theory).\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\n**The discussion of whether the CMI holds is the origin of the \"endogeneity\" problem.**\n\nYou will face reviewers arguing reasons of why the CMI might not hold in your case.\n\n- Many  will criticize a model by saying it has an ‚Äúendogeneity problem‚Äù, whithout explaining more.\n\n  - This is very generic. **Don't do that!**\n\n. . .\n\nThey should explain what is the source of the problem that is making the model violate CMI.\n\n - OVB, selection bias, reverse causality, simultaneity, etc?\n\n. . . \n\nGenerally speaking, endogeneity refers to a violation of CMI, meaning that $x$ and $\\mu$ are correlated.\n\nThis is always a plausible possibility, since $\\mu$ carries a lot of stuff (something must be correlated with X).\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\n**Third assumption**\n\nCombining 1 and 2 leads to\n\n-   E($\\mu$\\|x) = 0\n    -   This is a very important assumption called **zero conditional mean assumption**.\n\n## Ordinary Least Squares {.smaller background=\"#454343\"}\n\nOur focus is to find estimates for $\\alpha$ and $\\beta$. Should we have access to the population, it would be easy. We could write:\n\n$$y_i= \\alpha + \\beta_1x_i + \\mu$$\n\n. . .\n\nBut remember that,\n\n$$E(u) = 0$$ $$E(u|x) = 0$$\n\nThe second bullet implies that the correlation between x and $\\mu$ is zero (we can write that as E(x,u) = 0).\n\n. . .\n\n::: callout-important\nRemember: $\\alpha$ and $\\beta$ are parameters to be estimated (i.e., constants), while $X$ and $Y$ are variables\n:::\n\n\n\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\nSo we can write that (in the **population**)\n\n$E(y - \\alpha - \\beta_1x ) = 0$\n\n$(x[y - \\alpha - \\beta_1x ]) = 0$\n\n. . .\n\nSo we can write that (in the **sample**)\n\n$\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{\\alpha} - \\hat{\\beta_1} x_i ) = 0$ , We will use this to find $\\alpha$:\n\n$\\frac{1}{n} \\sum_{i=1}^n (x_i [y_i - \\hat{\\alpha} - \\hat{\\beta_1} x_i ]) = 0$ , We will use this to find $\\beta$:\n\n\n\n\n\n\n\n## Finding $\\alpha$ {.smaller background=\"#454343\"}\n\nFrom before:\n\n$$\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{\\alpha} - \\hat{\\beta_1} x_i ) =0$$\n\n. . .\n\nPassing the sum operator through\n\n$$\\frac{1}{n}\\sum_{i=1}^n(y_i) - \\frac{1}{n}\\sum_{i=1}^n(\\hat{\\alpha})  - \\frac{1}{n} \\sum_{i=1}^n(\\hat{\\beta_1} x_i )=0$$\n\n. . .\n\nCoefficients are constants, so we can get rid of the sum operator.\n\n$$\\frac{1}{n}\\sum_{i=1}^n(y_i) - \\hat{\\alpha}  - \\hat{\\beta_1} \\frac{1}{n}  \\sum_{i=1}^n(\\ x_i )=0$$\n\n\n\n\n\n\n\n\n## Finding $\\alpha$ {.smaller background=\"#454343\"}\n\n-   We know that $\\frac{1}{n}\\sum_{i=1}^n(y_i)$ is $\\bar{y_i}$ (the mean)\n\n$$\\bar{y_i} - \\hat{\\alpha}  - \\hat{\\beta_1}  \\bar{x_i}=0$$\n\n. . .\n\nSo we write:\n\n$$\\hat{\\alpha}  = \\bar{y_i} -  \\hat{\\beta_1}   \\bar{x_i}$$\n\n. . .\n\n**Easy Peasy** üòÄ\n\n\n\n\n\n## Finding $\\beta$ {.smaller background=\"#454343\"}\n\nFrom before:\n\n$$\\frac{1}{n} \\sum_{i=1}^n (x_i [y_i - \\hat{\\alpha} - \\hat{\\beta_1} x_i ]) = 0$$\n\n. . .\n\nBut now we have:\n\n$$\\hat{\\alpha}  = \\bar{y_i} -  \\hat{\\beta_1}   \\bar{x_i}$$\n\n. . .\n\nThus,\n\n$$\\frac{1}{n} \\sum_{i=1}^n (x_i [y_i - (\\bar{y_i} -  \\hat{\\beta_1}   \\bar{x_i}) - \\hat{\\beta_1} x_i ]) = 0$$\n\n\n\n\n## Finding $\\beta$ {.smaller background=\"#454343\"}\n\nTurning into\n\n$$\\frac{1}{n} \\sum_{i=1}^n (x_i [y_i - \\bar{y_i} ])  -  \\frac{1}{n} \\sum_{i=1}^n (x_i [\\hat{\\beta_1} x_i - \\hat{\\beta_1} \\bar{x_i} ]) = 0$$\n\n. . .\n\nOr\n\n$$\\frac{1}{n} \\sum_{i=1}^n (x_i [y_i - \\bar{y_i} ])  =  \\hat{\\beta_1} \\frac{1}{n} \\sum_{i=1}^n (x_i [ x_i  - \\bar{x_i} ]) $$\n\n. . . \n\nLast step (I am skipping some steps here)\n\n$$\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x} )(y_i - \\bar{y_i} )  =  \\hat{\\beta_1} \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x_i} )^2 $$\n\n\n\n\n## Finding $\\beta$ {.smaller background=\"#454343\"}\n\n\nIf the variance of x is not zero, we can write $\\beta$ as\n\n$$\\hat{\\beta_1} = \\frac{\\sum_i^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_i^n (x_i - \\bar{x})^2} = \\frac{Covariance(x_i,y_i)}{Variance(x_i)}$$\n\n\n\n## Finding $\\mu$  {.smaller background=\"#454343\"}\n\nNow that we have $\\hat{Y_i}=\\hat{\\alpha} + \\hat{\\beta_1} X_i$, we can estimate the residual $\\mu$\n\n$$\\hat{\\mu} = Y_i - \\hat{Y_i}$$\n\n\nWhich is the same as:\n\n$$\\hat{\\mu} = Y_i - \\hat{\\alpha} - \\hat{\\beta_1}x_i$$\n\nMost residuals will not be 0, meaning they do not lie on the best fitting line. \n\n\n\n\n\n\n## Finding $\\mu$  {.smaller background=\"#454343\"}\n\nThe job of an OLS model is find the parameters to minimize the squared error (i.e., to find the best fitting line).\n\n$$\\sum_{i=1}^n \\hat{\\mu}^2 = \\sum_{i=1}^n(Y_i - \\hat{Y_i})^2$$\n\n\n\n\n\n\n\n\n\n\n\n\n## Regression ([Source](https://mixtape.scunning.com/02-probability_and_regression#ordinary-least-squares)) {.smaller background=\"#454343\"}\n\nAnother example of regression. The differences in the coefficients are due to the differences in the seed of the random variables generator.\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(tidyverse)\nset.seed(1)\ntb <- tibble(\n  x = rnorm(10000),\n  u = rnorm(10000),\n  y = 5.5*x + 12*u # notice that I am defining the beta1 here. The 5.5 is the \"true\" beta we want to estimate.\n) \nreg_tb <- lm(y ~ x, data=tb) \nsummary(reg_tb)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nnp.random.seed(1)\n\nobs = 10000\ndata = pd.DataFrame({\n    'x': np.random.normal(size=obs),\n    'u': np.random.normal(size=obs),\n})\ndata['y'] = 5.5 * data['x'] + 12 * data['u']\n\nX = sm.add_constant(data['x'])\nmodel = sm.OLS(data['y'], X).fit()\n\nprint(model.summary())\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nset seed 1 \nset obs 10000 \ngen x = rnormal() \ngen u  = rnormal() \ngen y  = 5.5*x + 12*u \nreg y x \n```\n:::\n\n\n\n\n\n\n## Regression ([Source](https://mixtape.scunning.com/02-probability_and_regression#ordinary-least-squares)) {.smaller background=\"#454343\"}\n\nAnother example of regression. The differences in the coefficients are due to the differences in the seed of the random variables generator.\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(tidyverse)\nset.seed(1)\ntb <- tibble(\n  x = rnorm(10000),\n  u = rnorm(10000),\n  y = 5.5*x + 12*u # notice that I am defining the beta1 here. The 5.5 is the \"true\" beta we want to estimate.\n) \nreg_tb <- lm(y ~ x, data=tb) \ntb %>% \n  lm(y ~ x, .) %>% \n  ggplot(aes(x=x, y=y)) + \n  ggtitle(\"OLS Regression Line\") +\n  geom_point(size = 0.05, color = \"black\", alpha = 0.5) +\n  geom_smooth(method = lm, color = \"black\") +\n  annotate(\"text\", x = -1.5, y = 30, color = \"red\", \n           label = paste(\"Intercept = \", reg_tb$coefficients[1])) +\n  annotate(\"text\", x = 1.5, y = -30, color = \"blue\", \n           label = paste(\"Slope =\", reg_tb$coefficients[2]))\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a scatterplot with the OLS regression line using Seaborn\nsns.set(style='whitegrid')\nplt.figure(figsize=(7, 5))\nsns.scatterplot(x='x', y='y', data=data, color='black', alpha=0.5, s=5)\nsns.regplot(x='x', y='y', data=data, color='black', scatter=False, line_kws={'color':'black'})\nplt.title('OLS Regression Line')\nplt.annotate(f'Intercept = {model.params[0]:.2f}', xy=(-1.5, 30), color='red')\nplt.annotate(f'Slope = {model.params[1]:.2f}', xy=(1.5, -30), color='blue')\nplt.show()\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nset seed 1 \nset obs 10000 \ngen x = rnormal() \ngen u  = rnormal() \ngen y  = 5.5*x + 12*u \nreg y x \npredict yhat1 \ngen yhat2 = -0.0750109  + 5.598296*x \npredict uhat1, residual \ngen uhat2=y-yhat2 \nqui sum uhat* \ntwoway (lfit y x, lcolor(black) lwidth(medium)) (scatter y x, mcolor(black) msize(tiny) msymbol(point)), title(OLS Regression Line) \nqui graph export \"files/graph3.svg\", replace\n```  \n\n![](files/graph3.svg) \n\n:::\n\n\n\n\n\n\n## Regression ([Source](https://mixtape.scunning.com/02-probability_and_regression#ordinary-least-squares)) {.smaller background=\"#454343\"}\n\nUsing the previous regressions, we can show  that:\n\n1)  $\\hat{y_i} = \\hat{\\alpha} + \\hat{\\beta_1} x_i$\n\n2)  $\\hat{\\mu_i} = y_i - \\hat{y_i}$\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n\nlibrary(tidyverse)\nset.seed(1)\ntb <- tibble(\n  x = rnorm(10000),\n  u = rnorm(10000),\n  y = 5.5*x + 12*u # notice that I am defining the beta1 here. The 5.5 is the \"true\" beta we want to estimate.\n) \nreg_tb <- lm(y ~ x, data=tb) \n\ntb <- tb %>% \n  mutate(\n    yhat1 = predict(lm(y ~ x, .)),\n    yhat2 = reg_tb$coefficients[1] + reg_tb$coefficients[2]*x, \n    uhat1 = residuals(lm(y ~ x, .)),\n    uhat2 = y - yhat2\n  )\nsummary(tb[-1:-3])\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nnp.random.seed(1)\nobs = 10000\nx = np.random.normal(size=obs)\nu = np.random.normal(size=obs)\ny = 5.5 * x + 12 * u\n\nX = sm.add_constant(x)\nmodel = sm.OLS(y, X).fit()\n\ntb = pd.DataFrame({'x': x, 'u': u, 'y': y})\ntb['yhat1'] = model.predict(X)\ntb['uhat1'] = y - tb['yhat1']\ntb['yhat2'] = model.params[0] + model.params[1] * x\ntb['uhat2'] = y - tb['yhat2']\n\nprint(tb[['yhat1','yhat2', 'uhat1','uhat2']].describe())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nset seed 1 \nclear \nqui set obs 10000 \ngen x = rnormal() \ngen u  = rnormal() \ngen y  = 5.5*x + 12*u \nqui reg y x \npredict uhat1, residual \npredict yhat1 \ngen yhat2 = -0.0750109  + 5.598296*x \ngen uhat2=y-yhat2 \nsum yhat*  uhat* \n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n# Properties of OLS {.smaller background=\"#bfc4d9\"}\n\n\n## Properties of OLS {.smaller background=\"#bfc4d9\"}\n\nWe can easily show that (remember from before) \n\n$$\\sum_i^n(y_i - \\hat{\\alpha} - \\hat{\\beta_1} x_i) = 0$$\n\nAnd that \n\n$$\\sum_i^n \\hat{\\mu}  = 0$$\n\nThe graphs next slide shows a spherical figure, suggesting that the residual is not correlated with the the fitted values ($\\hat{y_i}$)\n\n\n\n\n\n\n\n## Properties of OLS {.smaller background=\"#bfc4d9\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(tidyverse)\nset.seed(1)\ntb <- tibble(\n  x = rnorm(10000),\n  u = rnorm(10000),\n  y = 5.5*x + 12*u # notice that I am defining the beta1 here. The 5.5 is the \"true\" beta we want to estimate.\n) \nreg_tb <- lm(y ~ x, data=tb) \ntb <- tb %>% \n  mutate(\n    yhat1 = predict(lm(y ~ x, .)),\n    yhat2 = reg_tb$coefficients[1] + reg_tb$coefficients[2]*x, \n    uhat1 = residuals(lm(y ~ x, .)),\n    uhat2 = y - yhat2\n  )\ntb %>% \n  lm(uhat1 ~ yhat1 , .) %>% \n  ggplot(aes(x=yhat1, y=uhat1)) + \n  geom_point(size = 0.1, color = \"black\") +\n  geom_smooth(method = lm, color = \"black\")\n```\n\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nnp.random.seed(1)\nobs = 10000\nx = np.random.normal(size=obs)\nu = np.random.normal(size=obs)\ny = 5.5 * x + 12 * u\n\nX = sm.add_constant(x)\nmodel = sm.OLS(y, X).fit()\n\ntb = pd.DataFrame({'x': x, 'u': u, 'y': y})\ntb['yhat1'] = model.predict(X)\ntb['uhat1'] = y - tb['yhat1']\ntb['yhat2'] = model.params[0] + model.params[1] * x\ntb['uhat2'] = y - tb['yhat2']\nmodel = sm.OLS(tb['uhat1'], sm.add_constant(tb['yhat1'])).fit()\n# Create a scatter plot with a regression line\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(7, 4.5))\nsns.scatterplot(x='yhat1', y='uhat1', data=tb, size=0.05, color='black', alpha=0.5)\nsns.regplot(x='yhat1', y='uhat1', data=tb, scatter=False, color='black')\nplt.xlabel('yhat1')\nplt.ylabel('uhat1')\nplt.title('Scatter Plot of uhat1 vs. yhat1')\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nset seed 1 \nset obs 10000 \ngen x = rnormal() \ngen u  = rnormal() \ngen y  = 5.5*x + 12*u \nqui reg y x \npredict yhat1 \npredict uhat1, residual \ntwoway (lfit uhat1 yhat1 , lcolor(black) lwidth(large)) (scatter uhat1 yhat1 , mcolor(black)  msymbol(point))\nqui graph export \"files/graph4.svg\", replace\n```  \n\n![](files/graph4.svg) \n\n:::\n\n\n\n\n\n\n\n\n\n\n## Properties of OLS {.smaller background=\"#bfc4d9\"}\n\nSimilarly, we can easily show that \n\n$$\\sum_i^nx_i(y_i - \\hat{\\alpha} - \\hat{\\beta_1} x_i) = 0$$\n \nAnd that\n \n$$\\sum_i^nx_i\\hat{\\mu}  = 0$$\n\n \nMeaning that the sample covariance between the X and the residual will be always zero.\n\n\n\n\n\n\n\n\n\n## Properties of OLS {.smaller background=\"#bfc4d9\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(tidyverse)\nset.seed(1)\ntb <- tibble(\n  x = rnorm(10000),\n  u = rnorm(10000),\n  y = 5.5*x + 12*u # notice that I am defining the beta1 here. The 5.5 is the \"true\" beta we want to estimate.\n) \nreg_tb <- lm(y ~ x, data=tb) \ntb <- tb %>% \n  mutate(\n    yhat1 = predict(lm(y ~ x, .)),\n    yhat2 = reg_tb$coefficients[1] + reg_tb$coefficients[2]*x, \n    uhat1 = residuals(lm(y ~ x, .)),\n    uhat2 = y - yhat2\n  )\ntb %>% \n  lm(uhat1 ~ x , .) %>% \n  ggplot(aes(x=x, y=uhat1)) + \n  geom_point(size = 0.1, color = \"black\") +\n  geom_smooth(method = lm, color = \"black\")\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nnp.random.seed(1)\nobs = 10000\nx = np.random.normal(size=obs)\nu = np.random.normal(size=obs)\ny = 5.5 * x + 12 * u\n\nX = sm.add_constant(x)\nmodel = sm.OLS(y, X).fit()\n\ntb = pd.DataFrame({'x': x, 'u': u, 'y': y})\ntb['yhat1'] = model.predict(X)\ntb['uhat1'] = y - tb['yhat1']\ntb['yhat2'] = model.params[0] + model.params[1] * x\ntb['uhat2'] = y - tb['yhat2']\nmodel = sm.OLS(tb['uhat1'], sm.add_constant(tb['yhat1'])).fit()\n# Create a scatter plot with a regression line\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(7, 4.5))\nsns.scatterplot(x='x', y='uhat1', data=tb, size=0.05, color='black', alpha=0.5)\nsns.regplot(x='x', y='uhat1', data=tb, scatter=False, color='black')\nplt.xlabel('x')\nplt.ylabel('uhat1')\nplt.title('Scatter Plot of uhat1 vs. x')\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nset seed 1 \nset obs 10000 \ngen x = rnormal() \ngen u  = rnormal() \ngen y  = 5.5*x + 12*u \nqui reg y x \npredict yhat1 \npredict uhat1, residual \ntwoway (lfit uhat1 x , lcolor(black) lwidth(large)) (scatter uhat1 x , mcolor(black) msymbol(point))\nqui graph export \"files/graph5.svg\", replace\n```  \n\n![](files/graph5.svg) \n\n:::\n\n\n## Properties of OLS {.smaller background=\"#bfc4d9\"}\n\nLet's say you estimate a model and find the $\\hat{\\mu}$.\n\nIf you calculate the correlation between the X and $\\hat{\\mu}$, you will find zero.\n\n**This is by construction!** It is not an evidence that CMI is nos violated.\n\nIn fact, the OLS \"assumes\" and \"forces\" zero correlation.\n\n**It is intuitive: if you are \"forcing\" zero correlation when the correlation is not in fact zero, your coefficients will be biased.**\n\nThe previous graphs actually show zero correlation. But that is expected and does not suggest the model is not violating CMI.\n\n**At the end of the day, CMI is untestable and unverifiable**.\n\n\n\n\n\n\n\n\n\n# Goodness-of-fit {.smaller background=\"#dff2c7\"}\n\n## Goodness-of-fit {.smaller background=\"#dff2c7\"}\n\n**Understanding what SSR, SSE and SST mean** \n\n- SSE = Sum of Squares Explained = $\\sum_i^n(\\hat{y_i}-\\bar{y})^2$\n- SSR = Sum of Squares Residuals = $\\sum_i^n\\hat{\\mu}^2$\n- SST = Sum of Squares Total = SSE + SSR = $\\sum_i^n(y_i-\\hat{y_i})^2$ \n\n\nR-squared is simply the ratio of portion explained over the total that could be explained.\n\n\n$$R^2 = \\frac{SSE}{SST} = 1-\\frac{SSR}{SST}$$\n\n\n\n## Goodness-of-fit {.smaller background=\"#dff2c7\"}\n\n![](figs/R2.jpg)\n\n\n\n\n\n## Goodness-of-fit {.smaller background=\"#dff2c7\"}\n\nYou can think this way:\n\n1) If X does not explain Y, then the best predictor of Y is $\\bar{y}$. In that case, your model does not explain anything of Y, thus $R^2$ is zero, and $\\hat{y_i}=\\bar{y}$\n\n. . .\n\n2) If X partially explains Y, then $\\hat{y_i} \\neq \\bar{y}$, meaning that $\\hat{y_i}$ has some inclination (like the figure next slide). This means that $SSE>0$ and your $R^2>0$ but $R^2<1$\n\n. . .\n\n3) Whatever is not explained by $\\hat{y_i}$ is left to $\\sum_i^2\\hat{\\mu}^2$, meaning that SSR will be non-zero.\n\n. . .\n\n4) The ratio of the portion that you can explain by  $\\hat{y_i}$ over the total that is to be explained  $y_i-\\hat{y_i}$ if the $R^2$.\n\n\n\n\n\n\n## Goodness-of-fit {.smaller background=\"#dff2c7\"}\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) # importing dataset from a stata dta file\ndata <- read.dta(\"files/CEOSAL1.dta\")\nattach(data)\n# Statistics of salary \nmean(salary)\n# OLS model\nmodel <- lm(salary ~ roe)\nsalaryhat <- fitted(model)                      # Predict values for dependent variable\nuhat <- resid(model)                            # Predict regression residuals\nsalarymean <- rep(mean(salary),length(salary))  # Generating the mean of salary \nsummary(model)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\nprint(data['salary'].mean())\n# OLS model\nX = data['roe']\nX = sm.add_constant(X)\ny = data['salary']\n\nmodel = sm.OLS(y, X).fit()  # Fit the linear regression model\nsalaryhat = model.fittedvalues  # Predicted values for the dependent variable\nuhat = model.resid  # Predict regression residuals\nsalarymean = pd.Series([y.mean()] * len(y))  # Generating the mean of salary\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nsum salary \nreg salary roe \npredict salaryhat , xb\t\t\t\t\npredict uhat, resid\t\t\t\t\t\negen salarymean = mean(salary)\t\t\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n## Goodness-of-fit {.smaller background=\"#dff2c7\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) # importing dataset from a stata dta file\nmydata <- read.dta(\"files/CEOSAL1.dta\")\nattach(mydata)\nmodel <- lm(salary ~ roe)\nsalaryhat <- fitted(model)                      # Predict values for dependent variable\nuhat <- resid(model)                            # Predict regression residuals\nsalarymean <- rep(mean(salary),length(salary))  # Generating the mean of salary \n# r-squared is simply the ratio of portion explained over total that could be explained - Understanding what SSR, SSE and SST mean \nplot(salary ~ roe)\nabline(lm(salary ~ roe), col = \"blue\")\nabline(lm(salarymean ~ roe), col = \"red\")\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\nX = data[['roe']]\ny = data['salary']\nsalarymean = np.repeat(y.mean(), len(y))\nX_mean = X.mean()\ny_mean = y.mean()\nslope = np.sum((X - X_mean) * (y - y_mean)) / np.sum((X - X_mean) ** 2)\nintercept = y_mean - slope * X_mean\nsalaryhat = slope * X + intercept\n# Plotting the data and regression lines\nplt.scatter(X, y,  alpha=0.7)\nplt.plot(X, salaryhat,  color='blue', linewidth=2)\nplt.plot(X, salarymean, color='red',  linewidth=2)\nplt.xlabel('roe')\nplt.ylabel('salary')\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nsum salary , d\nreg salary roe \npredict salaryhat , xb\t\t\t\t\npredict uhat, resid\t\t\t\t\t\negen salarymean = mean(salary)\t\t\n\ntwoway (scatter salary roe) (lfit salary roe) (lfit salarymean roe) \nqui graph export \"files/graph4_6.svg\", replace\n```  \n\n![](files/graph4_6.svg) \n\n:::\n\n\n\n\n\n\n\n\n\n\n## Goodness-of-fit {.smaller background=\"#dff2c7\"}\n\nManually calculating $R^2$\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) # importing dataset from a stata dta file\nmydata <- read.dta(\"files/CEOSAL1.dta\")\nattach(mydata)\nmodel <- lm(salary ~ roe)\nsalaryhat <- fitted(model)                      # Predict values for dependent variable\nuhat <- resid(model)                            # Predict regression residuals\nsalarymean <- rep(mean(salary),length(salary))  # Generating the mean of salary \n\n# r-squared is simply the ratio of portion explained over total that could be explained\nssr  <- sum(uhat^2)\nssrB <- sum((salary    - salaryhat)^2)\nsst  <- sum((salary    - salarymean)^2)\nsse  <- sum((salaryhat - salarymean)^2)\nsse / sst\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\nX = data['roe']\ny = data['salary']\nX = sm.add_constant(X)  # Add a constant term (intercept)\nmodel = sm.OLS(y, X).fit()\nsalaryhat = model.fittedvalues\nuhat = model.resid\nsalarymean = np.repeat(y.mean(), len(y))\n# Calculate R-squared\nssr = np.sum(uhat**2)\nssrB = np.sum((y - salaryhat)**2)\nsst = np.sum((y - salarymean)**2)\nsse = np.sum((salaryhat - salarymean)**2)\nrsquared = sse / sst\nprint(rsquared)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nqui reg salary roe \npredict salaryhat , xb\t\t\t\t\npredict uhat, resid\t\t\t\t\t\negen salarymean = mean(salary)\t\t\negen sst  = total((salary    - salarymean)^2)  \negen ssr  = total((salary    - salaryhat)^2)\negen ssrB = total(uhat^2)\t\t\t\t\t\negen sse  = total((salaryhat - salarymean)^2)\t\ndi sse / sst\n```  \n\n:::\n\n\n\n\n\n\n\n# Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\n## Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\nWhen we estimate coefficients we have some \"error of estimation\".\n\n- Basically, you are searching the \"true\" coefficient using a sample, which should be representative of the population but it is not the population itself.\n\n- This means that the coefficient estimated is estimated with error.\n\n- We would like (e.g., we will need) to impose some \"structure\" to that error.\n\n\n\n\n\n\n## Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\n**Standard error and T-stat**\n\nTo assess if the variables are significantly related, you need to assess the significance of $\\beta$ coefficients.\n\nUsing the example from Wooldridge, we know that the Beta of ROE is `18.591`, while the standard error of ROE is `11.123`.\n\n. . .\n\n- The standard error is a measure of the accuracy of your estimate. If you find a large standard error, your estimate does not have good accuracy. \n\n- Ideally, you would find small standard errors, meaning that your coefficient is accurately estimated. \n\n- However, you do not have good control over the magnitude of the standard errors. \n\n\n\n\n\n\n## Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\n**Standard error and T-stat**\n\nIf you have a large standard error, probably you coefficient will not be significantly different from zero. You can test whether your coefficient is significantly different from zero computing the t-statistics as follows:\n\n$$t_{\\beta} = \\frac{\\hat{\\beta}}{se(\\hat{\\beta})}$$\n\nIf $t_{\\beta}$ is large enough, you can say that $\\beta$ is significantly different from zero. Usually, $t_{\\beta}$ larger than 2 is enough to be significant. \n\n\n\n\n\n## Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\nIn the previous example, you can find the t-stat manually as follows ($t_{\\beta} =\\frac{\\hat{\\beta}}{se(\\hat{\\beta})}$):\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) # importing dataset from a stata dta file\ndata <- read.dta(\"files/CEOSAL1.dta\")\nattach(data)\n# OLS model\nmodel <- lm(salary ~ roe)\nsummary(model)$coefficients[2,1] / summary(model)$coefficients[2,2] \nsummary(model)$coefficients[2,3]\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\n# OLS model\nX = data['roe']\nX = sm.add_constant(X)\ny = data['salary']\nmodel = sm.OLS(y, X).fit()  \n# Extract and calculate specific coefficients\ncoef_beta = model.params['roe']\ncoef_std_error = model.bse['roe']\n# Calculate t-value\nt_value = coef_beta / coef_std_error\n# Print the coefficient and t-value\nprint(\"Coefficient (beta):\", coef_beta)\nprint(\"Standard Error:\", coef_std_error)\nprint(\"t-value:\", t_value)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nqui reg salary roe \nlocal beta = _b[roe]\nlocal std_error = _se[roe]\nlocal t_value = `beta' / `std_error'\ndisplay \"Coefficient (beta): \" `beta'\ndisplay \"Standard Error: \" `std_error'\ndisplay \"t-value: \" `t_value'\n```  \n\n:::\n\n\n\n\n\n\n\n## Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\nNaturally, the previous analysis requires an estimate of $\\beta$ and an estimate of the $\\beta$'s standard error.\n\n\nThe standard error can be defined as:\n\n$$se(\\hat{\\beta_1})=\\frac{\\hat{\\sigma}}{\\sqrt{SST_x}}$$\n\n- Where $\\hat{\\sigma}$ is the standard deviation of the error term in the regression, which can be calculated as:\n\n$$\\hat{\\sigma} = \\sqrt{\\frac{SSR}{n-2}}$$\n\n    - The $n-2$ here is an adjustment for the degrees of freedom in the regression.\n\n- SST is defined as before $\\sum_i^n(y_i-\\hat{y_i})^2$ \n\n\n\n\n\n\n\n\n\n\n## Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) # importing dataset from a stata dta file\ndata <- read.dta(\"files/CEOSAL1.dta\")\nattach(data)\n# OLS model\nmodel <- lm(salary ~ roe)\n# Extract the standard error of the coefficient for 'roe'\nsummary(model)$coefficients[\"roe\", \"Std. Error\"]\n\n#calculating manually\n# Extract the residuals\nresiduals <- resid(model)\n# Number of observations (n)\nn <- length(residuals)\n# Calculate the mean of the independent variable (roe)\nroe_mean <- mean(roe)\n# Calculate the sum of squared deviations of roe from its mean (SXX)\nSST <- sum((roe - roe_mean)^2)\n# Calculate the sum of squared errors (SSE)\nSSR <- sum(residuals^2)\n# Calculate the standard error of beta\nSd_beta <- sqrt(SSR / ((n - 2)))\n# Calculate S.E\nSe_beta <- Sd_beta / sqrt(SST)\n# Print the standard error of beta\nprint(Se_beta)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\nX = data['roe']\ny = data['salary']\nX = sm.add_constant(X)  \nmodel = sm.OLS(y, X).fit()\n# Extract the standard error of the coefficient for 'roe'\nbeta_se_summary = model.bse['roe']\nprint(\"Standard Error (from summary):\", beta_se_summary)\n# Calculate it manually\n# Extract the residuals\nresiduals = model.resid\n# Number of observations (n)\nn = len(residuals)\n# Calculate the mean of the independent variable (roe)\nroe_mean = X['roe'].mean()\n# Calculate the sum of squared deviations of roe from its mean (SST)\nSST = np.sum((X['roe'] - roe_mean) ** 2)\n# Calculate the sum of squared errors (SSE)\nSSE = np.sum(residuals ** 2)\n# Calculate the standard error of beta (Sd_beta)\nSd_beta = np.sqrt(SSE / (n - 2))\n# Calculate SE_beta\nSE_beta = Sd_beta / np.sqrt(SST)\nprint(\"Standard Error (manually calculated):\", SE_beta)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nqui reg salary roe \ngen beta_se_summary = _se[roe]\ngen n = _N\npredict residuals, residuals\nsum roe, meanonly\ngen roe_diff = roe - r(mean)\negen roe_diff_sq = total(roe_diff^2)\ngen residuals_sq = residuals^2\negen residuals_sq_sum = total(residuals_sq)\ngen Sd_beta = sqrt(residuals_sq_sum / (n - 2))\ngen SE_beta = Sd_beta / sqrt(roe_diff_sq)\ndisplay \"Standard Error (from summary): \" sum(beta_se_summary)\ndisplay \"Standard Error (manually calculated): \" sum(SE_beta)\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n## Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\n**Another comment:**\n\n$$se(\\hat{\\beta_1})=\\frac{\\hat{\\sigma}}{\\sqrt{SST_x}}$$\n\n1) The larger $\\hat{\\sigma}$ is, the larger the variance of $\\beta$. That is, the more \"noise\" in the association between x and Y, the harder it is to learn something about $\\beta$.\n\n2) However, more variation in x, the larger the SST, so the smaller is the variance of $\\beta$.\n\n\n\n\n\n\n\n\n\n# Robust standard errors {.smaller background=\"#e0cafc\"}\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\nLooking at both equations below:\n\n$$t_{\\beta} = \\frac{\\hat{\\beta}}{se(\\hat{\\beta})}$$\n\n\n$$se(\\hat{\\beta_1})=\\frac{\\hat{\\sigma}}{\\sqrt{SST_x}}$$\n\n\n**What happens if $\\hat{\\sigma}$ is not constant (for the values of x)?**\n\n**In other words, how realistic is to assume that the variance in the errors is the same for all slices of x?**\n\n**Can you think of an example where that may happen?**\n\n\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\n**Earnings = f(education)**\n\nPhD have a higher variance of earnings than non-educated people.\n\n. . .\n\n**Leveragge=f(Size)**\n\nIt is quite possible that small firms will have less options of leverage than large companies. \n\nThis means that a sub-sample of large companies will have higher variance in the leverage decisions (and thus the error terms) than the sub-sample of small firms\n\n\n\n\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\nOne of the key assumptions in OLS estimators is homoscedasticity \n\nThat is, the assumption is that the variance of the errors is homoscedastic (constant variance in all slices of X). \n\nIt means that throughout all observations, the error term shows the **same variance**. \n\nIf errors are not homoscedastic, we have the heteroscedasticity problem.\n\n\n. . . \n\nHeteroskedasticity **does not cause bias or inconsistency in the OLS estimators** of the $\\beta$ like the OVB would. \n\nIt also does not affect the $R^2$. \n\n**What Heteroscedasticity does is to bias the standard errors of the estimates.**\n\n\n\n\n\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\n![](files/homoscedasticity.png)\n\n\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\n\n\n![](files/heteroscedasticity.png)\n\n\n\n\n\n\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\n**Homoskedascticity** = Constant $\\hat{\\sigma}$ to all slices of X.\n\n**Heteroskedascticity** = Non-constant $\\hat{\\sigma}$ to all slices of X.\n\n**Without homoskedascticity, OLS no longer has the minimum mean squared errors**, which means that the *estimated standard errors are biased*, which in turn creates bias in the t-stat and the inference you'll make with your model.\n \n\n. . . \n\nFortunately, we have an easy solution for that.\n\n\n$$Var(\\hat{\\beta_1}) = \\frac{\\sum_i^n(x_i-\\bar{x})^2\\hat{\\mu}^2}{SST^2_x}$$\n\nThis formula simply \"includes\" the heteroskedascticity in the calculation of $Var(\\hat{\\beta_1})$, meaning this correct the estimated standard deviation to heteroskedascticity.\n\nWe call this correction as **Robust Standard Errors** (White Robust).\n\n. . .\n\nIn other words, you should always use Robust Standard Errors. It is easy to use it with R.\n\n\n\n\n\n\n\n\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\n**Using Robust Standard-errors.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(sandwich)\nlibrary(foreign) \nlibrary(lmtest)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\nmodel <- lm(salary ~ roe, data = data)\nrobust_model <- coeftest(model, vcov = vcovHC(model, type = \"HC3\"))\nSE_beta_robust <- robust_model[\"roe\", \"Std. Error\"]\ncat(\"Robust Standard Error :\", SE_beta_robust, \"\\n\")\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\ndata = pd.read_stata(\"files/CEOSAL1.DTA\")\nmodel = smf.ols(\"salary ~ roe\", data=data)\nresults = model.fit(cov_type='HC3')  \nSE_beta_robust = results.bse['roe']\nprint(\"Robust Standard Error :\", SE_beta_robust)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\n\nqui reg salary roe \ngen beta_se_non = _se[roe]\n\nqui reg salary roe , robust\ngen beta_se_summary = _se[roe]\n\ndi \"Standard Error (non-robust): \" sum(beta_se_non)\ndi \"Standard Error (robust): \" sum(beta_se_summary)\n```  \n\n:::\n\n\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\n\nNotice that the standard errors have changed quite significantly  in this example. \n\nUsually, the robust standard errors are larger than the traditional ones in empirical works.\n\n**But, in this example, they are smaller.**\n\n. . . \n\nPerhaps more importantly:\n\n**Once the S.e. change, you should expect that the t-stat of the estimates also change.**\n\n\n. . . \n\n\n**Final comment**: robust standard errors are robust in the case of homoskedasticity.\n\n::: {.callout-warning}\nThus, you should always use robust S.E.\n:::\n\n\n\n\n\n\n\n\n\n# Clustered standard errors {.smaller background=\"#edc5d1\"}\n\n## Clustered standard errors {.smaller background=\"#edc5d1\"}\n\nAlmost always, someone will ask you whether you clustered your standard errors.\n\n**The intuition is the following:**\n\n- When you do not cluster, you are assuming that all observations are independently and identically distributed (i.i.d.), which may or may not be true.\n\n- Imagine you are studying the effect of class size on students achievement.\n\n- How much of a effect would **have the teacher of a class**? \n\n. . . \n\n- In this design, the teacher influences the achievement of all the students in the same class, and one teacher cannot be at two classes at the same time.\n\n- Thus, it would be wise to cluster the errors at the class-level. This assumes that the residual of each individual is clustered with the other individuals in the same class.\n\n\n. . .\n\nIn principle, clustering solves any form of dependence of the residuals in your data.\n\n\n\n\n\n\n\n\n## Clustered standard errors {.smaller background=\"#edc5d1\"}\n\nIn corporate finance/accounting research panel data research, the tradition is to cluster at the **firm-level**.\n\n- The reason is that the observations of the same firm are not independent trough time, thus are correlated. \n\nBut, there is a lot of debate about this decision. \n\n. . .\n\nThe tip is to cluster where the **randomness exist**. That is quite subjective. In the class size example, the **randomness** comes out of the teacher, since each teacher has their own ways of teaching (materials, resources, etc.).\n\n. . .\n \nBut, it is a good practice to stress this decision a bit in your own research by **also showing results with clustered s.e. at the industry-level**.\n\n. . .\n\n**Final tip**: usually the minimum number of cluster is about 30. Less than that might be insufficient (but, again, the guidance in this topic is very subjective).  \n\n\n\n \n\n\n## Clustered standard errors {.smaller background=\"#edc5d1\"}\n\nThe clustered standard errors are different because I am fabricating the clusters here for the sake of the coding.\n\nIn your real research, you would have the cluster at hands. \n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(sandwich)\nlibrary(foreign) \nlibrary(lmtest)\nlibrary(plm)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\nmodel <- lm(salary ~ roe, data = data)\nrobust_model <- coeftest(model, vcov = vcovHC(model, type = \"HC3\"))\n#clustered\ndata$cluster <- rep(1:35, length.out = nrow(data))\nmodel <- plm(salary ~ roe, data = data, index = c(\"cluster\"))\n\nclustered_se <- vcovHC(model, type = \"HC3\", cluster = \"group\")\nSE_beta_clustered <- sqrt(clustered_se[\"roe\", \"roe\"])\n\ncat(\"Standard Error (robust):\", SE_beta_robust, \"\\n\")\ncat(\"Standard Error (clustered)::\", SE_beta_clustered, \"\\n\")\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Read the dataset\ndata = pd.read_stata(\"files/CEOSAL1.DTA\")\n\n# Create a new variable 'cluster' with cluster numbers ranging from 1 to 35\ndata['cluster'] = list(range(1, 36)) * (len(data) // 35)\n\n# Fit the linear regression model\nmodel = sm.OLS(data['salary'], sm.add_constant(data['roe'])).fit()\n\n# Compute robust standard errors\nrobust_model = model.get_robustcov_results(cov_type='HC3')\nSE_beta_robust = robust_model.cov_params().loc['roe', 'roe'] ** 0.5\n\n# Fit the linear regression model with clustered standard errors\nmodel_clustered = sm.OLS(data['salary'], sm.add_constant(data['roe'])).fit(cov_type='cluster', cov_kwds={'groups': data['cluster']})\n\n# Extract the clustered standard errors for 'roe'\nclustered_se = model_clustered.HC3_se.loc['roe']\n\nprint(\"Robust Standard Error (HC3):\", SE_beta_robust)\nprint(\"Clustered Standard Error (HC3):\", clustered_se)\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\n\nqui reg salary roe \ngen beta_se_non = _se[roe]\n\nqui reg salary roe , robust\ngen beta_se_summary = _se[roe]\n\negen cluster = seq(), block(6)\nqui regress salary roe , vce(cluster cluster)\ngen SE_beta_clustered = _se[roe]\n\ndi \"Standard Error (non-robust): \" sum(beta_se_non)\ndi \"Standard Error (robust): \" sum(beta_se_summary)\ndi \"Standard Error (clustered): \" sum(SE_beta_clustered)\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n## **THANK YOU!** {background=\"#b1cafa\"}\n\n::: columns\n::: {.column width=\"60%\"}\n**QUESTIONS?**\n\n![](figs/qa2.png){width=\"150%\" heigth=\"150%\"}\n:::\n\n::: {.column width=\"40%\"}\n**Henrique C. Martins**\n\n-   [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)\n-   [Personal Website](https://henriquemartins.net/)\n-   [LinkedIn](https://www.linkedin.com/in/henriquecastror/)\n-   [Lattes](http://lattes.cnpq.br/6076997472159785)\n-   [Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)\\\n:::\n:::\n\n::: footer\n:::\n","srcMarkdownNoYaml":"\n\n\n\n```{r setup}\n#| include: false\n#| warning: false\n\n\n# library(reticulate)\n# use_python(\"C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe\")\nlibrary(reticulate)\nlibrary(Statamarkdown)\n#reticulate::py_install(\"matplotlib\")\n#reticulate::py_install(\"seaborn\")\n#reticulate::py_install(\"pyfinance\")\n#reticulate::py_install(\"xlrd\")\n#reticulate::py_install(\"quandl\")\n\n```\n\n\n\n\n\n\n# More about Regressions {.smaller background=\"#454343\"}\n\n## Regression {.smaller background=\"#454343\"}\n\nLinear regressions are the workhorse tool in econometrics\n\n-   **Simplicity**: straightforward to understand, implement, and visualize.\n\n. . .\n\n-   **Interpretability**: coefficients have clear interpretations.\n    -   represents the change in the Y for a one-unit change in the X, holding all other variables constant.\n\n. . .\n\n-   **Versatility**: simple linear regression or *multiple linear regression*.\n\n. . .\n\n-   **Assumptions**: linearity, independence of errors, homoscedasticity, and normality of errors.\n\n. . .\n\n-   **Baseline Model**: You can compare the performance of more advanced models to linear regression.\n\n. . .\n\n-   **Estimation**: provides clear estimates of the coefficients' confidence intervals and hypothesis testing.\n\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\nIn this setting, the variables $y$ and $x$ can have several names.\n\n| Y                  | X                    |\n|--------------------|----------------------|\n| Dependent variable | Independent variable |\n| Explained variable | Explanatory variable |\n| Response variable  | Control variable     |\n| Predicted variable | Predictor variable   |\n| Regressand         | Regressor            |\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\nBroadly, we are interested in how y is explained by x?\n\n-   $y_i = \\alpha + \\beta_1 x_i + \\epsilon$\n\n. . .\n\nPerhaps $\\epsilon$ is the most important part of a regression.\n\nThe interpretation is \"*everything that is not explained by X and that explains Y*\".\n\n. . .\n\nA comment\n\n-   Usually, the literature uses $\\epsilon$ for the \"estimated\" residual.\n\n-   And $\\mu$ for the \"true\" residual, which necessarily implies that the assumptions hold.\n\n-   At the end od the day, you don't need to worry to much with the notation of this term because we are always in the \"estimated world\", and almost never in the \"true world\".\n\n-   The \"true world\" implies that you are studying the population or that you have a true random sample of the population\n\n    -   $y_i = \\alpha + \\beta_1 x_i + \\mu$\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\n**Remember**\n\n- $y, x$, and $\\mu$ are random variables\n- $y and x$ are observable\n- $\\mu$ and $\\beta$ are unobservable\n- $\\mu$ captures everything that determines y after accounting for x \n\nOur goal is to estimate Œ≤\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\nThere are some assumptions/requirements about $\\mu$ in a OLS\n\n**First assumption**\n\n-   E($\\mu$) = 0\n\n    -   This is a simple assumption, not strong at all.\n    -   It simply assumes that the average of $\\mu$ is zero in the population.\n    -   Basically, any non-zero mean is absorbed by the intercept\n        -   Say that E($\\mu$) = k\n        -   We could rewrite $\\mu = k + w$, where E(w)=0\n        -   Then, model becomes $y=(\\alpha +ùëò) + \\betaùë•+ùë§$\n        -   Intercept is now just (Œ± + k), and error, w, is mean zero\n\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\n**Second assumption**\n\n-   E($\\mu$\\|x) = E($\\mu$) for all values of x\n    -   It says that the average value of $\\mu$ does not depend on the value of x (i.e., the slice of the population you are looking at).\n    -   We say that $\\mu$ is mean-independent of x.\n    -   This is true if the X and the $\\mu$ are independent to each other.\n    -   Implies that x and $\\mu$ are *uncorrelated*.\n    -   **Conditional Mean Independence (CMI)**.\n    -   This is one of the keys assumption to *causal inference*.\n\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\n**Second assumption**\n\n**Example**\n\nLet's say the model is:\n\n$$wage = \\alpha + \\beta Schooling_{years} + \\epsilon$$\n\n-   where $\\epsilon$ represents *unobserved ability*.\n\nDoes CMI hold?\n\nThat is E(ability\\|x=8)=E(ability\\|x=10)=E(ability\\|x=12)?\n\n. . .\n\n**Probably not**, because the unobserved ability should depend on the years of schooling.\n\nThe solution (not trivial) would be to include ability as a new X.\n\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\n**Another example**\n\nConsider the following model (with only one X)\n\n$$Leverage_i = \\alpha + \\beta_1 Profitability_i + \\mu_i$$\n\n-   CMI says that, to every firm *i*, $\\mu$ is the same, even when firms have different profitability.\n\n-   Can you think on examples when this assumption may not hold in this model?\n\n. . .\n\n1)  unprofitable firms have higher bankruptcy risk, which should make them to have lower leverage (tradeoff theory).\n\n2)  unprofitable firms have low cash, which should make them to have more leverage (pecking order theory).\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\n**The discussion of whether the CMI holds is the origin of the \"endogeneity\" problem.**\n\nYou will face reviewers arguing reasons of why the CMI might not hold in your case.\n\n- Many  will criticize a model by saying it has an ‚Äúendogeneity problem‚Äù, whithout explaining more.\n\n  - This is very generic. **Don't do that!**\n\n. . .\n\nThey should explain what is the source of the problem that is making the model violate CMI.\n\n - OVB, selection bias, reverse causality, simultaneity, etc?\n\n. . . \n\nGenerally speaking, endogeneity refers to a violation of CMI, meaning that $x$ and $\\mu$ are correlated.\n\nThis is always a plausible possibility, since $\\mu$ carries a lot of stuff (something must be correlated with X).\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\n**Third assumption**\n\nCombining 1 and 2 leads to\n\n-   E($\\mu$\\|x) = 0\n    -   This is a very important assumption called **zero conditional mean assumption**.\n\n## Ordinary Least Squares {.smaller background=\"#454343\"}\n\nOur focus is to find estimates for $\\alpha$ and $\\beta$. Should we have access to the population, it would be easy. We could write:\n\n$$y_i= \\alpha + \\beta_1x_i + \\mu$$\n\n. . .\n\nBut remember that,\n\n$$E(u) = 0$$ $$E(u|x) = 0$$\n\nThe second bullet implies that the correlation between x and $\\mu$ is zero (we can write that as E(x,u) = 0).\n\n. . .\n\n::: callout-important\nRemember: $\\alpha$ and $\\beta$ are parameters to be estimated (i.e., constants), while $X$ and $Y$ are variables\n:::\n\n\n\n\n\n\n\n\n## Regression {.smaller background=\"#454343\"}\n\nSo we can write that (in the **population**)\n\n$E(y - \\alpha - \\beta_1x ) = 0$\n\n$(x[y - \\alpha - \\beta_1x ]) = 0$\n\n. . .\n\nSo we can write that (in the **sample**)\n\n$\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{\\alpha} - \\hat{\\beta_1} x_i ) = 0$ , We will use this to find $\\alpha$:\n\n$\\frac{1}{n} \\sum_{i=1}^n (x_i [y_i - \\hat{\\alpha} - \\hat{\\beta_1} x_i ]) = 0$ , We will use this to find $\\beta$:\n\n\n\n\n\n\n\n## Finding $\\alpha$ {.smaller background=\"#454343\"}\n\nFrom before:\n\n$$\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{\\alpha} - \\hat{\\beta_1} x_i ) =0$$\n\n. . .\n\nPassing the sum operator through\n\n$$\\frac{1}{n}\\sum_{i=1}^n(y_i) - \\frac{1}{n}\\sum_{i=1}^n(\\hat{\\alpha})  - \\frac{1}{n} \\sum_{i=1}^n(\\hat{\\beta_1} x_i )=0$$\n\n. . .\n\nCoefficients are constants, so we can get rid of the sum operator.\n\n$$\\frac{1}{n}\\sum_{i=1}^n(y_i) - \\hat{\\alpha}  - \\hat{\\beta_1} \\frac{1}{n}  \\sum_{i=1}^n(\\ x_i )=0$$\n\n\n\n\n\n\n\n\n## Finding $\\alpha$ {.smaller background=\"#454343\"}\n\n-   We know that $\\frac{1}{n}\\sum_{i=1}^n(y_i)$ is $\\bar{y_i}$ (the mean)\n\n$$\\bar{y_i} - \\hat{\\alpha}  - \\hat{\\beta_1}  \\bar{x_i}=0$$\n\n. . .\n\nSo we write:\n\n$$\\hat{\\alpha}  = \\bar{y_i} -  \\hat{\\beta_1}   \\bar{x_i}$$\n\n. . .\n\n**Easy Peasy** üòÄ\n\n\n\n\n\n## Finding $\\beta$ {.smaller background=\"#454343\"}\n\nFrom before:\n\n$$\\frac{1}{n} \\sum_{i=1}^n (x_i [y_i - \\hat{\\alpha} - \\hat{\\beta_1} x_i ]) = 0$$\n\n. . .\n\nBut now we have:\n\n$$\\hat{\\alpha}  = \\bar{y_i} -  \\hat{\\beta_1}   \\bar{x_i}$$\n\n. . .\n\nThus,\n\n$$\\frac{1}{n} \\sum_{i=1}^n (x_i [y_i - (\\bar{y_i} -  \\hat{\\beta_1}   \\bar{x_i}) - \\hat{\\beta_1} x_i ]) = 0$$\n\n\n\n\n## Finding $\\beta$ {.smaller background=\"#454343\"}\n\nTurning into\n\n$$\\frac{1}{n} \\sum_{i=1}^n (x_i [y_i - \\bar{y_i} ])  -  \\frac{1}{n} \\sum_{i=1}^n (x_i [\\hat{\\beta_1} x_i - \\hat{\\beta_1} \\bar{x_i} ]) = 0$$\n\n. . .\n\nOr\n\n$$\\frac{1}{n} \\sum_{i=1}^n (x_i [y_i - \\bar{y_i} ])  =  \\hat{\\beta_1} \\frac{1}{n} \\sum_{i=1}^n (x_i [ x_i  - \\bar{x_i} ]) $$\n\n. . . \n\nLast step (I am skipping some steps here)\n\n$$\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x} )(y_i - \\bar{y_i} )  =  \\hat{\\beta_1} \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x_i} )^2 $$\n\n\n\n\n## Finding $\\beta$ {.smaller background=\"#454343\"}\n\n\nIf the variance of x is not zero, we can write $\\beta$ as\n\n$$\\hat{\\beta_1} = \\frac{\\sum_i^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_i^n (x_i - \\bar{x})^2} = \\frac{Covariance(x_i,y_i)}{Variance(x_i)}$$\n\n\n\n## Finding $\\mu$  {.smaller background=\"#454343\"}\n\nNow that we have $\\hat{Y_i}=\\hat{\\alpha} + \\hat{\\beta_1} X_i$, we can estimate the residual $\\mu$\n\n$$\\hat{\\mu} = Y_i - \\hat{Y_i}$$\n\n\nWhich is the same as:\n\n$$\\hat{\\mu} = Y_i - \\hat{\\alpha} - \\hat{\\beta_1}x_i$$\n\nMost residuals will not be 0, meaning they do not lie on the best fitting line. \n\n\n\n\n\n\n## Finding $\\mu$  {.smaller background=\"#454343\"}\n\nThe job of an OLS model is find the parameters to minimize the squared error (i.e., to find the best fitting line).\n\n$$\\sum_{i=1}^n \\hat{\\mu}^2 = \\sum_{i=1}^n(Y_i - \\hat{Y_i})^2$$\n\n\n\n\n\n\n\n\n\n\n\n\n## Regression ([Source](https://mixtape.scunning.com/02-probability_and_regression#ordinary-least-squares)) {.smaller background=\"#454343\"}\n\nAnother example of regression. The differences in the coefficients are due to the differences in the seed of the random variables generator.\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(tidyverse)\nset.seed(1)\ntb <- tibble(\n  x = rnorm(10000),\n  u = rnorm(10000),\n  y = 5.5*x + 12*u # notice that I am defining the beta1 here. The 5.5 is the \"true\" beta we want to estimate.\n) \nreg_tb <- lm(y ~ x, data=tb) \nsummary(reg_tb)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nnp.random.seed(1)\n\nobs = 10000\ndata = pd.DataFrame({\n    'x': np.random.normal(size=obs),\n    'u': np.random.normal(size=obs),\n})\ndata['y'] = 5.5 * data['x'] + 12 * data['u']\n\nX = sm.add_constant(data['x'])\nmodel = sm.OLS(data['y'], X).fit()\n\nprint(model.summary())\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nset seed 1 \nset obs 10000 \ngen x = rnormal() \ngen u  = rnormal() \ngen y  = 5.5*x + 12*u \nreg y x \n```\n:::\n\n\n\n\n\n\n## Regression ([Source](https://mixtape.scunning.com/02-probability_and_regression#ordinary-least-squares)) {.smaller background=\"#454343\"}\n\nAnother example of regression. The differences in the coefficients are due to the differences in the seed of the random variables generator.\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(tidyverse)\nset.seed(1)\ntb <- tibble(\n  x = rnorm(10000),\n  u = rnorm(10000),\n  y = 5.5*x + 12*u # notice that I am defining the beta1 here. The 5.5 is the \"true\" beta we want to estimate.\n) \nreg_tb <- lm(y ~ x, data=tb) \ntb %>% \n  lm(y ~ x, .) %>% \n  ggplot(aes(x=x, y=y)) + \n  ggtitle(\"OLS Regression Line\") +\n  geom_point(size = 0.05, color = \"black\", alpha = 0.5) +\n  geom_smooth(method = lm, color = \"black\") +\n  annotate(\"text\", x = -1.5, y = 30, color = \"red\", \n           label = paste(\"Intercept = \", reg_tb$coefficients[1])) +\n  annotate(\"text\", x = 1.5, y = -30, color = \"blue\", \n           label = paste(\"Slope =\", reg_tb$coefficients[2]))\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a scatterplot with the OLS regression line using Seaborn\nsns.set(style='whitegrid')\nplt.figure(figsize=(7, 5))\nsns.scatterplot(x='x', y='y', data=data, color='black', alpha=0.5, s=5)\nsns.regplot(x='x', y='y', data=data, color='black', scatter=False, line_kws={'color':'black'})\nplt.title('OLS Regression Line')\nplt.annotate(f'Intercept = {model.params[0]:.2f}', xy=(-1.5, 30), color='red')\nplt.annotate(f'Slope = {model.params[1]:.2f}', xy=(1.5, -30), color='blue')\nplt.show()\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nset seed 1 \nset obs 10000 \ngen x = rnormal() \ngen u  = rnormal() \ngen y  = 5.5*x + 12*u \nreg y x \npredict yhat1 \ngen yhat2 = -0.0750109  + 5.598296*x \npredict uhat1, residual \ngen uhat2=y-yhat2 \nqui sum uhat* \ntwoway (lfit y x, lcolor(black) lwidth(medium)) (scatter y x, mcolor(black) msize(tiny) msymbol(point)), title(OLS Regression Line) \nqui graph export \"files/graph3.svg\", replace\n```  \n\n![](files/graph3.svg) \n\n:::\n\n\n\n\n\n\n## Regression ([Source](https://mixtape.scunning.com/02-probability_and_regression#ordinary-least-squares)) {.smaller background=\"#454343\"}\n\nUsing the previous regressions, we can show  that:\n\n1)  $\\hat{y_i} = \\hat{\\alpha} + \\hat{\\beta_1} x_i$\n\n2)  $\\hat{\\mu_i} = y_i - \\hat{y_i}$\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n\nlibrary(tidyverse)\nset.seed(1)\ntb <- tibble(\n  x = rnorm(10000),\n  u = rnorm(10000),\n  y = 5.5*x + 12*u # notice that I am defining the beta1 here. The 5.5 is the \"true\" beta we want to estimate.\n) \nreg_tb <- lm(y ~ x, data=tb) \n\ntb <- tb %>% \n  mutate(\n    yhat1 = predict(lm(y ~ x, .)),\n    yhat2 = reg_tb$coefficients[1] + reg_tb$coefficients[2]*x, \n    uhat1 = residuals(lm(y ~ x, .)),\n    uhat2 = y - yhat2\n  )\nsummary(tb[-1:-3])\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nnp.random.seed(1)\nobs = 10000\nx = np.random.normal(size=obs)\nu = np.random.normal(size=obs)\ny = 5.5 * x + 12 * u\n\nX = sm.add_constant(x)\nmodel = sm.OLS(y, X).fit()\n\ntb = pd.DataFrame({'x': x, 'u': u, 'y': y})\ntb['yhat1'] = model.predict(X)\ntb['uhat1'] = y - tb['yhat1']\ntb['yhat2'] = model.params[0] + model.params[1] * x\ntb['uhat2'] = y - tb['yhat2']\n\nprint(tb[['yhat1','yhat2', 'uhat1','uhat2']].describe())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nset seed 1 \nclear \nqui set obs 10000 \ngen x = rnormal() \ngen u  = rnormal() \ngen y  = 5.5*x + 12*u \nqui reg y x \npredict uhat1, residual \npredict yhat1 \ngen yhat2 = -0.0750109  + 5.598296*x \ngen uhat2=y-yhat2 \nsum yhat*  uhat* \n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n# Properties of OLS {.smaller background=\"#bfc4d9\"}\n\n\n## Properties of OLS {.smaller background=\"#bfc4d9\"}\n\nWe can easily show that (remember from before) \n\n$$\\sum_i^n(y_i - \\hat{\\alpha} - \\hat{\\beta_1} x_i) = 0$$\n\nAnd that \n\n$$\\sum_i^n \\hat{\\mu}  = 0$$\n\nThe graphs next slide shows a spherical figure, suggesting that the residual is not correlated with the the fitted values ($\\hat{y_i}$)\n\n\n\n\n\n\n\n## Properties of OLS {.smaller background=\"#bfc4d9\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(tidyverse)\nset.seed(1)\ntb <- tibble(\n  x = rnorm(10000),\n  u = rnorm(10000),\n  y = 5.5*x + 12*u # notice that I am defining the beta1 here. The 5.5 is the \"true\" beta we want to estimate.\n) \nreg_tb <- lm(y ~ x, data=tb) \ntb <- tb %>% \n  mutate(\n    yhat1 = predict(lm(y ~ x, .)),\n    yhat2 = reg_tb$coefficients[1] + reg_tb$coefficients[2]*x, \n    uhat1 = residuals(lm(y ~ x, .)),\n    uhat2 = y - yhat2\n  )\ntb %>% \n  lm(uhat1 ~ yhat1 , .) %>% \n  ggplot(aes(x=yhat1, y=uhat1)) + \n  geom_point(size = 0.1, color = \"black\") +\n  geom_smooth(method = lm, color = \"black\")\n```\n\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nnp.random.seed(1)\nobs = 10000\nx = np.random.normal(size=obs)\nu = np.random.normal(size=obs)\ny = 5.5 * x + 12 * u\n\nX = sm.add_constant(x)\nmodel = sm.OLS(y, X).fit()\n\ntb = pd.DataFrame({'x': x, 'u': u, 'y': y})\ntb['yhat1'] = model.predict(X)\ntb['uhat1'] = y - tb['yhat1']\ntb['yhat2'] = model.params[0] + model.params[1] * x\ntb['uhat2'] = y - tb['yhat2']\nmodel = sm.OLS(tb['uhat1'], sm.add_constant(tb['yhat1'])).fit()\n# Create a scatter plot with a regression line\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(7, 4.5))\nsns.scatterplot(x='yhat1', y='uhat1', data=tb, size=0.05, color='black', alpha=0.5)\nsns.regplot(x='yhat1', y='uhat1', data=tb, scatter=False, color='black')\nplt.xlabel('yhat1')\nplt.ylabel('uhat1')\nplt.title('Scatter Plot of uhat1 vs. yhat1')\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nset seed 1 \nset obs 10000 \ngen x = rnormal() \ngen u  = rnormal() \ngen y  = 5.5*x + 12*u \nqui reg y x \npredict yhat1 \npredict uhat1, residual \ntwoway (lfit uhat1 yhat1 , lcolor(black) lwidth(large)) (scatter uhat1 yhat1 , mcolor(black)  msymbol(point))\nqui graph export \"files/graph4.svg\", replace\n```  \n\n![](files/graph4.svg) \n\n:::\n\n\n\n\n\n\n\n\n\n\n## Properties of OLS {.smaller background=\"#bfc4d9\"}\n\nSimilarly, we can easily show that \n\n$$\\sum_i^nx_i(y_i - \\hat{\\alpha} - \\hat{\\beta_1} x_i) = 0$$\n \nAnd that\n \n$$\\sum_i^nx_i\\hat{\\mu}  = 0$$\n\n \nMeaning that the sample covariance between the X and the residual will be always zero.\n\n\n\n\n\n\n\n\n\n## Properties of OLS {.smaller background=\"#bfc4d9\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(tidyverse)\nset.seed(1)\ntb <- tibble(\n  x = rnorm(10000),\n  u = rnorm(10000),\n  y = 5.5*x + 12*u # notice that I am defining the beta1 here. The 5.5 is the \"true\" beta we want to estimate.\n) \nreg_tb <- lm(y ~ x, data=tb) \ntb <- tb %>% \n  mutate(\n    yhat1 = predict(lm(y ~ x, .)),\n    yhat2 = reg_tb$coefficients[1] + reg_tb$coefficients[2]*x, \n    uhat1 = residuals(lm(y ~ x, .)),\n    uhat2 = y - yhat2\n  )\ntb %>% \n  lm(uhat1 ~ x , .) %>% \n  ggplot(aes(x=x, y=uhat1)) + \n  geom_point(size = 0.1, color = \"black\") +\n  geom_smooth(method = lm, color = \"black\")\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nnp.random.seed(1)\nobs = 10000\nx = np.random.normal(size=obs)\nu = np.random.normal(size=obs)\ny = 5.5 * x + 12 * u\n\nX = sm.add_constant(x)\nmodel = sm.OLS(y, X).fit()\n\ntb = pd.DataFrame({'x': x, 'u': u, 'y': y})\ntb['yhat1'] = model.predict(X)\ntb['uhat1'] = y - tb['yhat1']\ntb['yhat2'] = model.params[0] + model.params[1] * x\ntb['uhat2'] = y - tb['yhat2']\nmodel = sm.OLS(tb['uhat1'], sm.add_constant(tb['yhat1'])).fit()\n# Create a scatter plot with a regression line\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(7, 4.5))\nsns.scatterplot(x='x', y='uhat1', data=tb, size=0.05, color='black', alpha=0.5)\nsns.regplot(x='x', y='uhat1', data=tb, scatter=False, color='black')\nplt.xlabel('x')\nplt.ylabel('uhat1')\nplt.title('Scatter Plot of uhat1 vs. x')\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nset seed 1 \nset obs 10000 \ngen x = rnormal() \ngen u  = rnormal() \ngen y  = 5.5*x + 12*u \nqui reg y x \npredict yhat1 \npredict uhat1, residual \ntwoway (lfit uhat1 x , lcolor(black) lwidth(large)) (scatter uhat1 x , mcolor(black) msymbol(point))\nqui graph export \"files/graph5.svg\", replace\n```  \n\n![](files/graph5.svg) \n\n:::\n\n\n## Properties of OLS {.smaller background=\"#bfc4d9\"}\n\nLet's say you estimate a model and find the $\\hat{\\mu}$.\n\nIf you calculate the correlation between the X and $\\hat{\\mu}$, you will find zero.\n\n**This is by construction!** It is not an evidence that CMI is nos violated.\n\nIn fact, the OLS \"assumes\" and \"forces\" zero correlation.\n\n**It is intuitive: if you are \"forcing\" zero correlation when the correlation is not in fact zero, your coefficients will be biased.**\n\nThe previous graphs actually show zero correlation. But that is expected and does not suggest the model is not violating CMI.\n\n**At the end of the day, CMI is untestable and unverifiable**.\n\n\n\n\n\n\n\n\n\n# Goodness-of-fit {.smaller background=\"#dff2c7\"}\n\n## Goodness-of-fit {.smaller background=\"#dff2c7\"}\n\n**Understanding what SSR, SSE and SST mean** \n\n- SSE = Sum of Squares Explained = $\\sum_i^n(\\hat{y_i}-\\bar{y})^2$\n- SSR = Sum of Squares Residuals = $\\sum_i^n\\hat{\\mu}^2$\n- SST = Sum of Squares Total = SSE + SSR = $\\sum_i^n(y_i-\\hat{y_i})^2$ \n\n\nR-squared is simply the ratio of portion explained over the total that could be explained.\n\n\n$$R^2 = \\frac{SSE}{SST} = 1-\\frac{SSR}{SST}$$\n\n\n\n## Goodness-of-fit {.smaller background=\"#dff2c7\"}\n\n![](figs/R2.jpg)\n\n\n\n\n\n## Goodness-of-fit {.smaller background=\"#dff2c7\"}\n\nYou can think this way:\n\n1) If X does not explain Y, then the best predictor of Y is $\\bar{y}$. In that case, your model does not explain anything of Y, thus $R^2$ is zero, and $\\hat{y_i}=\\bar{y}$\n\n. . .\n\n2) If X partially explains Y, then $\\hat{y_i} \\neq \\bar{y}$, meaning that $\\hat{y_i}$ has some inclination (like the figure next slide). This means that $SSE>0$ and your $R^2>0$ but $R^2<1$\n\n. . .\n\n3) Whatever is not explained by $\\hat{y_i}$ is left to $\\sum_i^2\\hat{\\mu}^2$, meaning that SSR will be non-zero.\n\n. . .\n\n4) The ratio of the portion that you can explain by  $\\hat{y_i}$ over the total that is to be explained  $y_i-\\hat{y_i}$ if the $R^2$.\n\n\n\n\n\n\n## Goodness-of-fit {.smaller background=\"#dff2c7\"}\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) # importing dataset from a stata dta file\ndata <- read.dta(\"files/CEOSAL1.dta\")\nattach(data)\n# Statistics of salary \nmean(salary)\n# OLS model\nmodel <- lm(salary ~ roe)\nsalaryhat <- fitted(model)                      # Predict values for dependent variable\nuhat <- resid(model)                            # Predict regression residuals\nsalarymean <- rep(mean(salary),length(salary))  # Generating the mean of salary \nsummary(model)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\nprint(data['salary'].mean())\n# OLS model\nX = data['roe']\nX = sm.add_constant(X)\ny = data['salary']\n\nmodel = sm.OLS(y, X).fit()  # Fit the linear regression model\nsalaryhat = model.fittedvalues  # Predicted values for the dependent variable\nuhat = model.resid  # Predict regression residuals\nsalarymean = pd.Series([y.mean()] * len(y))  # Generating the mean of salary\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nsum salary \nreg salary roe \npredict salaryhat , xb\t\t\t\t\npredict uhat, resid\t\t\t\t\t\negen salarymean = mean(salary)\t\t\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n## Goodness-of-fit {.smaller background=\"#dff2c7\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) # importing dataset from a stata dta file\nmydata <- read.dta(\"files/CEOSAL1.dta\")\nattach(mydata)\nmodel <- lm(salary ~ roe)\nsalaryhat <- fitted(model)                      # Predict values for dependent variable\nuhat <- resid(model)                            # Predict regression residuals\nsalarymean <- rep(mean(salary),length(salary))  # Generating the mean of salary \n# r-squared is simply the ratio of portion explained over total that could be explained - Understanding what SSR, SSE and SST mean \nplot(salary ~ roe)\nabline(lm(salary ~ roe), col = \"blue\")\nabline(lm(salarymean ~ roe), col = \"red\")\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\nX = data[['roe']]\ny = data['salary']\nsalarymean = np.repeat(y.mean(), len(y))\nX_mean = X.mean()\ny_mean = y.mean()\nslope = np.sum((X - X_mean) * (y - y_mean)) / np.sum((X - X_mean) ** 2)\nintercept = y_mean - slope * X_mean\nsalaryhat = slope * X + intercept\n# Plotting the data and regression lines\nplt.scatter(X, y,  alpha=0.7)\nplt.plot(X, salaryhat,  color='blue', linewidth=2)\nplt.plot(X, salarymean, color='red',  linewidth=2)\nplt.xlabel('roe')\nplt.ylabel('salary')\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nsum salary , d\nreg salary roe \npredict salaryhat , xb\t\t\t\t\npredict uhat, resid\t\t\t\t\t\negen salarymean = mean(salary)\t\t\n\ntwoway (scatter salary roe) (lfit salary roe) (lfit salarymean roe) \nqui graph export \"files/graph4_6.svg\", replace\n```  \n\n![](files/graph4_6.svg) \n\n:::\n\n\n\n\n\n\n\n\n\n\n## Goodness-of-fit {.smaller background=\"#dff2c7\"}\n\nManually calculating $R^2$\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) # importing dataset from a stata dta file\nmydata <- read.dta(\"files/CEOSAL1.dta\")\nattach(mydata)\nmodel <- lm(salary ~ roe)\nsalaryhat <- fitted(model)                      # Predict values for dependent variable\nuhat <- resid(model)                            # Predict regression residuals\nsalarymean <- rep(mean(salary),length(salary))  # Generating the mean of salary \n\n# r-squared is simply the ratio of portion explained over total that could be explained\nssr  <- sum(uhat^2)\nssrB <- sum((salary    - salaryhat)^2)\nsst  <- sum((salary    - salarymean)^2)\nsse  <- sum((salaryhat - salarymean)^2)\nsse / sst\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\nX = data['roe']\ny = data['salary']\nX = sm.add_constant(X)  # Add a constant term (intercept)\nmodel = sm.OLS(y, X).fit()\nsalaryhat = model.fittedvalues\nuhat = model.resid\nsalarymean = np.repeat(y.mean(), len(y))\n# Calculate R-squared\nssr = np.sum(uhat**2)\nssrB = np.sum((y - salaryhat)**2)\nsst = np.sum((y - salarymean)**2)\nsse = np.sum((salaryhat - salarymean)**2)\nrsquared = sse / sst\nprint(rsquared)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nqui reg salary roe \npredict salaryhat , xb\t\t\t\t\npredict uhat, resid\t\t\t\t\t\negen salarymean = mean(salary)\t\t\negen sst  = total((salary    - salarymean)^2)  \negen ssr  = total((salary    - salaryhat)^2)\negen ssrB = total(uhat^2)\t\t\t\t\t\negen sse  = total((salaryhat - salarymean)^2)\t\ndi sse / sst\n```  \n\n:::\n\n\n\n\n\n\n\n# Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\n## Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\nWhen we estimate coefficients we have some \"error of estimation\".\n\n- Basically, you are searching the \"true\" coefficient using a sample, which should be representative of the population but it is not the population itself.\n\n- This means that the coefficient estimated is estimated with error.\n\n- We would like (e.g., we will need) to impose some \"structure\" to that error.\n\n\n\n\n\n\n## Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\n**Standard error and T-stat**\n\nTo assess if the variables are significantly related, you need to assess the significance of $\\beta$ coefficients.\n\nUsing the example from Wooldridge, we know that the Beta of ROE is `18.591`, while the standard error of ROE is `11.123`.\n\n. . .\n\n- The standard error is a measure of the accuracy of your estimate. If you find a large standard error, your estimate does not have good accuracy. \n\n- Ideally, you would find small standard errors, meaning that your coefficient is accurately estimated. \n\n- However, you do not have good control over the magnitude of the standard errors. \n\n\n\n\n\n\n## Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\n**Standard error and T-stat**\n\nIf you have a large standard error, probably you coefficient will not be significantly different from zero. You can test whether your coefficient is significantly different from zero computing the t-statistics as follows:\n\n$$t_{\\beta} = \\frac{\\hat{\\beta}}{se(\\hat{\\beta})}$$\n\nIf $t_{\\beta}$ is large enough, you can say that $\\beta$ is significantly different from zero. Usually, $t_{\\beta}$ larger than 2 is enough to be significant. \n\n\n\n\n\n## Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\nIn the previous example, you can find the t-stat manually as follows ($t_{\\beta} =\\frac{\\hat{\\beta}}{se(\\hat{\\beta})}$):\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) # importing dataset from a stata dta file\ndata <- read.dta(\"files/CEOSAL1.dta\")\nattach(data)\n# OLS model\nmodel <- lm(salary ~ roe)\nsummary(model)$coefficients[2,1] / summary(model)$coefficients[2,2] \nsummary(model)$coefficients[2,3]\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\n# OLS model\nX = data['roe']\nX = sm.add_constant(X)\ny = data['salary']\nmodel = sm.OLS(y, X).fit()  \n# Extract and calculate specific coefficients\ncoef_beta = model.params['roe']\ncoef_std_error = model.bse['roe']\n# Calculate t-value\nt_value = coef_beta / coef_std_error\n# Print the coefficient and t-value\nprint(\"Coefficient (beta):\", coef_beta)\nprint(\"Standard Error:\", coef_std_error)\nprint(\"t-value:\", t_value)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nqui reg salary roe \nlocal beta = _b[roe]\nlocal std_error = _se[roe]\nlocal t_value = `beta' / `std_error'\ndisplay \"Coefficient (beta): \" `beta'\ndisplay \"Standard Error: \" `std_error'\ndisplay \"t-value: \" `t_value'\n```  \n\n:::\n\n\n\n\n\n\n\n## Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\nNaturally, the previous analysis requires an estimate of $\\beta$ and an estimate of the $\\beta$'s standard error.\n\n\nThe standard error can be defined as:\n\n$$se(\\hat{\\beta_1})=\\frac{\\hat{\\sigma}}{\\sqrt{SST_x}}$$\n\n- Where $\\hat{\\sigma}$ is the standard deviation of the error term in the regression, which can be calculated as:\n\n$$\\hat{\\sigma} = \\sqrt{\\frac{SSR}{n-2}}$$\n\n    - The $n-2$ here is an adjustment for the degrees of freedom in the regression.\n\n- SST is defined as before $\\sum_i^n(y_i-\\hat{y_i})^2$ \n\n\n\n\n\n\n\n\n\n\n## Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) # importing dataset from a stata dta file\ndata <- read.dta(\"files/CEOSAL1.dta\")\nattach(data)\n# OLS model\nmodel <- lm(salary ~ roe)\n# Extract the standard error of the coefficient for 'roe'\nsummary(model)$coefficients[\"roe\", \"Std. Error\"]\n\n#calculating manually\n# Extract the residuals\nresiduals <- resid(model)\n# Number of observations (n)\nn <- length(residuals)\n# Calculate the mean of the independent variable (roe)\nroe_mean <- mean(roe)\n# Calculate the sum of squared deviations of roe from its mean (SXX)\nSST <- sum((roe - roe_mean)^2)\n# Calculate the sum of squared errors (SSE)\nSSR <- sum(residuals^2)\n# Calculate the standard error of beta\nSd_beta <- sqrt(SSR / ((n - 2)))\n# Calculate S.E\nSe_beta <- Sd_beta / sqrt(SST)\n# Print the standard error of beta\nprint(Se_beta)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\nX = data['roe']\ny = data['salary']\nX = sm.add_constant(X)  \nmodel = sm.OLS(y, X).fit()\n# Extract the standard error of the coefficient for 'roe'\nbeta_se_summary = model.bse['roe']\nprint(\"Standard Error (from summary):\", beta_se_summary)\n# Calculate it manually\n# Extract the residuals\nresiduals = model.resid\n# Number of observations (n)\nn = len(residuals)\n# Calculate the mean of the independent variable (roe)\nroe_mean = X['roe'].mean()\n# Calculate the sum of squared deviations of roe from its mean (SST)\nSST = np.sum((X['roe'] - roe_mean) ** 2)\n# Calculate the sum of squared errors (SSE)\nSSE = np.sum(residuals ** 2)\n# Calculate the standard error of beta (Sd_beta)\nSd_beta = np.sqrt(SSE / (n - 2))\n# Calculate SE_beta\nSE_beta = Sd_beta / np.sqrt(SST)\nprint(\"Standard Error (manually calculated):\", SE_beta)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nqui reg salary roe \ngen beta_se_summary = _se[roe]\ngen n = _N\npredict residuals, residuals\nsum roe, meanonly\ngen roe_diff = roe - r(mean)\negen roe_diff_sq = total(roe_diff^2)\ngen residuals_sq = residuals^2\negen residuals_sq_sum = total(residuals_sq)\ngen Sd_beta = sqrt(residuals_sq_sum / (n - 2))\ngen SE_beta = Sd_beta / sqrt(roe_diff_sq)\ndisplay \"Standard Error (from summary): \" sum(beta_se_summary)\ndisplay \"Standard Error (manually calculated): \" sum(SE_beta)\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n## Variance of coefficients {.smaller background=\"#c4f5d7\"}\n\n**Another comment:**\n\n$$se(\\hat{\\beta_1})=\\frac{\\hat{\\sigma}}{\\sqrt{SST_x}}$$\n\n1) The larger $\\hat{\\sigma}$ is, the larger the variance of $\\beta$. That is, the more \"noise\" in the association between x and Y, the harder it is to learn something about $\\beta$.\n\n2) However, more variation in x, the larger the SST, so the smaller is the variance of $\\beta$.\n\n\n\n\n\n\n\n\n\n# Robust standard errors {.smaller background=\"#e0cafc\"}\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\nLooking at both equations below:\n\n$$t_{\\beta} = \\frac{\\hat{\\beta}}{se(\\hat{\\beta})}$$\n\n\n$$se(\\hat{\\beta_1})=\\frac{\\hat{\\sigma}}{\\sqrt{SST_x}}$$\n\n\n**What happens if $\\hat{\\sigma}$ is not constant (for the values of x)?**\n\n**In other words, how realistic is to assume that the variance in the errors is the same for all slices of x?**\n\n**Can you think of an example where that may happen?**\n\n\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\n**Earnings = f(education)**\n\nPhD have a higher variance of earnings than non-educated people.\n\n. . .\n\n**Leveragge=f(Size)**\n\nIt is quite possible that small firms will have less options of leverage than large companies. \n\nThis means that a sub-sample of large companies will have higher variance in the leverage decisions (and thus the error terms) than the sub-sample of small firms\n\n\n\n\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\nOne of the key assumptions in OLS estimators is homoscedasticity \n\nThat is, the assumption is that the variance of the errors is homoscedastic (constant variance in all slices of X). \n\nIt means that throughout all observations, the error term shows the **same variance**. \n\nIf errors are not homoscedastic, we have the heteroscedasticity problem.\n\n\n. . . \n\nHeteroskedasticity **does not cause bias or inconsistency in the OLS estimators** of the $\\beta$ like the OVB would. \n\nIt also does not affect the $R^2$. \n\n**What Heteroscedasticity does is to bias the standard errors of the estimates.**\n\n\n\n\n\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\n![](files/homoscedasticity.png)\n\n\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\n\n\n![](files/heteroscedasticity.png)\n\n\n\n\n\n\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\n**Homoskedascticity** = Constant $\\hat{\\sigma}$ to all slices of X.\n\n**Heteroskedascticity** = Non-constant $\\hat{\\sigma}$ to all slices of X.\n\n**Without homoskedascticity, OLS no longer has the minimum mean squared errors**, which means that the *estimated standard errors are biased*, which in turn creates bias in the t-stat and the inference you'll make with your model.\n \n\n. . . \n\nFortunately, we have an easy solution for that.\n\n\n$$Var(\\hat{\\beta_1}) = \\frac{\\sum_i^n(x_i-\\bar{x})^2\\hat{\\mu}^2}{SST^2_x}$$\n\nThis formula simply \"includes\" the heteroskedascticity in the calculation of $Var(\\hat{\\beta_1})$, meaning this correct the estimated standard deviation to heteroskedascticity.\n\nWe call this correction as **Robust Standard Errors** (White Robust).\n\n. . .\n\nIn other words, you should always use Robust Standard Errors. It is easy to use it with R.\n\n\n\n\n\n\n\n\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\n**Using Robust Standard-errors.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(sandwich)\nlibrary(foreign) \nlibrary(lmtest)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\nmodel <- lm(salary ~ roe, data = data)\nrobust_model <- coeftest(model, vcov = vcovHC(model, type = \"HC3\"))\nSE_beta_robust <- robust_model[\"roe\", \"Std. Error\"]\ncat(\"Robust Standard Error :\", SE_beta_robust, \"\\n\")\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\ndata = pd.read_stata(\"files/CEOSAL1.DTA\")\nmodel = smf.ols(\"salary ~ roe\", data=data)\nresults = model.fit(cov_type='HC3')  \nSE_beta_robust = results.bse['roe']\nprint(\"Robust Standard Error :\", SE_beta_robust)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\n\nqui reg salary roe \ngen beta_se_non = _se[roe]\n\nqui reg salary roe , robust\ngen beta_se_summary = _se[roe]\n\ndi \"Standard Error (non-robust): \" sum(beta_se_non)\ndi \"Standard Error (robust): \" sum(beta_se_summary)\n```  \n\n:::\n\n\n\n\n##  Robust standard errors {.smaller background=\"#e0cafc\"}\n\n\nNotice that the standard errors have changed quite significantly  in this example. \n\nUsually, the robust standard errors are larger than the traditional ones in empirical works.\n\n**But, in this example, they are smaller.**\n\n. . . \n\nPerhaps more importantly:\n\n**Once the S.e. change, you should expect that the t-stat of the estimates also change.**\n\n\n. . . \n\n\n**Final comment**: robust standard errors are robust in the case of homoskedasticity.\n\n::: {.callout-warning}\nThus, you should always use robust S.E.\n:::\n\n\n\n\n\n\n\n\n\n# Clustered standard errors {.smaller background=\"#edc5d1\"}\n\n## Clustered standard errors {.smaller background=\"#edc5d1\"}\n\nAlmost always, someone will ask you whether you clustered your standard errors.\n\n**The intuition is the following:**\n\n- When you do not cluster, you are assuming that all observations are independently and identically distributed (i.i.d.), which may or may not be true.\n\n- Imagine you are studying the effect of class size on students achievement.\n\n- How much of a effect would **have the teacher of a class**? \n\n. . . \n\n- In this design, the teacher influences the achievement of all the students in the same class, and one teacher cannot be at two classes at the same time.\n\n- Thus, it would be wise to cluster the errors at the class-level. This assumes that the residual of each individual is clustered with the other individuals in the same class.\n\n\n. . .\n\nIn principle, clustering solves any form of dependence of the residuals in your data.\n\n\n\n\n\n\n\n\n## Clustered standard errors {.smaller background=\"#edc5d1\"}\n\nIn corporate finance/accounting research panel data research, the tradition is to cluster at the **firm-level**.\n\n- The reason is that the observations of the same firm are not independent trough time, thus are correlated. \n\nBut, there is a lot of debate about this decision. \n\n. . .\n\nThe tip is to cluster where the **randomness exist**. That is quite subjective. In the class size example, the **randomness** comes out of the teacher, since each teacher has their own ways of teaching (materials, resources, etc.).\n\n. . .\n \nBut, it is a good practice to stress this decision a bit in your own research by **also showing results with clustered s.e. at the industry-level**.\n\n. . .\n\n**Final tip**: usually the minimum number of cluster is about 30. Less than that might be insufficient (but, again, the guidance in this topic is very subjective).  \n\n\n\n \n\n\n## Clustered standard errors {.smaller background=\"#edc5d1\"}\n\nThe clustered standard errors are different because I am fabricating the clusters here for the sake of the coding.\n\nIn your real research, you would have the cluster at hands. \n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(sandwich)\nlibrary(foreign) \nlibrary(lmtest)\nlibrary(plm)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\nmodel <- lm(salary ~ roe, data = data)\nrobust_model <- coeftest(model, vcov = vcovHC(model, type = \"HC3\"))\n#clustered\ndata$cluster <- rep(1:35, length.out = nrow(data))\nmodel <- plm(salary ~ roe, data = data, index = c(\"cluster\"))\n\nclustered_se <- vcovHC(model, type = \"HC3\", cluster = \"group\")\nSE_beta_clustered <- sqrt(clustered_se[\"roe\", \"roe\"])\n\ncat(\"Standard Error (robust):\", SE_beta_robust, \"\\n\")\ncat(\"Standard Error (clustered)::\", SE_beta_clustered, \"\\n\")\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Read the dataset\ndata = pd.read_stata(\"files/CEOSAL1.DTA\")\n\n# Create a new variable 'cluster' with cluster numbers ranging from 1 to 35\ndata['cluster'] = list(range(1, 36)) * (len(data) // 35)\n\n# Fit the linear regression model\nmodel = sm.OLS(data['salary'], sm.add_constant(data['roe'])).fit()\n\n# Compute robust standard errors\nrobust_model = model.get_robustcov_results(cov_type='HC3')\nSE_beta_robust = robust_model.cov_params().loc['roe', 'roe'] ** 0.5\n\n# Fit the linear regression model with clustered standard errors\nmodel_clustered = sm.OLS(data['salary'], sm.add_constant(data['roe'])).fit(cov_type='cluster', cov_kwds={'groups': data['cluster']})\n\n# Extract the clustered standard errors for 'roe'\nclustered_se = model_clustered.HC3_se.loc['roe']\n\nprint(\"Robust Standard Error (HC3):\", SE_beta_robust)\nprint(\"Clustered Standard Error (HC3):\", clustered_se)\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\n\nqui reg salary roe \ngen beta_se_non = _se[roe]\n\nqui reg salary roe , robust\ngen beta_se_summary = _se[roe]\n\negen cluster = seq(), block(6)\nqui regress salary roe , vce(cluster cluster)\ngen SE_beta_clustered = _se[roe]\n\ndi \"Standard Error (non-robust): \" sum(beta_se_non)\ndi \"Standard Error (robust): \" sum(beta_se_summary)\ndi \"Standard Error (clustered): \" sum(SE_beta_clustered)\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n## **THANK YOU!** {background=\"#b1cafa\"}\n\n::: columns\n::: {.column width=\"60%\"}\n**QUESTIONS?**\n\n![](figs/qa2.png){width=\"150%\" heigth=\"150%\"}\n:::\n\n::: {.column width=\"40%\"}\n**Henrique C. Martins**\n\n-   [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)\n-   [Personal Website](https://henriquemartins.net/)\n-   [LinkedIn](https://www.linkedin.com/in/henriquecastror/)\n-   [Lattes](http://lattes.cnpq.br/6076997472159785)\n-   [Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)\\\n:::\n:::\n\n::: footer\n:::\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","css":["logo.css"],"output-file":"part_4.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.4.549","auto-stretch":true,"html":{"css":"webex.css","include-after-body":"webex.js"},"editor":"visual","title":"Empirical Methods in Finance","subtitle":"Part 4","author":"Henrique C. Martins","title-slide-attributes":{"data-background-color":"#b1cafa"},"include-after":["<script type=\"text/javascript\">\n  Reveal.on('ready', event => {\n    if (event.indexh === 0) {\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n  });\n  Reveal.addEventListener('slidechanged', (event) => {\n    if (event.indexh === 0) {\n      Reveal.configure({ slideNumber: null });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n    if (event.indexh === 1) {\n      Reveal.configure({ slideNumber: 'c' });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n    }\n  });\n</script>\n"],"slideNumber":true,"theme":"simple","chalkboard":true,"previewLinks":"auto","logo":"figs/background8.png","footer":"**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  ","multiplex":true,"scrollable":true}}},"projectFormats":["html"]}