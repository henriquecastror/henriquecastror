{"title":"Empirical Methods in Finance","markdown":{"yaml":{"title":"Empirical Methods in Finance","subtitle":"Practicing 5","author":"Henrique C. Martins","format":{"revealjs":{"slide-number":true,"theme":"simple","chalkboard":true,"preview-links":"auto","logo":"figs/background8.png","css":"logo.css","footer":"**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  ","multiplex":true,"scrollable":true}},"title-slide-attributes":{"data-background-color":"#b1cafa"},"include-after":"<script type=\"text/javascript\">\n  Reveal.on('ready', event => {\n    if (event.indexh === 0) {\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n  });\n  Reveal.addEventListener('slidechanged', (event) => {\n    if (event.indexh === 0) {\n      Reveal.configure({ slideNumber: null });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n    if (event.indexh === 1) {\n      Reveal.configure({ slideNumber: 'c' });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n    }\n  });\n</script>\n"},"headingText":"library(reticulate)","containsRefs":false,"markdown":"\n\n```{r setup}\n#| include: false\n#| warning: false\n\n# use_python(\"C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe\")\nlibrary(reticulate)\nlibrary(Statamarkdown)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggthemes)\n#reticulate::py_install(\"matplotlib\")\n#reticulate::py_install(\"seaborn\")\n#reticulate::py_install(\"pyfinance\")\n#reticulate::py_install(\"xlrd\")\n#reticulate::py_install(\"quandl\")\n#reticulate::py_install(\"linearmodels\")\n#reticulate::py_install(\"causalml\")\n\n```\n\n\n\n\n# Linear regression {.smaller}\n\n## Regression Basics {.smaller} \n\n\nLet's learn how to run a regression in R and how to compute the Beta.\n\nFirst, let's remember what a linear regression is. In a linear regression, we want to estimate the population paramenters $\\beta_0$ and $\\beta_1$ of the following model.\n\n$$ y = \\beta_0 + \\beta_1 \\times x + \\mu$$\n\nIn this setting, the variables $y$ and $x$ can have several names.\n\n| Y                  | X                    |\n|--------------------|----------------------|\n| Dependent variable | Independent variable |\n| Explained variable | Explanatory variable |\n| Response variable  | Control variable     |\n| Predicted variable | Predictor variable   |\n| Regressand         | Regressor            |\n\n\n## Regression Basics {.smaller} \n\nThe variable $\\mu$, called the error term, represents all factors that are $X$ that also affect $y$. These factors are unobserved in your model. It has specific properties and assumptions. \n\nThe parameter $\\beta_0$, i.e., the intercept, is often called the *constant term*, but it is rarely useful in the type of analysis we'll run. \n\nWe can estimate these parameters as follows:\n\n$$\\beta_1 = \\frac{Cov(x,y)}{Var(x)}$$\n\n$$\\beta_0 = \\hat{y} - \\hat{\\beta_1} \\hat{x}$$\n\n\n## Regression Basics {.smaller} \n\n\nLet's estimate Example 2.3 of Wooldridge (2020).\n\n```{r}\n#| warning: false\n#| echo: true\n#| fig-align: center\n#| message: false\n#| fig-width: 7\n#| fig-height: 4\nlibrary(wooldridge)\ndata(ceosal1)\nreg <- lm(ceosal1$salary ~ ceosal1$roe)\nsummary(reg)\n```\n\n\n## Regression Basics {.smaller} \n\nLet's find the parameters manually now.\n\n```{r}\n#| warning: false\n#| echo: true\n#| fig-align: center\n#| message: false\n#| fig-width: 7\n#| fig-height: 4\ncov(ceosal1$roe, ceosal1$salary)\nvar(ceosal1$roe)\nmean(ceosal1$salary)\nmean(ceosal1$roe)\n(b1 <- cov(ceosal1$roe, ceosal1$salary)/var(ceosal1$roe))\n(b0 <- mean(ceosal1$salary) - b1*mean(ceosal1$roe))\n```\n\n\n\n\n\n\n\n\n\n## Graph  {.smaller} \n\nLet's visualize the relationship between these variables now.\n\n```{r}\n#| warning: false\n#| echo: true\n#| fig-align: center\n#| message: false\n#| fig-width: 7\n#| fig-height: 4\nlibrary(ggplot2)\nlibrary(ggthemes)\nggplot(ceosal1) +  geom_point( aes(x=roe, y=salary)) +\n                   geom_smooth(data = ceosal1, aes(x=roe, y=salary) , method = lm) + \n                   theme_solarized()\n```\n\nSo we see the positive relationship between a firm's ROE and the Salary paid to the CEO as the regression showed before.\n\n\n\n\n\n\n\n## Regression's coefficients {.smaller} \n\nLet's run the regression again.\n\n```{r}\n#| warning: false\n#| fig-align: center\n#| message: false\n#| fig-width: 7\n#| fig-height: 4\n#| collapse: true\n#| echo: true\nmodel <- lm(ceosal1$salary ~ ceosal1$roe)\nsummary(model)\n```\n\n\n## Regression's coefficients {.smaller} \n\nBeta of ROE is **`r round( summary(model)$coefficients[2,1], 3)`**\n\nStandard error of ROE is **`r round( summary(model)$coefficients[2,2], 3)`**.\n\nT-stat of ROE is **`r round( summary(model)$coefficients[2,3], 3)`**.\n\nAnd p-value is **`r round( summary(model)$coefficients[2,4], 3)`**. So it is barely significantly different from zero at the 10% threshold. \n\nThe intercept is **`r round( summary(model)$coefficients[1,1], 3)`**.\n\n\n\n\n\n\n\n\n\n\n## Predicting salary {.smaller} \n\nThe line from the previous graph contains the estimated salary value of each CEO.\n\nLet's say that Firm A shows a ROE of `r round(ceosal1$roe[1],3)` and a Salary of `r round(ceosal1$salary[1], 3)`. You know that the Beta of the linear regression is `r round( summary(model)$coefficients[2,1], 3)` and the intercept is `r round( summary(model)$coefficients[1,1], 3)`. Using these estimates, you can estimate that the salary of Firm A's CEO is:\n\n$$`r round(ceosal1$roe[1],3)` \\times  `r round( summary(model)$coefficients[2,1], 3)`  + `r round( summary(model)$coefficients[1,1], 3)` = `r round( ceosal1$roe[1] *summary(model)$coefficients[2,1]  +  summary(model)$coefficients[1,1] , 3)`$$\n\nIf you do the same to all observations in the dataset, you get the red points below. The darkgreen line connects the points and represents the association between ROE and Salary.\n\nNow, if you can trust your estimates you can estimate (\"predict\") the salary for a given new ROE. For instance, you could estimate what is the salary of a new firm that shows a ROE of, let's say, 30%.\n\n\n\n\n\n\n\n## R-squared {.smaller} \n\nWe can see in the previous example that the r-squared of the regression is `r round (var(fitted(model))/ var(ceosal1$salary) * 100 , 3)` percent. This means that the variable ROE explains around 1.3% of the variation of Salary.\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \n\nsummary(model)$r.squared\nsummary(model)$adj.r.squared\n```\n\n\n\n## Residual {.smaller} \n\nNotice in the graph above that there is a distance between the **\"real\"** values of salary (i.e., blue points) and the **estimated** values of salary (i.e., the red points).\n\nThis distance is called **error** or **residual**.\n\nOne thing that you need to understand is that, in a OLS model, the regression line is selected in a way that **minimizes the sum of the squared values of the residuals**.\n\nYou can compute the errors as follows.\n\nNotice that most residuals are very close to zero. This basically shows that the red points (i.e., the estimated value) are very close to the blue points (i.e., the \"real\" data) in most of the observations.\n\nBut notice there are some observations with large residual, showing they are very far from the estimated value.\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \nceosal1$uhat <- resid(model)\nggplot(ceosal1) +  geom_point( aes(x=roe, y=uhat), color = \"darkred\") +\n                   theme_solarized()\n```\n\nThe residuals have a mean of zero.\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \nsummary(ceosal1$uhat) \n```\n\n\n\n\n## Standard error and T-stat {.smaller} \n\n\nTo assess if the variables are significantly related, you need to assess the significance of $\\beta$ coefficients.\n\nUsing the example from Wooldridge, we know that the Beta of ROE is **`r round( summary(model)$coefficients[2,1], 3)`**, while the standard error of ROE is **`r round( summary(model)$coefficients[2,2], 3)`**.\n\nThe standard error is a measure of the accuracy of your estimate. If you find a large standard error, your estimate does not have good accuracy. Ideally, you would find small standard errors, meaning that your coefficient is accurately estimated. However, you do not have good control over the magnitude of the standard errors. \n\n\n## Standard error and T-stat {.smaller} \n\nIf you have a large standard error, probably you coefficient will not be significantly different from zero. You can test whether your coefficient is significantly different from zero computing the t-statistics as follows:\n\n$$t_{\\beta} = \\frac{\\hat{\\beta}}{se(\\hat{\\beta})}$$\n\nIf $t_{\\beta}$ is large enough, you can say that $\\beta$ is significantly different from zero.  Usually, $t_{\\beta}$ larger than 2 is enough to be significant. \n\n\n\n\n\n## Standard error and T-stat {.smaller} \n\nIn the previous example, you can find the  t-stat manually as follows:\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \nsummary(model)$coefficients[2,1] / summary(model)$coefficients[2,2] \nsummary(model)$coefficients[2,3]\n```\n\nLarge t-stats will lead you to low p-values. Usually, we interpret that p-values lower than 10% suggest significance, but you would prefer p-values lower than at least 5%.\n\nYou can find the p-values of the previous example as follows.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \nsummary(model)$coefficients[2,4]\npt(summary(model)$coefficients[2,3], 207, lower.tail=FALSE) * 2\n```\n\nIn this case, p-value is lower than 10% so you can make the case that the relationship is significant at the 10% level. But notice that the relationship is not significant at the level of 5%.\n\n\n\n\n\n## Confidence intervals {.smaller} \n\nWe can further explore significance calculating confidence intervals. First, let's compute the intervals at 5%. Because our test is a two-tailed test, 5% means 2.5% in each tail.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \nconfint(model)\n```\n\n\n\n## Confidence intervals {.smaller} \n\n\nWe can see above that the interval contains the value of Zero (notice that the estimate of ROE goes from a negative to a positive value, thus containing the zero). This means that you cannot separate your estimate of Beta from zero. In other words, your coefficient is not *significantly different from zero* in this case. This supports the previous finding that the coefficient is not significantly different from zero at the level of 5%.\n\n\n## Confidence intervals {.smaller} \n\n\nLet's see what are the confidence intervals at the level of 10%.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \nconfint(model, level = 0.90)\n```\n\n\nNow, we can see that the interval does not contain zero, which means that Beta is *significantly different from zero* at the level of 10%. Again, it confirms the previous findings.  \n\n\n\n\n\n\n\n\n## Multiple regressions {.smaller} \n\nA multiple regression follows a model like:\n\n$$ y = \\beta_0 + \\beta_1 \\times x_1 + \\beta_2 \\times x_2 +  ... + \\beta_k \\times x_k + \\mu$$\n\n## Multiple regressions {.smaller} \n\nLet's estimate this equation using Example 3.1 of Wooldridge.\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \nlibrary(wooldridge)\ndata(gpa1)\ngpa <- lm(gpa1$colGPA ~ gpa1$hsGPA + gpa1$ACT)\nsummary(gpa)\n\n```\n\n\n\n\n\n\n\n\n\n\n## **THANK YOU!** {background=\"#b1cafa\"}\n\n::: columns\n::: {.column width=\"60%\"}\n**QUESTIONS?**\n\n![](figs/qa2.png){width=\"150%\" heigth=\"150%\"}\n:::\n\n::: {.column width=\"40%\"}\n**Henrique C. Martins**\n\n-   [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)\n-   [Personal Website](https://henriquemartins.net/)\n-   [LinkedIn](https://www.linkedin.com/in/henriquecastror/)\n-   [Lattes](http://lattes.cnpq.br/6076997472159785)\n-   [Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)\\\n:::\n:::\n\n::: footer\n:::\n","srcMarkdownNoYaml":"\n\n```{r setup}\n#| include: false\n#| warning: false\n\n# library(reticulate)\n# use_python(\"C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe\")\nlibrary(reticulate)\nlibrary(Statamarkdown)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggthemes)\n#reticulate::py_install(\"matplotlib\")\n#reticulate::py_install(\"seaborn\")\n#reticulate::py_install(\"pyfinance\")\n#reticulate::py_install(\"xlrd\")\n#reticulate::py_install(\"quandl\")\n#reticulate::py_install(\"linearmodels\")\n#reticulate::py_install(\"causalml\")\n\n```\n\n\n\n\n# Linear regression {.smaller}\n\n## Regression Basics {.smaller} \n\n\nLet's learn how to run a regression in R and how to compute the Beta.\n\nFirst, let's remember what a linear regression is. In a linear regression, we want to estimate the population paramenters $\\beta_0$ and $\\beta_1$ of the following model.\n\n$$ y = \\beta_0 + \\beta_1 \\times x + \\mu$$\n\nIn this setting, the variables $y$ and $x$ can have several names.\n\n| Y                  | X                    |\n|--------------------|----------------------|\n| Dependent variable | Independent variable |\n| Explained variable | Explanatory variable |\n| Response variable  | Control variable     |\n| Predicted variable | Predictor variable   |\n| Regressand         | Regressor            |\n\n\n## Regression Basics {.smaller} \n\nThe variable $\\mu$, called the error term, represents all factors that are $X$ that also affect $y$. These factors are unobserved in your model. It has specific properties and assumptions. \n\nThe parameter $\\beta_0$, i.e., the intercept, is often called the *constant term*, but it is rarely useful in the type of analysis we'll run. \n\nWe can estimate these parameters as follows:\n\n$$\\beta_1 = \\frac{Cov(x,y)}{Var(x)}$$\n\n$$\\beta_0 = \\hat{y} - \\hat{\\beta_1} \\hat{x}$$\n\n\n## Regression Basics {.smaller} \n\n\nLet's estimate Example 2.3 of Wooldridge (2020).\n\n```{r}\n#| warning: false\n#| echo: true\n#| fig-align: center\n#| message: false\n#| fig-width: 7\n#| fig-height: 4\nlibrary(wooldridge)\ndata(ceosal1)\nreg <- lm(ceosal1$salary ~ ceosal1$roe)\nsummary(reg)\n```\n\n\n## Regression Basics {.smaller} \n\nLet's find the parameters manually now.\n\n```{r}\n#| warning: false\n#| echo: true\n#| fig-align: center\n#| message: false\n#| fig-width: 7\n#| fig-height: 4\ncov(ceosal1$roe, ceosal1$salary)\nvar(ceosal1$roe)\nmean(ceosal1$salary)\nmean(ceosal1$roe)\n(b1 <- cov(ceosal1$roe, ceosal1$salary)/var(ceosal1$roe))\n(b0 <- mean(ceosal1$salary) - b1*mean(ceosal1$roe))\n```\n\n\n\n\n\n\n\n\n\n## Graph  {.smaller} \n\nLet's visualize the relationship between these variables now.\n\n```{r}\n#| warning: false\n#| echo: true\n#| fig-align: center\n#| message: false\n#| fig-width: 7\n#| fig-height: 4\nlibrary(ggplot2)\nlibrary(ggthemes)\nggplot(ceosal1) +  geom_point( aes(x=roe, y=salary)) +\n                   geom_smooth(data = ceosal1, aes(x=roe, y=salary) , method = lm) + \n                   theme_solarized()\n```\n\nSo we see the positive relationship between a firm's ROE and the Salary paid to the CEO as the regression showed before.\n\n\n\n\n\n\n\n## Regression's coefficients {.smaller} \n\nLet's run the regression again.\n\n```{r}\n#| warning: false\n#| fig-align: center\n#| message: false\n#| fig-width: 7\n#| fig-height: 4\n#| collapse: true\n#| echo: true\nmodel <- lm(ceosal1$salary ~ ceosal1$roe)\nsummary(model)\n```\n\n\n## Regression's coefficients {.smaller} \n\nBeta of ROE is **`r round( summary(model)$coefficients[2,1], 3)`**\n\nStandard error of ROE is **`r round( summary(model)$coefficients[2,2], 3)`**.\n\nT-stat of ROE is **`r round( summary(model)$coefficients[2,3], 3)`**.\n\nAnd p-value is **`r round( summary(model)$coefficients[2,4], 3)`**. So it is barely significantly different from zero at the 10% threshold. \n\nThe intercept is **`r round( summary(model)$coefficients[1,1], 3)`**.\n\n\n\n\n\n\n\n\n\n\n## Predicting salary {.smaller} \n\nThe line from the previous graph contains the estimated salary value of each CEO.\n\nLet's say that Firm A shows a ROE of `r round(ceosal1$roe[1],3)` and a Salary of `r round(ceosal1$salary[1], 3)`. You know that the Beta of the linear regression is `r round( summary(model)$coefficients[2,1], 3)` and the intercept is `r round( summary(model)$coefficients[1,1], 3)`. Using these estimates, you can estimate that the salary of Firm A's CEO is:\n\n$$`r round(ceosal1$roe[1],3)` \\times  `r round( summary(model)$coefficients[2,1], 3)`  + `r round( summary(model)$coefficients[1,1], 3)` = `r round( ceosal1$roe[1] *summary(model)$coefficients[2,1]  +  summary(model)$coefficients[1,1] , 3)`$$\n\nIf you do the same to all observations in the dataset, you get the red points below. The darkgreen line connects the points and represents the association between ROE and Salary.\n\nNow, if you can trust your estimates you can estimate (\"predict\") the salary for a given new ROE. For instance, you could estimate what is the salary of a new firm that shows a ROE of, let's say, 30%.\n\n\n\n\n\n\n\n## R-squared {.smaller} \n\nWe can see in the previous example that the r-squared of the regression is `r round (var(fitted(model))/ var(ceosal1$salary) * 100 , 3)` percent. This means that the variable ROE explains around 1.3% of the variation of Salary.\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \n\nsummary(model)$r.squared\nsummary(model)$adj.r.squared\n```\n\n\n\n## Residual {.smaller} \n\nNotice in the graph above that there is a distance between the **\"real\"** values of salary (i.e., blue points) and the **estimated** values of salary (i.e., the red points).\n\nThis distance is called **error** or **residual**.\n\nOne thing that you need to understand is that, in a OLS model, the regression line is selected in a way that **minimizes the sum of the squared values of the residuals**.\n\nYou can compute the errors as follows.\n\nNotice that most residuals are very close to zero. This basically shows that the red points (i.e., the estimated value) are very close to the blue points (i.e., the \"real\" data) in most of the observations.\n\nBut notice there are some observations with large residual, showing they are very far from the estimated value.\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \nceosal1$uhat <- resid(model)\nggplot(ceosal1) +  geom_point( aes(x=roe, y=uhat), color = \"darkred\") +\n                   theme_solarized()\n```\n\nThe residuals have a mean of zero.\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \nsummary(ceosal1$uhat) \n```\n\n\n\n\n## Standard error and T-stat {.smaller} \n\n\nTo assess if the variables are significantly related, you need to assess the significance of $\\beta$ coefficients.\n\nUsing the example from Wooldridge, we know that the Beta of ROE is **`r round( summary(model)$coefficients[2,1], 3)`**, while the standard error of ROE is **`r round( summary(model)$coefficients[2,2], 3)`**.\n\nThe standard error is a measure of the accuracy of your estimate. If you find a large standard error, your estimate does not have good accuracy. Ideally, you would find small standard errors, meaning that your coefficient is accurately estimated. However, you do not have good control over the magnitude of the standard errors. \n\n\n## Standard error and T-stat {.smaller} \n\nIf you have a large standard error, probably you coefficient will not be significantly different from zero. You can test whether your coefficient is significantly different from zero computing the t-statistics as follows:\n\n$$t_{\\beta} = \\frac{\\hat{\\beta}}{se(\\hat{\\beta})}$$\n\nIf $t_{\\beta}$ is large enough, you can say that $\\beta$ is significantly different from zero.  Usually, $t_{\\beta}$ larger than 2 is enough to be significant. \n\n\n\n\n\n## Standard error and T-stat {.smaller} \n\nIn the previous example, you can find the  t-stat manually as follows:\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \nsummary(model)$coefficients[2,1] / summary(model)$coefficients[2,2] \nsummary(model)$coefficients[2,3]\n```\n\nLarge t-stats will lead you to low p-values. Usually, we interpret that p-values lower than 10% suggest significance, but you would prefer p-values lower than at least 5%.\n\nYou can find the p-values of the previous example as follows.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \nsummary(model)$coefficients[2,4]\npt(summary(model)$coefficients[2,3], 207, lower.tail=FALSE) * 2\n```\n\nIn this case, p-value is lower than 10% so you can make the case that the relationship is significant at the 10% level. But notice that the relationship is not significant at the level of 5%.\n\n\n\n\n\n## Confidence intervals {.smaller} \n\nWe can further explore significance calculating confidence intervals. First, let's compute the intervals at 5%. Because our test is a two-tailed test, 5% means 2.5% in each tail.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \nconfint(model)\n```\n\n\n\n## Confidence intervals {.smaller} \n\n\nWe can see above that the interval contains the value of Zero (notice that the estimate of ROE goes from a negative to a positive value, thus containing the zero). This means that you cannot separate your estimate of Beta from zero. In other words, your coefficient is not *significantly different from zero* in this case. This supports the previous finding that the coefficient is not significantly different from zero at the level of 5%.\n\n\n## Confidence intervals {.smaller} \n\n\nLet's see what are the confidence intervals at the level of 10%.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \nconfint(model, level = 0.90)\n```\n\n\nNow, we can see that the interval does not contain zero, which means that Beta is *significantly different from zero* at the level of 10%. Again, it confirms the previous findings.  \n\n\n\n\n\n\n\n\n## Multiple regressions {.smaller} \n\nA multiple regression follows a model like:\n\n$$ y = \\beta_0 + \\beta_1 \\times x_1 + \\beta_2 \\times x_2 +  ... + \\beta_k \\times x_k + \\mu$$\n\n## Multiple regressions {.smaller} \n\nLet's estimate this equation using Example 3.1 of Wooldridge.\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\niris <- iris \nlibrary(wooldridge)\ndata(gpa1)\ngpa <- lm(gpa1$colGPA ~ gpa1$hsGPA + gpa1$ACT)\nsummary(gpa)\n\n```\n\n\n\n\n\n\n\n\n\n\n## **THANK YOU!** {background=\"#b1cafa\"}\n\n::: columns\n::: {.column width=\"60%\"}\n**QUESTIONS?**\n\n![](figs/qa2.png){width=\"150%\" heigth=\"150%\"}\n:::\n\n::: {.column width=\"40%\"}\n**Henrique C. Martins**\n\n-   [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)\n-   [Personal Website](https://henriquemartins.net/)\n-   [LinkedIn](https://www.linkedin.com/in/henriquecastror/)\n-   [Lattes](http://lattes.cnpq.br/6076997472159785)\n-   [Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)\\\n:::\n:::\n\n::: footer\n:::\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","css":["logo.css"],"output-file":"part_O5.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.4.549","auto-stretch":true,"html":{"css":"webex.css","include-after-body":"webex.js"},"editor":"visual","title":"Empirical Methods in Finance","subtitle":"Practicing 5","author":"Henrique C. Martins","title-slide-attributes":{"data-background-color":"#b1cafa"},"include-after":["<script type=\"text/javascript\">\n  Reveal.on('ready', event => {\n    if (event.indexh === 0) {\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n  });\n  Reveal.addEventListener('slidechanged', (event) => {\n    if (event.indexh === 0) {\n      Reveal.configure({ slideNumber: null });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n    if (event.indexh === 1) {\n      Reveal.configure({ slideNumber: 'c' });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n    }\n  });\n</script>\n"],"slideNumber":true,"theme":"simple","chalkboard":true,"previewLinks":"auto","logo":"figs/background8.png","footer":"**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  ","multiplex":true,"scrollable":true}}},"projectFormats":["html","revealjs"]}