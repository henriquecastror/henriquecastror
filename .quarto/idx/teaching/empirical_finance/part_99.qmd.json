{"title":"Inferência Causal","markdown":{"yaml":{"title":"Inferência Causal","subtitle":"Part 2","author":"Henrique C. Martins","format":{"revealjs":{"slide-number":true,"theme":"simple","chalkboard":true,"preview-links":"auto","logo":"figs/background2.png","css":"styles.css","footer":"<https://eaesp.fgv.br/>","multiplex":true}}},"headingText":"library(reticulate)","containsRefs":false,"markdown":"\n\n```{r setup}\n#| include: false\n#| warning: false\n\n# use_python(\"C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe\")\nlibrary(reticulate)\nlibrary(Statamarkdown)\n#reticulate::py_install(\"matplotlib\")\n#reticulate::py_install(\"seaborn\")\n#reticulate::py_install(\"pyfinance\")\n#reticulate::py_install(\"xlrd\")\n#reticulate::py_install(\"quandl\")\n#reticulate::py_install(\"linearmodels\")\n#reticulate::py_install(\"causalml\")\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# DiD Introduction {.smaller background=\"#c4f5d7\"}\n\n## DiD Introduction  {.smaller background=\"#c4f5d7\"}\n\nLet's introduce DiD using the most famous example in the topic: John Snow’s 1855 findings that demonstrated to the world that cholera was spread by fecally-contaminated water and not via the air (Snow 1855)\n\n- Snow compared the periods of 1849 and 1854 to analyze the impact of a  change in London's water supply dynamics. \n- Various water companies, each drawing water from different sections of the Thames river, served the city's water needs. \n- The downstream areas of the Thames, where some companies sourced their water, were susceptible to contamination due to the disposal of various substances, including fecal matter from cholera-infected individuals. \n- In the interim between 1849 and 1854, a pivotal policy was implemented: the Lambeth Company was mandated by an Act of Parliament to relocate its water intake upstream of London.\n\n\n\n\n\n## DiD Introduction  {.smaller background=\"#c4f5d7\"}\n\n**This is what happened.**\n\n| Region Supplier                        | Death Rates 1849 | Death Rates 1854 |\n|----------------------------------------|------------------|------------------|\n| Non-Lambeth Only (Dirty)               | 134.9            | 146.6            |\n| Lambeth + Others (Mix Dirty and Clean) | 130.1            | 84.9             |\n\n. . . \n\nThe specific DID estimate we can get here is:\n\n- $(84.9-130.1)-(146.9-134.9)=-57.2$.\n\nThis resembles a \"modern\" DiD. \n\n\n\n\n\n\n\n\n# Shocks {.smaller background=\"#95c9a9\"}\n\n## Shocks  {.smaller background=\"#95c9a9\"}\n\n**Shock-based designs use an external shock to limit selection bias. **\n\nThey are very hard to find, but if you do, you can reasonably estimate causal effects.\n\nThey are sources of exogenous variations in the X, which are crucial to causality when we have some of the problems discussed before. \n\nA shock is often considered the first-best solution to causal inference (randomized control trials are the second-best).\n\n. . . \n\nIn social sciences, we cannot have randomized control trials. It is not feasible to have experiments.\n\nThat is why we often explore the idea of \"Natural experiments\" or \"Natural shocks\" (I often use the terms interchangeably)\n\n\n\n\n\n\n\n## Shocks  {.smaller background=\"#95c9a9\"}\n\nA **Natural experiment** is an exogenous variation that change a variable in a random subset of firms. \n\n- Regulations, laws, etc.\n- Natural disasters.\n- Sudden death of CEOs or a product, etc.\n- The gender of a newborn.\n\n\nBecause the variation that occur in x is truly exogenous, the CMI holds and thus we can infer causality.\n\n\n\n\n\n\n\n## Shocks  {.smaller background=\"#95c9a9\"}\n\nA good shock has some conditions [source](https://cfr.pub/published/papers/cfr-0036.pdf):\n\n1) **Shock Strength**: The shock is strong enough to significantly change firm behavior or incentives.\n\n. . .\n \n2) **Exogenous Shock**: The shock came from “outside” the system one is studying.\n\n  - Treated firms did not choose whether to be treated, \n  - cannot anticipate the shock, \n  - the shock is expected to be permanent, and \n  - there is no reason to believe that which firms were treated depends on unobserved firm characteristics.\n  \nIf the shock is exogenous, or appears to be, we are less worried that unobservables might be correlated with both assignment to treatment and the potential outcomes, and thus generate omitted variable bias. \n\nShock exogeneity should be defended, not just assumed.\n\n\n\n\n\n\n\n## Shocks  {.smaller background=\"#95c9a9\"}\n\nA good shock has some conditions [source](https://cfr.pub/published/papers/cfr-0036.pdf):\n\n3) **“As If Random” Assignment**: The shock must separate firms into treated and controls in a manner which is close to random.\n\n. . .\n\n4) **Covariate balance:** The forcing and forced variables aside, the shock should produce reasonable covariate balance between treated and control firms, including “common support” (reasonable overlap between treated and control firms on all covariates). \n\nSomewhat imperfect balance can be address with balancing methods, but severe imbalance undermines shock credibility, even if the reason for imbalance is not obvious. Covariate balance should be reported.\n\n\n\n\n\n\n\n## Shocks  {.smaller background=\"#95c9a9\"}\n\nA good shock has some conditions [source](https://cfr.pub/published/papers/cfr-0036.pdf):\n\n5) **Only-Through Condition(s):** We must have reason to believe that the apparent effect of the shock on the outcome came only through the shock (sometimes, through a specific channel). \n\nThe shock must be “isolated”, there must be no other shock, at around the same time, that could also affect treated firms differently than control firms. \nAnd if one expects the shock to affect outcomes through a particular channel, the shock must also affect the outcome only through that channel.\n\n. . .\n\nIn IV analysis, this is called an **“exclusion restriction”** or **“only-through condition\"**, because one assumes away (excludes) other channels.\n\n\n\n\n\n\n## Shocks  {.smaller background=\"#95c9a9\"}\n\nThe idea of a natural shock is often mixed with the difference-in-differences (DiD) design.\n\nIt is more common that it should that people refer to shocks when they want to refer to DiD. \n\nA Did design explores a Natural shock to estimate causal effects.\n\n**A good shock generates, by nature, random assignment**.\n\n\n\n\n\n\n\n\n\n## Remember  {.smaller background=\"#693d3a\"}\n\n**Remember:**\n\nWhen dealing with **causal inference**, we have to find ways to approximate what the hidden potential outcome of the treated units is. \n\nThat is, the challenge in identifying causal effects is that the untreated potential outcomes, $Y_{i,0}$, are never\nobserved for the treated group ($D_i= 1$). The \"second\" term in the following equation:\n\nATET = $\\frac{1}{N} (E[Y_{i,1}|D_i=1] - E[Y_{i,0}|D_i=1])$\n\nWe need an empirical design to **\"observe\"** what we do not really observe (i.e., the counterfactual). \n\n\n\n\n\n\n\n# Single differences  {.smaller background=\"#a4baac\"}\n\n## Single differences  {.smaller background=\"#a4baac\"}\n\nThere are two single differences we can use to estimate the causal effect of a treatments (i.e., a shock).\n\n**1) Single Cross-Sectional Differences After Treatment**\n\n**2) Single Time-Series Difference Before and After Treatment**\n\n\n\n\n\n\n\n## Single differences  {.smaller background=\"#a4baac\"}\n\n**1) Single Cross-Sectional Differences After Treatment**\n\nOne approach to estimating a parameter that summarizes the treatment effect is to compare the post-treatment outcomes of the treatment and control groups. \n\nThis method is often used when there is no data available on pre-treatment outcomes.\n\nIt takes the form:\n\n$$y=\\beta_0+\\beta_1d_i+\\epsilon$$\n\nwhere $d_i$ is a dummy marking the units that are treated. The treatment effect is given by $\\beta_1$\n\nThis is a cross-sectional comparison, using only post-treatment values.\n\n\n\n\n\n\n\n\n## Single differences  {.smaller background=\"#a4baac\"}\n\n**1) Single Cross-Sectional Differences After Treatment**\n\nYou may add the interaction between the treatment dummy and the years.\n\n$$y=\\beta_0+\\beta_1d_i\\times year_1+\\beta_2d_i\\times year_2 +\\beta_3d_i\\times year_3+ . . . +\\epsilon$$\n\nThis design allows the treatment effect to vary over time by interacting the treatment dummy with period dummies.\n\n\n\n\n. . . \n\n\n**What is the endogeneity concern here?**\n\n. . .\n\nThe endogeneity concern is that these firms’ $y$ were different and could become more different between the treated and control groups even if the shock had not happened. \n\nThat is, there is something in the residual that explains the differences in $y$ that is correlated with the $d$.\n\n\n\n\n\n\n\n\n\n## Single differences  {.smaller background=\"#a4baac\"}\n\n**2) Single Time-Series Difference Before and After Treatment**\n\nA second way to estimate the treatment effect is to compare the outcome after the treatment with the outcome before the treatment for **just those units that are treated.**\n\nThe difference from before is that you have data before the treatment, but you only have data for the treated units.\n\n$$y=\\beta_0+\\beta_1 time+\\epsilon$$\n\nwhere $time$ marks the years after the treatment. The treatment effect is given by $\\beta_1$\n\n\n\n\n\n\n## Single differences  {.smaller background=\"#a4baac\"}\n\n**2) Single Time-Series Difference Before and After Treatment**\n\nYou can also estimate a multi-year regression.\n\n$$y=\\beta_0+\\beta_1year_1 +\\beta_2\\times year_2 +\\beta_3\\times year_3+ . . . +\\epsilon$$\n\n. . . \n\n\n**What is the endogeneity concern here?**\n\n. . .\n\nThe endogeneity concern is that these firms’ $y$ could have changed over the period of observation even if the shock had not happened. \n\nThat is, there is something in the error term that explains $y$ that is correlated with the $year$ dummies.\n\n\n\n\n\n## Single differences  {.smaller background=\"#a4baac\"}\n\nThe combination of the  **1) Single Cross-Sectional Differences After Treatment** and **2) Single Time-Series Difference Before and After Treatment** is called **Difference-in-Differences**.\n\n\n\n\n\n\n\n\n# Difference-in-Differences {.smaller background=\"#8bc9a2\"}\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\nThis is one of the most popular methods in social sciences for estimating causal effects in non-experimental settings.\n\nThe literature is exploding over the recent years.\n\n. . . \n\nThere is one group that is treated, another is the control. \n\nThere are two periods of time, before and after the treatment.\n\nAnd the treated group receives the treatment in the second period.\n\n**The key identifying assumption is that the average outcome among the treated and comparison populations would have followed “parallel trends” in the absence of treatment.**\n\n\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\n\n*The two single difference estimators complement one another.*\n\n*The cross-sectional comparison avoids the problem of omitted trends by comparing two groups over the same time period.*\n\n*The time series comparison avoids the problem of unobserved differences between two different groups of firms by looking at the same firms before and after the change.*\n\n*The double difference, difference-in-differences (DD), estimator combines these two estimators to take advantage of both estimators’ strengths.* \n\n([Roberts and Whited, 2014](https://doi.org/10.1016/B978-0-44-453594-8.00007-0))\n\n\n\n\n\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\n\nDifference-in-differences methods overcome the identification challenge via assumptions that allow us to impute the mean counterfactual untreated outcomes for the treated group by using \n\n- (a) the change in outcomes for the untreated group and \n- (b) the baseline outcomes for the treated group. \n\nThe key assumption for identifying the causal effect is the **parallel trends assumption**, which intuitively states that **the average outcome for the treated and untreated units would have evolved in parallel if treatment had not occurred**.\n\n**Assumption 1 (Parallel Trends).**\n\n$E[Y_{i,1,t=2}-Y_{i,0,t=1}|D_i=1] = E[Y_{i,1,t=2}-Y_{i,0,t=1}|D_i=0]$\n\n\nIn other words: **this condition means that in the absence of treatment, the average change in the $y$ would have been the same for both the treatment and control groups.**\n\n\n\n\n\n\n\n\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\n**The counterfactual is determined by the assumption of a parallel trend between the treated and control groups.** **[MM](https://www.masteringmetrics.com/)**\n\n\n![](figs/did1.png) \n\n\n\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\n![](figs/slides4-did.png) \n\n\n\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\nInserting more periods.\n\n![](figs/did2.png) \n\n\n\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\nA more realistic example. ([The effect](https://theeffectbook.net/ch-DifferenceinDifference.html))\n\n\n![](figs/theeffect1.png)\n\n\n\n\n\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\n\n**Assumption 2 (No anticipatory effects).**\n\n*The treatment has no causal effect prior to its implementation.*\n\n$$Y_{i,0,t=1}=Y_{i,1,t=1}$$\n\nPrior to the treatment (t=1), there is no effect of the treatment.\n\n\n\n\n\n\n\n\n\n# Estimation  {.smaller background=\"#c6f7ec\"}\n\n## Estimation   {.smaller background=\"#c6f7ec\"}\n\nLet's see how to implement a DiD model. The canonical DiD model is the followin:\n\n$$y_{i,t} = \\beta_0 + \\beta_1 time_t + \\beta_2 treated_i  + \\beta_3 treated_i \\times time_t + \\epsilon_{i,t}$$\n\nWhere:\n\n$treated_i$ is a dummy marking the units that received the treatment.\n\n$time_t$ is a dummy marking the year of the treatment and the years after.\n\n\n\n\n\n\n## Estimation   {.smaller background=\"#c6f7ec\"}\n\nLet's see how to implement a DiD model. The canonical DiD model is the followin:\n\n$$y_{i,t} = \\beta_0 + \\beta_1 time_t + \\beta_2 treated_i  + \\beta_3 treated_i \\times time_t + \\epsilon_{i,t}$$\n\n\n$\\beta_0$ is the **average $y_{i,t}$** for the **control units before the treatment ($time_t$ =0 and $treated_i$ = 0)**.\n\n$\\beta_0+\\beta_1$ is the **average $y_{i,t}$** for the **control units after the treatment ($time_t$ =1 and $treated_i$ = 0)**.\n\n$\\beta_0+\\beta_2$ is the **average $y_{i,t}$** for the **treated units before the treatment ($time_t$ =0 and $treated_i$ = 1)**.\n\n$\\beta_0+\\beta_1+\\beta_2+\\beta_3$ is the **average $y_{i,t}$** for the **treated units after the treatment ($time_t$ =1 and $treated_i$ = 1)**.\n\n\n\n\n## Estimation   {.smaller background=\"#c6f7ec\"}\n\n+------------------+-----------------------------------+---------------------+---------------------+\n|                  |   Post-treat. (1)                 |  Pre-treat. (2)     |  **Diff. (1-2)**    |\n+------------------+-----------------------------------+---------------------+---------------------+\n|   Treated (a)    | $\\beta_0+\\beta_1+\\beta_2+\\beta_3$ | $\\beta_0+\\beta_2$   |**$\\beta_1+\\beta_3$**|\n+------------------+-----------------------------------+---------------------+---------------------+\n|  Control (b)     | $\\beta_0+\\beta_1$                 |  $\\beta_0$          | **$\\beta_1$**       |\n+------------------+-----------------------------------+---------------------+---------------------+\n| **Diff. (a-b)**  | **$\\beta_2+\\beta_3$**             |  **$\\beta_2$**      | **$\\beta_3$**       |\n+------------------+-----------------------------------+---------------------+---------------------+\n\n\n**This is why we call difference-in-differences.**\n\n**$\\beta_3$** gives us an estimate of the treatment effect on the treated units (ATE).\n\nThis is very popular because you can run as a regression.\n\n. . . \n\nMany papers simply show this table as final result.\n\nHowever, it is also possible that you include additional covariates (i.e., controls) in a DiD design. \n\nYou can also modify this to test the timing of the treatment (discussed later).\n\n\n\n\n\n\n\n\n\n\n# Controls in DiD  {.smaller background=\"#d5f5ee\"}\n\n## Controls in DiD  {.smaller background=\"#d5f5ee\"}\n\nIf you add controls, you have something in the form:\n\n$$y_{i,t} = \\beta_0 + \\beta_1 time_t + \\beta_2 treated_i  + \\beta_3 treated_i \\times time_t + \\beta_4x_{1,i,t}+ \\beta_5x_{2,i,t}+ ...+ \\epsilon_{i,t}$$\n\n. . . \n\n::: {.callout-important}\nBad Controls problem: Never add a control that is also affected by the treatment. \n:::\n\n. . . \n\n::: {.callout-tip}\nUsing pre-treatment values is good practice. \n:::\n\nIf you are including a *good control*, you should get lower standard deviations of the estimated effects.\n\n\n\n\n\n## Controls in DiD  {.smaller background=\"#d5f5ee\"}\n\nOne way of adding more controls is by adding FEs.\n\n$$y_{i,t} = \\beta_0 + \\beta_1 time_t + \\beta_2 treated_i  + \\beta_3 treated_i \\times time_t + FirmFE+ YearFE + \\epsilon_{i,t}$$\n\nThey help control for unobserved heterogeneity at the firm-level and in each year.\n\n. . . \n\n**But what do $\\beta_1$ and  $\\beta_2$ mean now?**\n\n. . .\n \n**Answer:** They are perfectly correlated with the $FirmFE$ and $YearFE$, respectively.\n\nReason: because they don't vary across each firm and year, respectively. \n\nThe software will most likely drop one $FirmFE$ and one $YearFE$, thus $\\beta_1$ and  $\\beta_2$ mean nothing.\n\n. . .\n\nI am guilty of this mistake because a referee asked and I couldn't reply properly.\n\n\n\n\n\n\n\n\n## Controls in DiD  {.smaller background=\"#d5f5ee\"}\n\nYou will be better off if you estimate:\n\n$$y_{i,t} = \\beta_0 + \\beta_3 treated_i \\times time_t + FirmFE+ YearFE + \\epsilon_{i,t}$$\n\nThis is called a **generalized DiD**.\n\nThe $FirmFE$ allow that each firm has one intercept (i.e., one average $y_{i,t}$).\n\nThe $YearFE$ accommodate a potential shock in $y_{i,t}$ in each year.\n\n\n\n\n\n\n\n\n\n## Controls in DiD  {.smaller background=\"#d5f5ee\"}\n\nAdditionally, adding *good* controls can be helpful to maintain the shock **exogenous (i.e., random) after controlling by X.**\n\n. . . \n\nImagine that a financial shock is more likely to affect high leveraged firms than low-leverage ones.\n\n1) the shock will not change the leverage decision of each firm.\n\n2) we believe that high leveraged firms will have a different $y_{i,t}$ after the treatment.\n\nYou may add leverage as a control to make sure the shock is random given the firms' levels of leverage.\n\n. . .\n\nIf you may believe that the leverage will change after the treatment, you can use pre-treatment values (interacted with the time dummies).\n\nIn this case, you are controlling for the pre-treatment condition but are not including the bias that might come due to changes in leverage after the treatment.\n\n\n\n\n\n\n\n## Controls in DiD  {.smaller background=\"#d5f5ee\"}\n\n*If assignment is random, then including additional covariates should have a negligible effect on the estimated treatment effect.*\n\n*Thus, a large discrepancy between the treatment effect estimates with and without additional controls raises a red flag.* \n\n*If assignment to treatment and control groups is not random but dictated by an observable rule, then controlling for this rule via covariates in the regression satisfies the conditional mean zero assumption required for unbiased estimates.* \n\n*Regardless of the motivation, it is crucial to remember that any covariates included as controls must be unaffected by the treatment, a condition that eliminates other outcome variables and restricts most covariates to pre-treatment values.*\n\n([Roberts and Whited, 2014](https://doi.org/10.1016/B978-0-44-453594-8.00007-0))\n \n \n\n\n\n\n\n# Comment about DiD {.smaller background=\"#aecfc7\"}\n\n## Comment about DiD  {.smaller background=\"#aecfc7\"}\n\n([The effect](https://theeffectbook.net/ch-DifferenceinDifference.html))\n\n\n*Parallel trends means we have to think very carefully about how our dependent variable is measured and transformed.*\n\n*Because parallel trends is an assumption about the size of a gap remaining constant, which means something different depending on how you measure that gap.*\n\n*For example, if parallel trends holds for dependent variable $Y$, then it doesn’t hold for $ln(y)$, and vice versa*\n\n. . .\n\n*For example, say that in the pre-treatment period $y$ is 10 for the control group and 20 for the treated group. In the post-treatment period, in the counterfactual world where treatment never happened,$y$ would be 15 for the control group and 25 for the treated group. Gap of $20-10=10$  before, and $25-15=10$ after. Parallel trends holds!*\n\n. . . \n\n*However: $Ln(20)-ln(10)=.693$  before, and $ln(25)-ln(15)=.51$ after. *\n\n\n\n\n\n\n\n\n\n\n# Example of DiD  {.smaller background=\"#98b8b0\"}\n\n## Example of DiD  {.smaller background=\"#98b8b0\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load the necessary library\nlibrary(foreign)\ndata <- read.dta(\"files/kielmc.dta\")\ndata$y81_nearinc <- data$y81 * data$nearinc\nmodel1 <- lm(rprice ~ y81 + nearinc + y81_nearinc, data = data)\nmodel2 <- lm(rprice ~ y81 + nearinc + y81_nearinc + age + agesq, data = data)\nmodel3 <- lm(rprice ~ y81 + nearinc + y81_nearinc + age + agesq + intst + land + area + rooms + baths, data = data)\nsummary(model3)\n```\n\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/kielmc.dta\", clear\ngen y81_nearinc = y81 * nearinc\neststo:qui reg rprice y81 nearinc y81_nearinc \neststo:qui reg rprice y81 nearinc y81_nearinc age agesq \neststo:qui reg rprice y81 nearinc y81_nearinc age agesq intst land area rooms baths\nesttab , compress\n```  \n\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Regression Discontinuity Design (RDD) {.smaller background=\"#e3e2b8\"}\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n**The regression-discontinuity (RD) research design is a quasi experimental method.**\n\nHere the *treatment* is not a binary as before. \n\n**Treated units are assigned based on a cutoff score of a continuous variable.**\n\n. . .\n\nIn a RDD the treatment assignment is not random...\n\n... but it is based in the value of an observed covariate, in which the units lie on either side of the threshold.\n\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n**[Example Mastering Metrics](https://www.masteringmetrics.com/):**\n\n- Age for drinking is 18 in Brazil. \n\nWhy prohibiting younger than 18?\n\nThe theory behind this effort is that legal drinking at age 18 discourages binge drinking and promotes a culture of mature alcohol consumption. \n\nThe cutoff splits who can drink and who can't.\n\n\n\n\n\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\nThe name RDD comes from a *jump*, a discontinuity that occurs in a continuous variable.\n\nIn its simplest form, the design has a:\n\n- The assignment variable (e.g., age), \n\n- Two groups (above and below the cutoff),\n\n- The outcome variable.\n\n- You may include nonlinearities and control variables. \n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\nRDD is:\n\n$$D_i = 1(x_i>c) \\;$$\n\nwhere:\n\n- $$\\;D = 1 \\; if \\;X \\;_i>c $$ \n\n- $$\\;D = 0 \\; if \\;X \\;_i<c $$\n\n\n$X$ is called the forcing variable because it \"forces\" units into treatment or control groups.\n\n$X$ may be correlated with $Y_1$ or $Y_0$, so simply comparing treated and control units would not provide causal effects\n\nHowever, if the units are randomly assigned into treatment around the cutoff, we would have causal effects.\n\n\n\n\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\nThe main assumption that allows using RDD as a causal method is that\n\n**Next to the cut, the participants are similar. The only difference is that one individual is in each of the \"sides\".** [Source](http://dx.doi.org/10.1016/B978-0-08-097086-8.44049-3)\n\n![](figs/rdd0.png){width=60%  height=60%}\n\n\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n- The cutoff value **occurs at 50**\n\n- What are the differences between someone that scores 49.99 and 50.01 in the X variable?\n\n- The intuition is that these individuals are similar and comparable.\n\nIn the absence of **treatment**, the assumption is that the solid line would \"continue\" with the same inclination and values. \n\nThere is a discontinuity, however. This implies that the pretreatment  in the absence of the treatment should be the dashed line.\n\nThe discontinuity is the causal effect of X (at the cutoff) to Y.\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n*Unlike the matching and regression strategies based on treatment-control comparisons conditional on covariates, the validity of RDD is based on our willingness to extrapolate across values of the running variable, at least for values in the neighborhood of the cutoff at which treatment switches on.* [MM](https://www.masteringmetrics.com/)\n\n\n\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n**Again the drinking example:**\n\n\n**In the US it is 21 years (age & alcohol).** **[Example Mastering Metrics](https://www.masteringmetrics.com/):**\n\n*Notice the x-axis.*\n\n![](figs/rdd1.png)\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n**In the US it is 21 years.** **[Example Mastering Metrics](https://www.masteringmetrics.com/):**\n\n*Notice the x-axis.*\n\n![](figs/rdd2.png)\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\nExamples [Almeida et al 2016](https://doi.org/10.1016/j.jfineco.2015.08.008)\n\n\n![](figs/almeida.png)\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\nExamples [Flammer 2015](https://doi.org/10.1287/mnsc.2014.2038)\n\n\n![](figs/flammer.png)\n\n\n\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\nThis is (legally, it should be) an example of a **Sharp RDD**\n\nA **Sharp RDD** occurs when the cutoff is mandatory. There are no exceptions. In this case, there are no 17 years old drinking and driving. \n\nThe treatment is\n\n$$D_a= 1, \\;if \\;a \\;>=\\; 18, \\;0 \\;if \\;a\\; <\\; 18$$ \n\n\n. . . \n\nThe alternative is a **fuzzy RDD**, which occurs when there is some misassignment.\n\n- People from under the cut also receiving the treatment.\n- Ex. students that receive 5,96 usually are *approved* in a course (this is a misassignment).\n- To estimate a Fuzzy RDD, you can use the treatment as an instrumental variable (Angrist & Pischke, 2009).  \n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n**fuzzy RDD** ([The effect](https://theeffectbook.net/ch-DifferenceinDifference.html) )\n\n\n![](figs/fuzzy.png)\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n**fuzzy RDD** \n\n*Compliers*: Takes treatment if above threshold but not if below threshold.\n\n*Always-Takers*: Always takes the treatment, ignores the cut.\n\n*Never-Takers*: Never takes the treatment, ignores the cut.\n\n*Defiers*: Takes treatment if below the threshold, does not take the treatment if above the threshold.\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\nThings that are \"good\" to a RDD.\n\n- Age \n- Dates (you need 6 years to start school, 5,99 years is not allowed)\n  - Great example: Most of NHL players are those with anniversaries just after the enrollment date\n- Ranking systems\n- Location (when people cannot \"move\" easily)\n\n\n\n\n\n\n# Estimation {.smaller background=\"#d4d3bc\"}\n\n\n## Estimation   {.smaller background=\"#d4d3bc\"}\n\nA **Sharp RDD** can take the form of:\n\n$$Y_i = \\alpha + \\beta_1 D_a + \\beta_2 \\tilde{x}_1 + \\epsilon$$\n\nWhere $D_a$ is the treatment based on the cutoff.\n\n$x$ is the running variable (the difference between the actual $X$ and the cutoff in $X_0$).\n\n- For instance, months until the 18 years birthday.\n\n- We can also use the notation: $\\tilde{X}=x-x_0$ regarding the difference. \n\n\n\n\n\n\n\n## Estimation   {.smaller background=\"#d4d3bc\"}\n\nYou also should trim the sample to a reasonable window around the cutoff $c$ \n\n- Something like: $c-h<X_i<c+h$, where $h$ is a positive value that determines the window\n\n- There is no theoretical solution to how big should $h$ be. It invites robustness tests.\n\n\n\n## Estimation   {.smaller background=\"#d4d3bc\"}\n\n**Final Step:**\n\nDecide on the model of E[Y|X]:\n\n- linear in both \"sides\" with same slope\n\n- linear with different slopes\n\n- non-linear\n\n\n\n**Tip**: always execute a visual inspection to check which model os appropriate.\n\nAlso, always estimate different models and discuss the results.\n\n\n\n\n\n\n## Estimation   {.smaller background=\"#d4d3bc\"}\n\n**fuzzy RDD** \n\n\n$$Y_i = \\alpha + \\beta_1 D_a + \\beta_2 \\tilde{x}_1 + \\beta_3(Z \\times \\tilde{x}_1) \\epsilon$$\n\nWhere Z is 1 if unit is above the cut or 0 if unit is below the cut.\n\nNotice that D is not equal to Z, in these cases.\n\n\n\n\n\n\n\n\n\n# RDD Nonlinearities  {.smaller background=\"#f2f1d5\"}\n\n## RDD Nonlinearities   {.smaller background=\"#f2f1d5\"}\n\n\n**[Example Mastering Metrics](https://www.masteringmetrics.com/):**\n\nThis is a linear relationship, with the same slopes.\n\n![](figs/rdd3.png)\n\n$$E[Y|X,D] = \\alpha + \\beta_1 \\times D + \\beta_2 \\times \\tilde{X}$$\n\n\n\n## RDD Nonlinearities   {.smaller background=\"#f2f1d5\"}\n\n\n[The effect](https://theeffectbook.net/ch-DifferenceinDifference.html) This is a linear relationship, with different  slopes.\n\n![](figs/rdd_diff_slope.png)\n\n\n\n\n\n\n\n\n\n\n## RDD Nonlinearities   {.smaller background=\"#f2f1d5\"}\n\n**[Example Mastering Metrics](https://www.masteringmetrics.com/):**\n\nThis is a nonlinear relationship.\n\n![](figs/rdd4.png)\n\n\n\n\n\n\n\n\n\n\n## RDD Nonlinearities   {.smaller background=\"#f2f1d5\"}\n\n**[Example Mastering Metrics](https://www.masteringmetrics.com/):**\n\n Is the relationship linear or nonlinear here? If you misjudge the relationship, it will be hard to tell a story credibly.\n\n![](figs/rdd5.png)\n\n\n\n\n\n\n\n\n\n## RDD Nonlinearities   {.smaller background=\"#f2f1d5\"}\n\n**The takeaway:** **RDD is a graphical method. You need to show the graphs**.\n\n**Nobody will believe your story without the correct specification of the model**.\n\n. . .\n\nIn the first case:\n\n\n$$Y_i = \\alpha + \\beta_1 D_a + \\beta_2 x_1 + \\epsilon$$\n\nIn the second case:\n\n$$Y_i = \\alpha + \\beta_1 D_a + \\beta_2 x_1 + \\beta_3 x_1^2 + \\epsilon$$\n\n\n\n\n\n\n\n## RDD Nonlinearities   {.smaller background=\"#f2f1d5\"}\n\nWe can also add an interaction term (notice that I am changing the notation now to make it similar to MM)\n\n$$Y_i = \\alpha + \\beta_1 D_a + \\beta_2 (x - x_0) + \\beta_3 (x - x_0) D_a + \\epsilon$$\n\nThis allows for different inclinations before and after the cut.\n\n\n\n## RDD Nonlinearities   {.smaller background=\"#f2f1d5\"}\n\nOr even different nonlinearities before and after the cut:\n\n\n$$Y_i = \\alpha + \\beta_1 D_a + \\beta_2 (x - x_0) + \\beta_3 (x - x_0)^2 + \\beta_4 (x - x_0) D_a  + \\beta_5 (x - x_0)^2 D_a + \\epsilon$$\n\n\n![](figs/rdd6.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Example RDD  {.smaller background=\"#edecc0\"}\n\n## Example RDD **Clearly, this is not linear.** {.smaller background=\"#edecc0\"}\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(readxl)\nlibrary(ggplot2)\ndata  <- read_excel(\"files/RDD.xlsx\")\nggplot(data, aes(x, y))  + \n  geom_point(size=1.2) + \n  labs(y = \"\", x=\"\", title = \"Evolution of Y\") +\n  theme(plot.title = element_text(color=\"black\", size=20, face=\"bold\"),\n        panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n        axis.text.y = element_text(face=\"bold\", color=\"black\", size = 12),\n        axis.text.x = element_text(face=\"bold\", color=\"black\", size = 12),\n        legend.title = element_blank(),\n        legend.key.size = unit(1, \"cm\")) +\n    geom_smooth(method = \"lm\", fill = NA)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Read Excel file\ndata = pd.read_excel(\"files/RDD.xlsx\")\n# Generate line graph - Including all observations together\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(10, 5))\nscatter_plot = sns.scatterplot(x='x', y='y', data=data, s=50)\nscatter_plot.set_title(\"Evolution of Y\", fontsize=20, fontweight='bold')\nscatter_plot.set_xlabel(\"\", fontsize=12, fontweight='bold')\nscatter_plot.set_ylabel(\"\", fontsize=12, fontweight='bold')\n# Add regression line\nsns.regplot(x='x', y='y', data=data, scatter=False, line_kws={'color': 'blue'})\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: false\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nimport excel \"files/RDD.xlsx\", firstrow\ntwoway (scatter y x) (lfit y x)\nquietly graph export figs/graph1.svg, replace\n```  \n\n![](figs/graph1.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Example RDD  {.smaller background=\"#edecc0\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(readxl)\nlibrary(ggplot2)\ndata  <- read_excel(\"files/RDD.xlsx\")\n# Creating  groups\ndata$treated <- 0\ndata$treated[data$x >= 101] <- 1  \n# Generate a line graph - two groups\nggplot(data, aes(x, y, group=treated, color = factor(treated)))  + \n    geom_point( size=1.25) + \n    labs(y = \"\", x=\"\", title = \"RDD exemplo\")+\n    theme(plot.title = element_text(color=\"black\", size=25, face=\"bold\"),\n          panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n          axis.text.y = element_text(face=\"bold\", color=\"black\", size = 16),\n          axis.text.x = element_text(face=\"bold\", color=\"black\", size = 16),\n          legend.title = element_blank(),\n          legend.key.size = unit(2, \"cm\")) +\n    geom_smooth(method = \"lm\", fill = NA)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Read Excel file\ndata = pd.read_excel(\"files/RDD.xlsx\")\n# Create treated variable\ndata['treated'] = 0\ndata.loc[data['x'] >= 101, 'treated'] = 1\n# Generate line graph with two groups\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(10, 5))\nscatter_plot = sns.scatterplot(x='x', y='y', hue='treated', style='treated', data=data, s=50)\nscatter_plot.set_title(\"RDD exemplo\", fontsize=25, fontweight='bold')\nscatter_plot.set_xlabel(\"\", fontsize=16, fontweight='bold')\nscatter_plot.set_ylabel(\"\", fontsize=16, fontweight='bold')\nscatter_plot.legend().set_title('')\nscatter_plot.legend(title='', loc='upper left', fontsize='small')\n# Add regression lines\nsns.regplot(x='x', y='y', data=data[data['treated'] == 0], scatter=False, line_kws={'color': 'blue'})\nsns.regplot(x='x', y='y', data=data[data['treated'] == 1], scatter=False, line_kws={'color': 'orange'})\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: false\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nimport excel \"files/RDD.xlsx\", firstrow\ngen treated = 0\nreplace treated = 1 if x >= 101\ntwoway (scatter y x) (lfit y x if treated == 0) (lfit y x if treated == 1)\nquietly graph export figs/graph2.svg, replace\n```  \n\n![](figs/graph2.svg) \n\n:::\n\n\n\n\n\n\n\n\n\n## Example RDD  {.smaller background=\"#edecc0\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(readxl)\nlibrary(ggplot2)\ndata  <- read_excel(\"files/RDD.xlsx\")\n# Creating  groups\ndata$treated <- 0\ndata$treated[data$x >= 101] <- 1  \n# define cut\ncut <- 100\nband <- 50\nxlow = cut - band\nxhigh = cut + band\n# subset the data for the bandwidth\ndata <- subset(data, x > xlow & x <= xhigh, select=c(x, y,  treated))\n# Generate a line graph - two groups\nggplot(data, aes(x, y, group=treated, color = factor(treated)))  + \n  geom_point( size=1.25) + \n  labs(y = \"\", x=\"\", title = \"RDD example\")+\n  theme(plot.title = element_text(color=\"black\", size=25, face=\"bold\"),\n        panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n        axis.text.y = element_text(face=\"bold\", color=\"black\", size = 16),\n        axis.text.x = element_text(face=\"bold\", color=\"black\", size = 16),\n        legend.title = element_blank(),\n        legend.key.size = unit(2, \"cm\")) +\n  geom_smooth(method = \"lm\", fill = NA)\n```\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Read Excel file\ndata = pd.read_excel(\"files/RDD.xlsx\")\n# Create treated variable\ndata['treated'] = 0\ndata.loc[data['x'] >= 101, 'treated'] = 1\n# Define cut, band, and subset data\ncut = 100\nband = 50\nxlow = cut - band\nxhigh = cut + band\ndata = data[(data['x'] > xlow) & (data['x'] <= xhigh)][['x', 'y', 'treated']]\n# Generate line graph with two groups\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(10, 5))\nscatter_plot = sns.scatterplot(x='x', y='y', hue='treated', style='treated', data=data, s=50)\nscatter_plot.set_title(\"RDD example\", fontsize=25, fontweight='bold')\nscatter_plot.set_xlabel(\"\", fontsize=16, fontweight='bold')\nscatter_plot.set_ylabel(\"\", fontsize=16, fontweight='bold')\nscatter_plot.legend().set_title('')\nscatter_plot.legend(title='', loc='upper left', fontsize='small')\n# Add regression lines\nsns.regplot(x='x', y='y', data=data[data['treated'] == 0], scatter=False, line_kws={'color': 'blue'})\nsns.regplot(x='x', y='y', data=data[data['treated'] == 1], scatter=False, line_kws={'color': 'orange'})\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: false\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nimport excel \"files/RDD.xlsx\", firstrow\ngen treated = 0\nreplace treated = 1 if x >= 101\ngen cut = 100\ngen band = 50\ngen xlow = cut - band\ngen xhigh = cut + band\nkeep if x > xlow & x <= xhigh\ntwoway (scatter y x) (lfit y x if treated == 0) (lfit y x if treated == 1)\nquietly graph export figs/graph3.svg, replace\n```  \n\n![](figs/graph3.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Example RDD  {.smaller background=\"#edecc0\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(readxl)\nlibrary(ggplot2)\ndata  <- read_excel(\"files/RDD.xlsx\")\ndata$treated <- 0\ndata$treated[data$x >= 101] <- 1  \ncut <- 100\nband <- 50\nxlow = cut - band\nxhigh = cut + band\ndata <- subset(data, x > xlow & x <= xhigh, select=c(x, y,  treated))\n# Generating xhat - Now we are going to the RDD\ndata$xhat <- data$x - cut\n# Generating xhat * treated to allow different inclinations (we will use the findings of the last graph, i.e. that each group has a different trend.)\ndata$xhat_treated <- data$xhat * data$treated\n# RDD Assuming different trends\nrdd <- lm(y  ~ xhat + treated  + xhat_treated, data = data)\nsummary(rdd)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_excel(\"files/RDD.xlsx\")\n# Generate treated variable\ndata['treated'] = 0\ndata.loc[data['x'] >= 101, 'treated'] = 1\n# Define cut and band\ncut = 100\nband = 50\nxlow = cut - band\nxhigh = cut + band\n# Subset data\ndata = data[(data['x'] > xlow) & (data['x'] <= xhigh)]\n# Generate xhat and xhat_treated\ndata['xhat'] = data['x'] - cut\ndata['xhat_treated'] = data['xhat'] * data['treated']\n# Regression\nX = data[['xhat', 'treated', 'xhat_treated']]\nX = sm.add_constant(X)  # Add a constant term\ny = data['y']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nimport excel \"files/RDD.xlsx\", firstrow clear\ngen treated = 0\nreplace treated = 1 if x >= 101\ngen cut = 100\ngen band = 50\ngen xlow = cut - band\ngen xhigh = cut + band\nkeep if x > xlow & x <= xhigh\ngen xhat = x - cut\ngen xhat_treated = xhat * treated\nregress y xhat treated xhat_treated\n```  \n\n:::\n\n\n\n\n## Example RDD  {.smaller background=\"#edecc0\"}\n\nThe coefficient of x before the cut is 0.29 (t-stat 5.45), and after the cut, it is -0.51 (t-stat -6.75). \n\nWe also have the coefficient of the treatment, which is measured by the \"jump\" that occurs near the cut: **an estimated coefficient of 28.9 (t-stat 13.11). **\n\nIf this were a real example, this would be the causal effect of receiving the treatment (i.e., being beyond the cut).\n\n\n\n\n\n\n\n\n\n\n# Final comments RDD {.smaller background=\"#f0eeb4\"}\n\n## Final comments RDD {.smaller background=\"#f0eeb4\"}\n\n\n**Sensitivity to specification**\n\nMisspecification can lead to a \"spurious jump\". **Do not mix nonlinearity with a discontinuity**.\n\n. . . \n\n**Sensitivity to window**\n\nAlso, need to check many alternative $h$. \n\n. . . \n\n**Smoothness of the running around the cut**\n\nThe distribution of X should be smooth around the cut. If it is not, it might indicate that the treatment assignment was not random. \n\n. . . \n\n**Test comparability of units around the cut**\n\nCovariates balance. You can check whether there are jumps in control variables as an additional robustness.\n\n. . . \n\n**Placebo tests**\n\nTest whether the treatment is zero when it should be zero.\n\n\n\n\n\n\n\n\n\n\n# Synthetic Control {.smaller background=\"#f2c7c4\"}\n\n## Synthetic Control {.smaller background=\"#f2c7c4\"}\n\nIt is a method to estimate the effect of events or policy interventions, often at an **aggregate level** (cities, states, etc.)\n\nThe event occurs often in **only one unit**.\n\nIt compares the evolution of the  outcome for the treated unit to the evolution of the control group.\n\n- The control group contains many units. \n\n. . . \n\nThe limitation is often the selection of the control group. It is very **ambiguous**.\n\n\n\n\n\n\n## Synthetic Control {.smaller background=\"#f2c7c4\"}\n\n[Abadie, Diamond, and Hainmueller (2010)](https://doi.org/10.1198/jasa.2009.ap08746) apply synth  by using a cigarette tax in California called Proposition 99.\n\n*In 1988, California passed comprehensive tobacco control legislation called Proposition 99.*\n\n*Proposition 99 increased cigarette taxes by $0.25 a pack, spurred clean-air ordinances throughout the state, funded anti-smoking media campaigns, earmarked tax revenues to health and anti-smoking budgets, and produced more than $100 million a year in anti-tobacco projects.*\n\n*Other states had similar control programs, and they were dropped from their analysis.* [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#cuba-miami-and-the-mariel-boatlift)\n\n\n\n\n\n\n## Synthetic Control {.smaller background=\"#f2c7c4\"}\n\nThere was a trend before the treatment. **How can we estimate the causal effect?**\n\n![](figs/synth1.png)\n\n\n\n\n\n\n\n## Synthetic Control {.smaller background=\"#f2c7c4\"}\n\nThe goal is to  elect an optimal set of weights that when applied to the rest of the country produces the following figure: \n\n![](figs/synth3.png)\n\n\n## Synthetic Control {.smaller background=\"#f2c7c4\"}\n\nThe variables used for computing the weights are the following. You are creating weights such that weighting the other states, you can create a synthetic California. **Notice that, so far, the product of this analysis is only two data points per period.** \n\n\n![](figs/synth2.png)\n\n\n\n## Synthetic Control {.smaller background=\"#f2c7c4\"}\n\n**Tip**: Synth is also an graphical method, so graphs like the following are common. This is the difference between the two series.\n\n![](figs/synth4.png)\n\n\n\n\n# Inference {.smaller background=\"#d49692\"}\n\n## Inference {.smaller background=\"#d49692\"}\n\n\n**Notice that, so far, the product of this analysis is only two data points per period.** How can you stablish a \"significant\" causal effect?\n\n\n**Steps**\n\n1) Apply synth to each state in the control group (also called \"donor pool\"). \n\n2) Obtain a distribution of placebos.\n\n3) Compare the gap for California to the distribution of the placebo gaps.\n\n4) Then, test whether the effect for the treated unit is large enough relative to the placebos (i.e., to the effect estimated for a placebo unit randomly selected).\n\n\n\n\n\n\n\n## Inference {.smaller background=\"#d49692\"}\n\nNotice the bold line (treated unit) after the treatment. It is at the bottom. \n\n![](figs/synth5.png)\n\n## Inference {.smaller background=\"#d49692\"}\n\n*Abadie, Diamond, and Hainmueller (2010) recommend iteratively dropping the states whose pre-treatment RMSPE is considerably different than California’s because as you can see, they’re kind of blowing up the scale and making it hard to see what’s going on.* [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#cuba-miami-and-the-mariel-boatlift)\n\n![](figs/synth6.png)\n\n\n\n\n\n\n## Inference {.smaller background=\"#d49692\"}\n\nThe previous figure suggests the effect is large enough relative to the placebo effects.\n\n. . . \n\n\n1) The root mean squared prediction error (RMSPE) is:\n\n$$RMSPE = \\bigg (\\dfrac{1}{T-T_0} \\sum_{t=T_0+t}^T \\bigg (Y_{1t} - \\sum_{j=2}^{J+1} w_j^* Y_{jt} \\bigg )^2 \\bigg )^{\\tfrac{1}{2}}$$\n\n*It shows how far predictions fall from measured true values using Euclidean distance.*\n\n\n\n2) Sort the ratio post- to pre-treatment RMSPE in descending order\n\n3) Calculate the p-value as $\\frac{Rank}{Total}$.\n\n\n. . . \n\nBasically, these steps give how likely is the occurrence of the treated unit distance vis-a-vis the average placebo.\n\n\n\n\n\n\n\n\n\n\n\n## Inference {.smaller background=\"#d49692\"}\n\n**RMSPE**: in the previous example, California has the largest increase in the error after the treatment. Position 1 out of 39 states, implying an exact p-value of $\\frac{1}{38}=0.026$ (significant). \n\n\n\n![](figs/synth7.png)\n\n\n\n\n\n\n\n\n\n\n\n\n# Example Synth  {.smaller background=\"#f5a7a2\"}\n\n## Example Synth  {.smaller background=\"#f5a7a2\"}\n\n::: panel-tabset\n\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/synth_smoking.dta , clear\ntsset state year\nsynth cigsale beer(1984(1)1988) lnincome retprice age15to24 cigsale(1988) cigsale(1980) cigsale(1975), trunit(3) trperiod(1989) \n```  \n\n:::\n\n\n## Example Synth  {.smaller background=\"#f5a7a2\"}\n\n::: panel-tabset\n\n\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: false\n#| code-summary: \"Stata\"\nuse files/synth_smoking.dta , clear\nsynth cigsale beer lnincome(1980&1985) retprice cigsale(1988) cigsale(1980) cigsale(1975), trunit(3) trperiod(1989) fig\nquietly graph export figs/synth1.svg, replace\n```  \n\n![](figs/synth1.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n## Example Synth  {.smaller background=\"#f5a7a2\"}\n\n\n*Authors using synthetic control must do more than merely run the synth command when doing comparative case studies.* \n\n*They must find the exact-values through placebo-based inference, check for the quality of the pre-treatment fit, investigate the balance of the covariates used for matching, and check for the validity of the model through placebo estimation (e.g., rolling back the treatment date).*\n\n\n\n\n\n\n\n\n\n\n\n\n\n# [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#prison-construction-and-black-male-incarceration)  {.smaller background=\"#de938e\"}\n\n\n## [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#prison-construction-and-black-male-incarceration)  {.smaller background=\"#de938e\"}\n\nIn 1992, Texas expanded the prision system operational capacity.\n\n![](figs/synth8.png)\n\n\n\n\n\n\n\n## [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#prison-construction-and-black-male-incarceration)  {.smaller background=\"#de938e\"}\n\nThis is what happened.\n\n![](figs/synth9.png)\n\n\n\n\n\n\n## [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#prison-construction-and-black-male-incarceration)  {.smaller background=\"#de938e\"}\n\nSynthetic Texas.\n\n![](figs/synth10.png)\n\n\n\n\n## [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#prison-construction-and-black-male-incarceration)  {.smaller background=\"#de938e\"}\n\nGap between Synthetic Texas and Texas.\n\n![](figs/synth11.png)\n\n\n\n\n## [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#prison-construction-and-black-male-incarceration)  {.smaller background=\"#de938e\"}\n\nBuilding Placebos.\n\n![](figs/synth13.png)\n\n\n\n\n## [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#prison-construction-and-black-male-incarceration)  {.smaller background=\"#de938e\"}\n\nTexas is the one in the far right tail. \n\n![](figs/synth12.png)\n\n\n\n\n\n\n\n# Introduction IV {.smaller background=\"#bcd3f7\"}\n\n## Introduction IV {.smaller background=\"#bcd3f7\"}\n\nImagine the following model\n\n$$Ln(wage)=\\alpha + \\beta_1 educ + \\epsilon$$\n\nWe can infer that *educ* is correlated with *ability*, but the latter is in the error term. \n\n*educ* in this case is \"endogenous\".\n\n$$Cov(educ, \\epsilon) \\neq 0$$\n\n\n\n## Introduction IV {.smaller background=\"#bcd3f7\"}\n\n**The setup of an IV is**\n\nSuppose that we have an observable variable z that satisfies these two assumptions: \n\n(1) z is uncorrelated with u:\n\n$$Cov(z, \\epsilon) = 0$$\n\n\n(2) z is correlated with x:\n\n$$Cov(z, x) \\neq 0$$\n\n\n\nThen, we call z an instrumental variable for x, or sometimes simply an instrument for x.\n\n\n\n\n\n## Introduction IV {.smaller background=\"#bcd3f7\"}\n\nBefore we continue,\n\nIV is not a model, it is an **estimation method** \n\nI'll call it a **Design**.\n\n\nDo not say, I estimated an IV model (more often than it should be).\n\n\n\n\n\n\n\n\n\n\n# Instrumental Variables {.smaller background=\"#8eadde\"}\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\nImagine that you have one independent variable that is \"endogenous\":\n\n- $Cov(x_k,\\mu)\\neq 0$\n\n- You may have many other independent variables not \"endogenous\"\n\n. . .\n\nIn this situation:\n\n- $B_k$ is biased\n\n- The other betas will likely be biased as well, since it is unlikely that all other Xs are not correlated with $x_k$\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n$x_k$ is the endogenous variable.\n\n- It has \"good\" variation: \n\n  - the part that varies that is not correlated with $\\mu$\n\n- It has \"bad\" variation: \n\n  - the part that varies that is  correlated with $\\mu$\n\n. . . \n\nLet's assume now that you can find an instrument $z$\n\n- The instrument $z$ is correlated with $X_k$, but only the \"good\" variation, not the \"bad\".\n\n- The instrument $z$ does not explain $y$ directly, only through $x_k$.\n\n  - **Only through** condition.\n\n\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n**Relevance Condition**\n\nThe instrument $z$ is correlated with $x_k$.\n\n- This assumption is easy to test. Simply run a regression of $x=f(z, all\\; Xs)$ and check the significance of the $\\beta_z$.\n\n- This is called the **first stage of an IV regression**\n\n  - **Tip**: Always show the beta coefficient and the R2 (even if low) of the first-stage.  \n\n. . . \n\n**Exclusion Condition**\n\nThe instrument $z$ is not correlated with $\\mu$.\n\n- That is $cov(z,\\mu) = 0$\n\n- As all correlations with $\\mu$, you cannot test this prediction. You have to rely on the theory, create a story about that.\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n**An example of IV** [Murray](https://doi.org/10.1257/jep.20.4.111).\n\n![](figs/iv2.png)\n\n\n\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n**An example of IV** [Murray](http://dx.doi.org/10.1016/j.jcorpfin.2013.12.013).\n\n![](figs/iv6.png)\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#good-instruments-should-feel-weird)\n\n**Good instruments should feel weird**\n\nParents with two same-gender kids are more likely to try a third kid than a diverse-gender pair of parents.\n\nSo, you may use the gender of the kids as instrument for the likelihood of the mother go back to the labor market.\n\n\n\n\n\n\n# Angrist's example {.smaller background=\"#d2e2fc\"}\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\nRemember the **Fuzzy RDD**.\n\n- There is the *treatment*\n- There is the *position* (before or after the cut) \n\nThe *position* is an indication of receiving or not the treatment, but it is not definitive.\n\nThus, we can use the *position* as an IV for the treatment.\n\n\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#the-problem-of-weak-instruments)\n\n*One of the more seminal papers in instrumental variables for the modern period is Angrist and Krueger (1991).* \n\n*Their idea is simple and clever; a quirk in the United States educational system is that a child enters a grade on the basis of his or her birthday.* \n\n*For a long time, that cutoff was late December. If children were born on or before December 31, then they were assigned to the first grade. But if their birthday was on or after January 1, they were assigned to kindergarten. *\n\n*Thus two people—one born on December 31 and one born on January 1—were exogenously assigned different grades.*\n\nEveryone is forced to leave school when 16.\n\n\n\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#the-problem-of-weak-instruments)\n\n![](figs/iv3.png)\n\n*Angrist and Krueger had the insight that that small quirk was exogenously assigning more schooling to people born later in the year.*\n\n*The person born in December would reach age 16 with more education than the person born in January*\n\n\n\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#the-problem-of-weak-instruments)\n\n![](figs/iv4.jpg)\n\n\n**What is the instrument here?**\n\n\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#the-problem-of-weak-instruments)\n\n\n**What is the instrument here?**\n\nThe instrument is the quarter of birth.\n\n- People born in the 3rd and 4th quarter receive more education than others due to **compulsory schooling**.\n\n\n\n\n\n\n\n\n# Two-stage least squares (2SLS) {.smaller background=\"#abc8f5\"}\n\n## Two-stage least squares  (2SLS){.smaller background=\"#abc8f5\"}\n\n\nOne of the more intuitive instrumental variables estimators is the 2SLS. \n\n\n\n**The first stage is**\n\n$$x_k = \\delta + \\delta_1 z + \\delta_2 x_1 + . . .+ \\delta_n x_n + \\mu$$\n\nThen, you predict $x_k$ using the first stage.\n\n- \"predict\" means that you are finding the \"response\" Y of the equation after estimating the coefficients\n\n $$\\hat{x_k} = \\hat{\\delta} + \\hat{\\delta_1} z + \\hat{\\delta_2} x_1 + . . . + \\hat{\\delta_n} x_n $$\n\n\n. . . \n\nThen, **the second stage** is:\n\n\n$$y = \\alpha + \\beta_1 \\hat{x_k} + \\beta_2 x_1 + . . .+  \\beta_n x_n + \\mu$$\n\n\nThe idea using $\\hat{x_k}$ is that it represents only the variation that is not correlated with $\\mu$.\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#abc8f5\"}\n\nWe can write that:\n\n$$\\beta_1= \\frac{Cov(z,y)}{Cov(z,x_k)}$$\n\n\n- It shows that $\\beta_1$ is the population covariance between z and y divided by the population covariance between z and x.\n\n- Notice how this fails if z and x are uncorrelated, that is, if $Cov(z, x) = 0$ \n\n\n\n\n. . . \n\n\n\n$$\\beta_1= \\frac{\\sum_{i=1}^n(z_i-\\bar{z})(y_i-\\bar{y})}{\\sum_{i=1}^n(z_i-\\bar{z})(x_i-\\bar{x})}$$\n\n\nNotice that if $z$ and $x$ are the same (i.e., perfect correlation), $\\beta_1$ above is the OLS $\\beta$\n\n\n$$\\beta_1= \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$$\n\n\n\n# Weak Instruments  {.smaller background=\"#8da6cc\"}\n\n## Weak Instruments {.smaller background=\"#8da6cc\"}\n\nI did not demonstrate here, but we can say that (see pp; 517-18 Wooldridge):\n\n- though IV is consistent when $z$ and $u$ are uncorrelated and $z$ and $x$ have any positive or negative correlation, IV estimates can have large standard errors, especially if $z$ and $x$ are only weakly correlated.\n\nThis gives rise to the week instruments problem.\n\nWe can write the probability limit of the IV estimator:\n\n$$plim \\hat{\\beta_{iv}} = \\beta_1 + \\frac{Corr(z,\\mu)}{Corr(z,x)} \\times \\frac{\\sigma_{\\mu}}{\\sigma_x}$$\n\nIt shows that, even if $Corr(z,\\mu)$ is small, the inconsistency in the IV estimator can be very large if $Corr(z,x)$ is also small.\n\n\n\n\n\n\n\n## Weak Instruments {.smaller background=\"#8da6cc\"}\n\nTwo tips:\n\n1) Look at the F-stat of the first-stage. It should not be low.\n\n2) The sign of the coefficient in the first stage should be as expected.\n\n3) Look at the S.E. When the instrument is week, the S.E., are even larger than it should be.\n\n. . .\n\nOne more tip,\n\n4) Avoid computing the first stage by hand, it would give wrong estimates of the S.E. in the second stage.\n\n\n\n\n\n\n\n\n\n\n\n# Example {.smaller background=\"#95a9c7\"}\n\n## Example  {.smaller background=\"#95a9c7\"}\n\nOLS\n\n::: panel-tabset\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/mroz.dta , clear\nreg lwage educ \n```  \n\n:::\n\n\n\n\n\n\n\n\n## Example  {.smaller background=\"#95a9c7\"}\n\nThe IV estimate of the return to education is 5.9%, which is barely more than one-half of the  OLS estimate. This suggests that the OLS estimate is too high and is consistent with omitted ability bias.\n\n::: panel-tabset\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/mroz.dta , clear\nivreg lwage (educ =fatheduc ) , first\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Final Comments {.smaller background=\"#bcd3f7\"}\n\n## Final Comments {.smaller background=\"#bcd3f7\"}\n\nWhen you have only one Z, you can say that the model is **just identified**.\n\nBut you can have multiple IVs, in which case you will say that the model is **overidentified**.\n\n- You can implement IV design just as before\n\n- The relevance and exclusion assumptions are there as well.\n\n. . . \n\nAssuming that both conditions are satisfied, you will have more asymptotic efficiency in the IV estimates.\n\n\n\n\n\n\n## Final Comments {.smaller background=\"#bcd3f7\"}\n\n**It is rare to have multiple Zs. You should be happy if you have a good one!**\n\n. . . \n\n\nBut if you do have multiple IVs, you can test their quality...\n\n- If they are all valid, you should get consistent estimates...\n\n- ... even if you use only a subset of them.\n\n- So the test is about how similar the estimates are if you use subsets of IVs.\n\n\n**But this test does not give really an answer about whether the IVs are good.**\n\nThis always come from theory.\n\n\n\n\n## Final Comments {.smaller background=\"#bcd3f7\"}\n\n**Some more comments:**\n\n1) If you have an interaction term between $x_1$ and $x_2$, and $z$ is the instrument for $x_1$, you can \"create\" the instrument $zx_2$ for $x_2$.\n\n. . . \n\n2) GMM uses lagged variables as instruments. But, this is not a good decision if the variables are highly serially correlated.\n\n  - Lagged total assets is not a good instrument for total assets.\n  \n. . . \n\n3) Using the average-group of variable X is also problematic (i.e., the industry average own. concentration as IV of firm-level own. concentration)\n\n - This is no different than a group FE, making hard to believe in the exclusion restriction.\n\n\n\n\n## THANK YOU!\n\n::: columns\n::: {.column width=\"30%\"}\n![](figs/fgv.png){fig-align=\"right\"}\n:::\n\n::: {.column width=\"70%\"}\n**Henrique Castro Martins**\n\n-   [henrique.martins\\@fgv.br](henrique.martins@fgv.br)\n-   <https://eaesp.fgv.br/en/people/henrique-castro-martins>\n-   [henriquemartins.net](https://henriquemartins.net/)\n-   <https://www.linkedin.com/in/henriquecastror/>\n:::\n:::\n","srcMarkdownNoYaml":"\n\n```{r setup}\n#| include: false\n#| warning: false\n\n# library(reticulate)\n# use_python(\"C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe\")\nlibrary(reticulate)\nlibrary(Statamarkdown)\n#reticulate::py_install(\"matplotlib\")\n#reticulate::py_install(\"seaborn\")\n#reticulate::py_install(\"pyfinance\")\n#reticulate::py_install(\"xlrd\")\n#reticulate::py_install(\"quandl\")\n#reticulate::py_install(\"linearmodels\")\n#reticulate::py_install(\"causalml\")\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# DiD Introduction {.smaller background=\"#c4f5d7\"}\n\n## DiD Introduction  {.smaller background=\"#c4f5d7\"}\n\nLet's introduce DiD using the most famous example in the topic: John Snow’s 1855 findings that demonstrated to the world that cholera was spread by fecally-contaminated water and not via the air (Snow 1855)\n\n- Snow compared the periods of 1849 and 1854 to analyze the impact of a  change in London's water supply dynamics. \n- Various water companies, each drawing water from different sections of the Thames river, served the city's water needs. \n- The downstream areas of the Thames, where some companies sourced their water, were susceptible to contamination due to the disposal of various substances, including fecal matter from cholera-infected individuals. \n- In the interim between 1849 and 1854, a pivotal policy was implemented: the Lambeth Company was mandated by an Act of Parliament to relocate its water intake upstream of London.\n\n\n\n\n\n## DiD Introduction  {.smaller background=\"#c4f5d7\"}\n\n**This is what happened.**\n\n| Region Supplier                        | Death Rates 1849 | Death Rates 1854 |\n|----------------------------------------|------------------|------------------|\n| Non-Lambeth Only (Dirty)               | 134.9            | 146.6            |\n| Lambeth + Others (Mix Dirty and Clean) | 130.1            | 84.9             |\n\n. . . \n\nThe specific DID estimate we can get here is:\n\n- $(84.9-130.1)-(146.9-134.9)=-57.2$.\n\nThis resembles a \"modern\" DiD. \n\n\n\n\n\n\n\n\n# Shocks {.smaller background=\"#95c9a9\"}\n\n## Shocks  {.smaller background=\"#95c9a9\"}\n\n**Shock-based designs use an external shock to limit selection bias. **\n\nThey are very hard to find, but if you do, you can reasonably estimate causal effects.\n\nThey are sources of exogenous variations in the X, which are crucial to causality when we have some of the problems discussed before. \n\nA shock is often considered the first-best solution to causal inference (randomized control trials are the second-best).\n\n. . . \n\nIn social sciences, we cannot have randomized control trials. It is not feasible to have experiments.\n\nThat is why we often explore the idea of \"Natural experiments\" or \"Natural shocks\" (I often use the terms interchangeably)\n\n\n\n\n\n\n\n## Shocks  {.smaller background=\"#95c9a9\"}\n\nA **Natural experiment** is an exogenous variation that change a variable in a random subset of firms. \n\n- Regulations, laws, etc.\n- Natural disasters.\n- Sudden death of CEOs or a product, etc.\n- The gender of a newborn.\n\n\nBecause the variation that occur in x is truly exogenous, the CMI holds and thus we can infer causality.\n\n\n\n\n\n\n\n## Shocks  {.smaller background=\"#95c9a9\"}\n\nA good shock has some conditions [source](https://cfr.pub/published/papers/cfr-0036.pdf):\n\n1) **Shock Strength**: The shock is strong enough to significantly change firm behavior or incentives.\n\n. . .\n \n2) **Exogenous Shock**: The shock came from “outside” the system one is studying.\n\n  - Treated firms did not choose whether to be treated, \n  - cannot anticipate the shock, \n  - the shock is expected to be permanent, and \n  - there is no reason to believe that which firms were treated depends on unobserved firm characteristics.\n  \nIf the shock is exogenous, or appears to be, we are less worried that unobservables might be correlated with both assignment to treatment and the potential outcomes, and thus generate omitted variable bias. \n\nShock exogeneity should be defended, not just assumed.\n\n\n\n\n\n\n\n## Shocks  {.smaller background=\"#95c9a9\"}\n\nA good shock has some conditions [source](https://cfr.pub/published/papers/cfr-0036.pdf):\n\n3) **“As If Random” Assignment**: The shock must separate firms into treated and controls in a manner which is close to random.\n\n. . .\n\n4) **Covariate balance:** The forcing and forced variables aside, the shock should produce reasonable covariate balance between treated and control firms, including “common support” (reasonable overlap between treated and control firms on all covariates). \n\nSomewhat imperfect balance can be address with balancing methods, but severe imbalance undermines shock credibility, even if the reason for imbalance is not obvious. Covariate balance should be reported.\n\n\n\n\n\n\n\n## Shocks  {.smaller background=\"#95c9a9\"}\n\nA good shock has some conditions [source](https://cfr.pub/published/papers/cfr-0036.pdf):\n\n5) **Only-Through Condition(s):** We must have reason to believe that the apparent effect of the shock on the outcome came only through the shock (sometimes, through a specific channel). \n\nThe shock must be “isolated”, there must be no other shock, at around the same time, that could also affect treated firms differently than control firms. \nAnd if one expects the shock to affect outcomes through a particular channel, the shock must also affect the outcome only through that channel.\n\n. . .\n\nIn IV analysis, this is called an **“exclusion restriction”** or **“only-through condition\"**, because one assumes away (excludes) other channels.\n\n\n\n\n\n\n## Shocks  {.smaller background=\"#95c9a9\"}\n\nThe idea of a natural shock is often mixed with the difference-in-differences (DiD) design.\n\nIt is more common that it should that people refer to shocks when they want to refer to DiD. \n\nA Did design explores a Natural shock to estimate causal effects.\n\n**A good shock generates, by nature, random assignment**.\n\n\n\n\n\n\n\n\n\n## Remember  {.smaller background=\"#693d3a\"}\n\n**Remember:**\n\nWhen dealing with **causal inference**, we have to find ways to approximate what the hidden potential outcome of the treated units is. \n\nThat is, the challenge in identifying causal effects is that the untreated potential outcomes, $Y_{i,0}$, are never\nobserved for the treated group ($D_i= 1$). The \"second\" term in the following equation:\n\nATET = $\\frac{1}{N} (E[Y_{i,1}|D_i=1] - E[Y_{i,0}|D_i=1])$\n\nWe need an empirical design to **\"observe\"** what we do not really observe (i.e., the counterfactual). \n\n\n\n\n\n\n\n# Single differences  {.smaller background=\"#a4baac\"}\n\n## Single differences  {.smaller background=\"#a4baac\"}\n\nThere are two single differences we can use to estimate the causal effect of a treatments (i.e., a shock).\n\n**1) Single Cross-Sectional Differences After Treatment**\n\n**2) Single Time-Series Difference Before and After Treatment**\n\n\n\n\n\n\n\n## Single differences  {.smaller background=\"#a4baac\"}\n\n**1) Single Cross-Sectional Differences After Treatment**\n\nOne approach to estimating a parameter that summarizes the treatment effect is to compare the post-treatment outcomes of the treatment and control groups. \n\nThis method is often used when there is no data available on pre-treatment outcomes.\n\nIt takes the form:\n\n$$y=\\beta_0+\\beta_1d_i+\\epsilon$$\n\nwhere $d_i$ is a dummy marking the units that are treated. The treatment effect is given by $\\beta_1$\n\nThis is a cross-sectional comparison, using only post-treatment values.\n\n\n\n\n\n\n\n\n## Single differences  {.smaller background=\"#a4baac\"}\n\n**1) Single Cross-Sectional Differences After Treatment**\n\nYou may add the interaction between the treatment dummy and the years.\n\n$$y=\\beta_0+\\beta_1d_i\\times year_1+\\beta_2d_i\\times year_2 +\\beta_3d_i\\times year_3+ . . . +\\epsilon$$\n\nThis design allows the treatment effect to vary over time by interacting the treatment dummy with period dummies.\n\n\n\n\n. . . \n\n\n**What is the endogeneity concern here?**\n\n. . .\n\nThe endogeneity concern is that these firms’ $y$ were different and could become more different between the treated and control groups even if the shock had not happened. \n\nThat is, there is something in the residual that explains the differences in $y$ that is correlated with the $d$.\n\n\n\n\n\n\n\n\n\n## Single differences  {.smaller background=\"#a4baac\"}\n\n**2) Single Time-Series Difference Before and After Treatment**\n\nA second way to estimate the treatment effect is to compare the outcome after the treatment with the outcome before the treatment for **just those units that are treated.**\n\nThe difference from before is that you have data before the treatment, but you only have data for the treated units.\n\n$$y=\\beta_0+\\beta_1 time+\\epsilon$$\n\nwhere $time$ marks the years after the treatment. The treatment effect is given by $\\beta_1$\n\n\n\n\n\n\n## Single differences  {.smaller background=\"#a4baac\"}\n\n**2) Single Time-Series Difference Before and After Treatment**\n\nYou can also estimate a multi-year regression.\n\n$$y=\\beta_0+\\beta_1year_1 +\\beta_2\\times year_2 +\\beta_3\\times year_3+ . . . +\\epsilon$$\n\n. . . \n\n\n**What is the endogeneity concern here?**\n\n. . .\n\nThe endogeneity concern is that these firms’ $y$ could have changed over the period of observation even if the shock had not happened. \n\nThat is, there is something in the error term that explains $y$ that is correlated with the $year$ dummies.\n\n\n\n\n\n## Single differences  {.smaller background=\"#a4baac\"}\n\nThe combination of the  **1) Single Cross-Sectional Differences After Treatment** and **2) Single Time-Series Difference Before and After Treatment** is called **Difference-in-Differences**.\n\n\n\n\n\n\n\n\n# Difference-in-Differences {.smaller background=\"#8bc9a2\"}\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\nThis is one of the most popular methods in social sciences for estimating causal effects in non-experimental settings.\n\nThe literature is exploding over the recent years.\n\n. . . \n\nThere is one group that is treated, another is the control. \n\nThere are two periods of time, before and after the treatment.\n\nAnd the treated group receives the treatment in the second period.\n\n**The key identifying assumption is that the average outcome among the treated and comparison populations would have followed “parallel trends” in the absence of treatment.**\n\n\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\n\n*The two single difference estimators complement one another.*\n\n*The cross-sectional comparison avoids the problem of omitted trends by comparing two groups over the same time period.*\n\n*The time series comparison avoids the problem of unobserved differences between two different groups of firms by looking at the same firms before and after the change.*\n\n*The double difference, difference-in-differences (DD), estimator combines these two estimators to take advantage of both estimators’ strengths.* \n\n([Roberts and Whited, 2014](https://doi.org/10.1016/B978-0-44-453594-8.00007-0))\n\n\n\n\n\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\n\nDifference-in-differences methods overcome the identification challenge via assumptions that allow us to impute the mean counterfactual untreated outcomes for the treated group by using \n\n- (a) the change in outcomes for the untreated group and \n- (b) the baseline outcomes for the treated group. \n\nThe key assumption for identifying the causal effect is the **parallel trends assumption**, which intuitively states that **the average outcome for the treated and untreated units would have evolved in parallel if treatment had not occurred**.\n\n**Assumption 1 (Parallel Trends).**\n\n$E[Y_{i,1,t=2}-Y_{i,0,t=1}|D_i=1] = E[Y_{i,1,t=2}-Y_{i,0,t=1}|D_i=0]$\n\n\nIn other words: **this condition means that in the absence of treatment, the average change in the $y$ would have been the same for both the treatment and control groups.**\n\n\n\n\n\n\n\n\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\n**The counterfactual is determined by the assumption of a parallel trend between the treated and control groups.** **[MM](https://www.masteringmetrics.com/)**\n\n\n![](figs/did1.png) \n\n\n\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\n![](figs/slides4-did.png) \n\n\n\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\nInserting more periods.\n\n![](figs/did2.png) \n\n\n\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\nA more realistic example. ([The effect](https://theeffectbook.net/ch-DifferenceinDifference.html))\n\n\n![](figs/theeffect1.png)\n\n\n\n\n\n\n## Difference-in-Differences  {.smaller background=\"#8bc9a2\"}\n\n\n**Assumption 2 (No anticipatory effects).**\n\n*The treatment has no causal effect prior to its implementation.*\n\n$$Y_{i,0,t=1}=Y_{i,1,t=1}$$\n\nPrior to the treatment (t=1), there is no effect of the treatment.\n\n\n\n\n\n\n\n\n\n# Estimation  {.smaller background=\"#c6f7ec\"}\n\n## Estimation   {.smaller background=\"#c6f7ec\"}\n\nLet's see how to implement a DiD model. The canonical DiD model is the followin:\n\n$$y_{i,t} = \\beta_0 + \\beta_1 time_t + \\beta_2 treated_i  + \\beta_3 treated_i \\times time_t + \\epsilon_{i,t}$$\n\nWhere:\n\n$treated_i$ is a dummy marking the units that received the treatment.\n\n$time_t$ is a dummy marking the year of the treatment and the years after.\n\n\n\n\n\n\n## Estimation   {.smaller background=\"#c6f7ec\"}\n\nLet's see how to implement a DiD model. The canonical DiD model is the followin:\n\n$$y_{i,t} = \\beta_0 + \\beta_1 time_t + \\beta_2 treated_i  + \\beta_3 treated_i \\times time_t + \\epsilon_{i,t}$$\n\n\n$\\beta_0$ is the **average $y_{i,t}$** for the **control units before the treatment ($time_t$ =0 and $treated_i$ = 0)**.\n\n$\\beta_0+\\beta_1$ is the **average $y_{i,t}$** for the **control units after the treatment ($time_t$ =1 and $treated_i$ = 0)**.\n\n$\\beta_0+\\beta_2$ is the **average $y_{i,t}$** for the **treated units before the treatment ($time_t$ =0 and $treated_i$ = 1)**.\n\n$\\beta_0+\\beta_1+\\beta_2+\\beta_3$ is the **average $y_{i,t}$** for the **treated units after the treatment ($time_t$ =1 and $treated_i$ = 1)**.\n\n\n\n\n## Estimation   {.smaller background=\"#c6f7ec\"}\n\n+------------------+-----------------------------------+---------------------+---------------------+\n|                  |   Post-treat. (1)                 |  Pre-treat. (2)     |  **Diff. (1-2)**    |\n+------------------+-----------------------------------+---------------------+---------------------+\n|   Treated (a)    | $\\beta_0+\\beta_1+\\beta_2+\\beta_3$ | $\\beta_0+\\beta_2$   |**$\\beta_1+\\beta_3$**|\n+------------------+-----------------------------------+---------------------+---------------------+\n|  Control (b)     | $\\beta_0+\\beta_1$                 |  $\\beta_0$          | **$\\beta_1$**       |\n+------------------+-----------------------------------+---------------------+---------------------+\n| **Diff. (a-b)**  | **$\\beta_2+\\beta_3$**             |  **$\\beta_2$**      | **$\\beta_3$**       |\n+------------------+-----------------------------------+---------------------+---------------------+\n\n\n**This is why we call difference-in-differences.**\n\n**$\\beta_3$** gives us an estimate of the treatment effect on the treated units (ATE).\n\nThis is very popular because you can run as a regression.\n\n. . . \n\nMany papers simply show this table as final result.\n\nHowever, it is also possible that you include additional covariates (i.e., controls) in a DiD design. \n\nYou can also modify this to test the timing of the treatment (discussed later).\n\n\n\n\n\n\n\n\n\n\n# Controls in DiD  {.smaller background=\"#d5f5ee\"}\n\n## Controls in DiD  {.smaller background=\"#d5f5ee\"}\n\nIf you add controls, you have something in the form:\n\n$$y_{i,t} = \\beta_0 + \\beta_1 time_t + \\beta_2 treated_i  + \\beta_3 treated_i \\times time_t + \\beta_4x_{1,i,t}+ \\beta_5x_{2,i,t}+ ...+ \\epsilon_{i,t}$$\n\n. . . \n\n::: {.callout-important}\nBad Controls problem: Never add a control that is also affected by the treatment. \n:::\n\n. . . \n\n::: {.callout-tip}\nUsing pre-treatment values is good practice. \n:::\n\nIf you are including a *good control*, you should get lower standard deviations of the estimated effects.\n\n\n\n\n\n## Controls in DiD  {.smaller background=\"#d5f5ee\"}\n\nOne way of adding more controls is by adding FEs.\n\n$$y_{i,t} = \\beta_0 + \\beta_1 time_t + \\beta_2 treated_i  + \\beta_3 treated_i \\times time_t + FirmFE+ YearFE + \\epsilon_{i,t}$$\n\nThey help control for unobserved heterogeneity at the firm-level and in each year.\n\n. . . \n\n**But what do $\\beta_1$ and  $\\beta_2$ mean now?**\n\n. . .\n \n**Answer:** They are perfectly correlated with the $FirmFE$ and $YearFE$, respectively.\n\nReason: because they don't vary across each firm and year, respectively. \n\nThe software will most likely drop one $FirmFE$ and one $YearFE$, thus $\\beta_1$ and  $\\beta_2$ mean nothing.\n\n. . .\n\nI am guilty of this mistake because a referee asked and I couldn't reply properly.\n\n\n\n\n\n\n\n\n## Controls in DiD  {.smaller background=\"#d5f5ee\"}\n\nYou will be better off if you estimate:\n\n$$y_{i,t} = \\beta_0 + \\beta_3 treated_i \\times time_t + FirmFE+ YearFE + \\epsilon_{i,t}$$\n\nThis is called a **generalized DiD**.\n\nThe $FirmFE$ allow that each firm has one intercept (i.e., one average $y_{i,t}$).\n\nThe $YearFE$ accommodate a potential shock in $y_{i,t}$ in each year.\n\n\n\n\n\n\n\n\n\n## Controls in DiD  {.smaller background=\"#d5f5ee\"}\n\nAdditionally, adding *good* controls can be helpful to maintain the shock **exogenous (i.e., random) after controlling by X.**\n\n. . . \n\nImagine that a financial shock is more likely to affect high leveraged firms than low-leverage ones.\n\n1) the shock will not change the leverage decision of each firm.\n\n2) we believe that high leveraged firms will have a different $y_{i,t}$ after the treatment.\n\nYou may add leverage as a control to make sure the shock is random given the firms' levels of leverage.\n\n. . .\n\nIf you may believe that the leverage will change after the treatment, you can use pre-treatment values (interacted with the time dummies).\n\nIn this case, you are controlling for the pre-treatment condition but are not including the bias that might come due to changes in leverage after the treatment.\n\n\n\n\n\n\n\n## Controls in DiD  {.smaller background=\"#d5f5ee\"}\n\n*If assignment is random, then including additional covariates should have a negligible effect on the estimated treatment effect.*\n\n*Thus, a large discrepancy between the treatment effect estimates with and without additional controls raises a red flag.* \n\n*If assignment to treatment and control groups is not random but dictated by an observable rule, then controlling for this rule via covariates in the regression satisfies the conditional mean zero assumption required for unbiased estimates.* \n\n*Regardless of the motivation, it is crucial to remember that any covariates included as controls must be unaffected by the treatment, a condition that eliminates other outcome variables and restricts most covariates to pre-treatment values.*\n\n([Roberts and Whited, 2014](https://doi.org/10.1016/B978-0-44-453594-8.00007-0))\n \n \n\n\n\n\n\n# Comment about DiD {.smaller background=\"#aecfc7\"}\n\n## Comment about DiD  {.smaller background=\"#aecfc7\"}\n\n([The effect](https://theeffectbook.net/ch-DifferenceinDifference.html))\n\n\n*Parallel trends means we have to think very carefully about how our dependent variable is measured and transformed.*\n\n*Because parallel trends is an assumption about the size of a gap remaining constant, which means something different depending on how you measure that gap.*\n\n*For example, if parallel trends holds for dependent variable $Y$, then it doesn’t hold for $ln(y)$, and vice versa*\n\n. . .\n\n*For example, say that in the pre-treatment period $y$ is 10 for the control group and 20 for the treated group. In the post-treatment period, in the counterfactual world where treatment never happened,$y$ would be 15 for the control group and 25 for the treated group. Gap of $20-10=10$  before, and $25-15=10$ after. Parallel trends holds!*\n\n. . . \n\n*However: $Ln(20)-ln(10)=.693$  before, and $ln(25)-ln(15)=.51$ after. *\n\n\n\n\n\n\n\n\n\n\n# Example of DiD  {.smaller background=\"#98b8b0\"}\n\n## Example of DiD  {.smaller background=\"#98b8b0\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load the necessary library\nlibrary(foreign)\ndata <- read.dta(\"files/kielmc.dta\")\ndata$y81_nearinc <- data$y81 * data$nearinc\nmodel1 <- lm(rprice ~ y81 + nearinc + y81_nearinc, data = data)\nmodel2 <- lm(rprice ~ y81 + nearinc + y81_nearinc + age + agesq, data = data)\nmodel3 <- lm(rprice ~ y81 + nearinc + y81_nearinc + age + agesq + intst + land + area + rooms + baths, data = data)\nsummary(model3)\n```\n\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/kielmc.dta\", clear\ngen y81_nearinc = y81 * nearinc\neststo:qui reg rprice y81 nearinc y81_nearinc \neststo:qui reg rprice y81 nearinc y81_nearinc age agesq \neststo:qui reg rprice y81 nearinc y81_nearinc age agesq intst land area rooms baths\nesttab , compress\n```  \n\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Regression Discontinuity Design (RDD) {.smaller background=\"#e3e2b8\"}\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n**The regression-discontinuity (RD) research design is a quasi experimental method.**\n\nHere the *treatment* is not a binary as before. \n\n**Treated units are assigned based on a cutoff score of a continuous variable.**\n\n. . .\n\nIn a RDD the treatment assignment is not random...\n\n... but it is based in the value of an observed covariate, in which the units lie on either side of the threshold.\n\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n**[Example Mastering Metrics](https://www.masteringmetrics.com/):**\n\n- Age for drinking is 18 in Brazil. \n\nWhy prohibiting younger than 18?\n\nThe theory behind this effort is that legal drinking at age 18 discourages binge drinking and promotes a culture of mature alcohol consumption. \n\nThe cutoff splits who can drink and who can't.\n\n\n\n\n\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\nThe name RDD comes from a *jump*, a discontinuity that occurs in a continuous variable.\n\nIn its simplest form, the design has a:\n\n- The assignment variable (e.g., age), \n\n- Two groups (above and below the cutoff),\n\n- The outcome variable.\n\n- You may include nonlinearities and control variables. \n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\nRDD is:\n\n$$D_i = 1(x_i>c) \\;$$\n\nwhere:\n\n- $$\\;D = 1 \\; if \\;X \\;_i>c $$ \n\n- $$\\;D = 0 \\; if \\;X \\;_i<c $$\n\n\n$X$ is called the forcing variable because it \"forces\" units into treatment or control groups.\n\n$X$ may be correlated with $Y_1$ or $Y_0$, so simply comparing treated and control units would not provide causal effects\n\nHowever, if the units are randomly assigned into treatment around the cutoff, we would have causal effects.\n\n\n\n\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\nThe main assumption that allows using RDD as a causal method is that\n\n**Next to the cut, the participants are similar. The only difference is that one individual is in each of the \"sides\".** [Source](http://dx.doi.org/10.1016/B978-0-08-097086-8.44049-3)\n\n![](figs/rdd0.png){width=60%  height=60%}\n\n\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n- The cutoff value **occurs at 50**\n\n- What are the differences between someone that scores 49.99 and 50.01 in the X variable?\n\n- The intuition is that these individuals are similar and comparable.\n\nIn the absence of **treatment**, the assumption is that the solid line would \"continue\" with the same inclination and values. \n\nThere is a discontinuity, however. This implies that the pretreatment  in the absence of the treatment should be the dashed line.\n\nThe discontinuity is the causal effect of X (at the cutoff) to Y.\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n*Unlike the matching and regression strategies based on treatment-control comparisons conditional on covariates, the validity of RDD is based on our willingness to extrapolate across values of the running variable, at least for values in the neighborhood of the cutoff at which treatment switches on.* [MM](https://www.masteringmetrics.com/)\n\n\n\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n**Again the drinking example:**\n\n\n**In the US it is 21 years (age & alcohol).** **[Example Mastering Metrics](https://www.masteringmetrics.com/):**\n\n*Notice the x-axis.*\n\n![](figs/rdd1.png)\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n**In the US it is 21 years.** **[Example Mastering Metrics](https://www.masteringmetrics.com/):**\n\n*Notice the x-axis.*\n\n![](figs/rdd2.png)\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\nExamples [Almeida et al 2016](https://doi.org/10.1016/j.jfineco.2015.08.008)\n\n\n![](figs/almeida.png)\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\nExamples [Flammer 2015](https://doi.org/10.1287/mnsc.2014.2038)\n\n\n![](figs/flammer.png)\n\n\n\n\n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\nThis is (legally, it should be) an example of a **Sharp RDD**\n\nA **Sharp RDD** occurs when the cutoff is mandatory. There are no exceptions. In this case, there are no 17 years old drinking and driving. \n\nThe treatment is\n\n$$D_a= 1, \\;if \\;a \\;>=\\; 18, \\;0 \\;if \\;a\\; <\\; 18$$ \n\n\n. . . \n\nThe alternative is a **fuzzy RDD**, which occurs when there is some misassignment.\n\n- People from under the cut also receiving the treatment.\n- Ex. students that receive 5,96 usually are *approved* in a course (this is a misassignment).\n- To estimate a Fuzzy RDD, you can use the treatment as an instrumental variable (Angrist & Pischke, 2009).  \n\n\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n**fuzzy RDD** ([The effect](https://theeffectbook.net/ch-DifferenceinDifference.html) )\n\n\n![](figs/fuzzy.png)\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\n**fuzzy RDD** \n\n*Compliers*: Takes treatment if above threshold but not if below threshold.\n\n*Always-Takers*: Always takes the treatment, ignores the cut.\n\n*Never-Takers*: Never takes the treatment, ignores the cut.\n\n*Defiers*: Takes treatment if below the threshold, does not take the treatment if above the threshold.\n\n\n\n\n## Regression Discontinuity Design (RDD)   {.smaller background=\"#e3e2b8\"}\n\nThings that are \"good\" to a RDD.\n\n- Age \n- Dates (you need 6 years to start school, 5,99 years is not allowed)\n  - Great example: Most of NHL players are those with anniversaries just after the enrollment date\n- Ranking systems\n- Location (when people cannot \"move\" easily)\n\n\n\n\n\n\n# Estimation {.smaller background=\"#d4d3bc\"}\n\n\n## Estimation   {.smaller background=\"#d4d3bc\"}\n\nA **Sharp RDD** can take the form of:\n\n$$Y_i = \\alpha + \\beta_1 D_a + \\beta_2 \\tilde{x}_1 + \\epsilon$$\n\nWhere $D_a$ is the treatment based on the cutoff.\n\n$x$ is the running variable (the difference between the actual $X$ and the cutoff in $X_0$).\n\n- For instance, months until the 18 years birthday.\n\n- We can also use the notation: $\\tilde{X}=x-x_0$ regarding the difference. \n\n\n\n\n\n\n\n## Estimation   {.smaller background=\"#d4d3bc\"}\n\nYou also should trim the sample to a reasonable window around the cutoff $c$ \n\n- Something like: $c-h<X_i<c+h$, where $h$ is a positive value that determines the window\n\n- There is no theoretical solution to how big should $h$ be. It invites robustness tests.\n\n\n\n## Estimation   {.smaller background=\"#d4d3bc\"}\n\n**Final Step:**\n\nDecide on the model of E[Y|X]:\n\n- linear in both \"sides\" with same slope\n\n- linear with different slopes\n\n- non-linear\n\n\n\n**Tip**: always execute a visual inspection to check which model os appropriate.\n\nAlso, always estimate different models and discuss the results.\n\n\n\n\n\n\n## Estimation   {.smaller background=\"#d4d3bc\"}\n\n**fuzzy RDD** \n\n\n$$Y_i = \\alpha + \\beta_1 D_a + \\beta_2 \\tilde{x}_1 + \\beta_3(Z \\times \\tilde{x}_1) \\epsilon$$\n\nWhere Z is 1 if unit is above the cut or 0 if unit is below the cut.\n\nNotice that D is not equal to Z, in these cases.\n\n\n\n\n\n\n\n\n\n# RDD Nonlinearities  {.smaller background=\"#f2f1d5\"}\n\n## RDD Nonlinearities   {.smaller background=\"#f2f1d5\"}\n\n\n**[Example Mastering Metrics](https://www.masteringmetrics.com/):**\n\nThis is a linear relationship, with the same slopes.\n\n![](figs/rdd3.png)\n\n$$E[Y|X,D] = \\alpha + \\beta_1 \\times D + \\beta_2 \\times \\tilde{X}$$\n\n\n\n## RDD Nonlinearities   {.smaller background=\"#f2f1d5\"}\n\n\n[The effect](https://theeffectbook.net/ch-DifferenceinDifference.html) This is a linear relationship, with different  slopes.\n\n![](figs/rdd_diff_slope.png)\n\n\n\n\n\n\n\n\n\n\n## RDD Nonlinearities   {.smaller background=\"#f2f1d5\"}\n\n**[Example Mastering Metrics](https://www.masteringmetrics.com/):**\n\nThis is a nonlinear relationship.\n\n![](figs/rdd4.png)\n\n\n\n\n\n\n\n\n\n\n## RDD Nonlinearities   {.smaller background=\"#f2f1d5\"}\n\n**[Example Mastering Metrics](https://www.masteringmetrics.com/):**\n\n Is the relationship linear or nonlinear here? If you misjudge the relationship, it will be hard to tell a story credibly.\n\n![](figs/rdd5.png)\n\n\n\n\n\n\n\n\n\n## RDD Nonlinearities   {.smaller background=\"#f2f1d5\"}\n\n**The takeaway:** **RDD is a graphical method. You need to show the graphs**.\n\n**Nobody will believe your story without the correct specification of the model**.\n\n. . .\n\nIn the first case:\n\n\n$$Y_i = \\alpha + \\beta_1 D_a + \\beta_2 x_1 + \\epsilon$$\n\nIn the second case:\n\n$$Y_i = \\alpha + \\beta_1 D_a + \\beta_2 x_1 + \\beta_3 x_1^2 + \\epsilon$$\n\n\n\n\n\n\n\n## RDD Nonlinearities   {.smaller background=\"#f2f1d5\"}\n\nWe can also add an interaction term (notice that I am changing the notation now to make it similar to MM)\n\n$$Y_i = \\alpha + \\beta_1 D_a + \\beta_2 (x - x_0) + \\beta_3 (x - x_0) D_a + \\epsilon$$\n\nThis allows for different inclinations before and after the cut.\n\n\n\n## RDD Nonlinearities   {.smaller background=\"#f2f1d5\"}\n\nOr even different nonlinearities before and after the cut:\n\n\n$$Y_i = \\alpha + \\beta_1 D_a + \\beta_2 (x - x_0) + \\beta_3 (x - x_0)^2 + \\beta_4 (x - x_0) D_a  + \\beta_5 (x - x_0)^2 D_a + \\epsilon$$\n\n\n![](figs/rdd6.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Example RDD  {.smaller background=\"#edecc0\"}\n\n## Example RDD **Clearly, this is not linear.** {.smaller background=\"#edecc0\"}\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(readxl)\nlibrary(ggplot2)\ndata  <- read_excel(\"files/RDD.xlsx\")\nggplot(data, aes(x, y))  + \n  geom_point(size=1.2) + \n  labs(y = \"\", x=\"\", title = \"Evolution of Y\") +\n  theme(plot.title = element_text(color=\"black\", size=20, face=\"bold\"),\n        panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n        axis.text.y = element_text(face=\"bold\", color=\"black\", size = 12),\n        axis.text.x = element_text(face=\"bold\", color=\"black\", size = 12),\n        legend.title = element_blank(),\n        legend.key.size = unit(1, \"cm\")) +\n    geom_smooth(method = \"lm\", fill = NA)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Read Excel file\ndata = pd.read_excel(\"files/RDD.xlsx\")\n# Generate line graph - Including all observations together\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(10, 5))\nscatter_plot = sns.scatterplot(x='x', y='y', data=data, s=50)\nscatter_plot.set_title(\"Evolution of Y\", fontsize=20, fontweight='bold')\nscatter_plot.set_xlabel(\"\", fontsize=12, fontweight='bold')\nscatter_plot.set_ylabel(\"\", fontsize=12, fontweight='bold')\n# Add regression line\nsns.regplot(x='x', y='y', data=data, scatter=False, line_kws={'color': 'blue'})\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: false\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nimport excel \"files/RDD.xlsx\", firstrow\ntwoway (scatter y x) (lfit y x)\nquietly graph export figs/graph1.svg, replace\n```  \n\n![](figs/graph1.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Example RDD  {.smaller background=\"#edecc0\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(readxl)\nlibrary(ggplot2)\ndata  <- read_excel(\"files/RDD.xlsx\")\n# Creating  groups\ndata$treated <- 0\ndata$treated[data$x >= 101] <- 1  \n# Generate a line graph - two groups\nggplot(data, aes(x, y, group=treated, color = factor(treated)))  + \n    geom_point( size=1.25) + \n    labs(y = \"\", x=\"\", title = \"RDD exemplo\")+\n    theme(plot.title = element_text(color=\"black\", size=25, face=\"bold\"),\n          panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n          axis.text.y = element_text(face=\"bold\", color=\"black\", size = 16),\n          axis.text.x = element_text(face=\"bold\", color=\"black\", size = 16),\n          legend.title = element_blank(),\n          legend.key.size = unit(2, \"cm\")) +\n    geom_smooth(method = \"lm\", fill = NA)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Read Excel file\ndata = pd.read_excel(\"files/RDD.xlsx\")\n# Create treated variable\ndata['treated'] = 0\ndata.loc[data['x'] >= 101, 'treated'] = 1\n# Generate line graph with two groups\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(10, 5))\nscatter_plot = sns.scatterplot(x='x', y='y', hue='treated', style='treated', data=data, s=50)\nscatter_plot.set_title(\"RDD exemplo\", fontsize=25, fontweight='bold')\nscatter_plot.set_xlabel(\"\", fontsize=16, fontweight='bold')\nscatter_plot.set_ylabel(\"\", fontsize=16, fontweight='bold')\nscatter_plot.legend().set_title('')\nscatter_plot.legend(title='', loc='upper left', fontsize='small')\n# Add regression lines\nsns.regplot(x='x', y='y', data=data[data['treated'] == 0], scatter=False, line_kws={'color': 'blue'})\nsns.regplot(x='x', y='y', data=data[data['treated'] == 1], scatter=False, line_kws={'color': 'orange'})\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: false\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nimport excel \"files/RDD.xlsx\", firstrow\ngen treated = 0\nreplace treated = 1 if x >= 101\ntwoway (scatter y x) (lfit y x if treated == 0) (lfit y x if treated == 1)\nquietly graph export figs/graph2.svg, replace\n```  \n\n![](figs/graph2.svg) \n\n:::\n\n\n\n\n\n\n\n\n\n## Example RDD  {.smaller background=\"#edecc0\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(readxl)\nlibrary(ggplot2)\ndata  <- read_excel(\"files/RDD.xlsx\")\n# Creating  groups\ndata$treated <- 0\ndata$treated[data$x >= 101] <- 1  \n# define cut\ncut <- 100\nband <- 50\nxlow = cut - band\nxhigh = cut + band\n# subset the data for the bandwidth\ndata <- subset(data, x > xlow & x <= xhigh, select=c(x, y,  treated))\n# Generate a line graph - two groups\nggplot(data, aes(x, y, group=treated, color = factor(treated)))  + \n  geom_point( size=1.25) + \n  labs(y = \"\", x=\"\", title = \"RDD example\")+\n  theme(plot.title = element_text(color=\"black\", size=25, face=\"bold\"),\n        panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n        axis.text.y = element_text(face=\"bold\", color=\"black\", size = 16),\n        axis.text.x = element_text(face=\"bold\", color=\"black\", size = 16),\n        legend.title = element_blank(),\n        legend.key.size = unit(2, \"cm\")) +\n  geom_smooth(method = \"lm\", fill = NA)\n```\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Read Excel file\ndata = pd.read_excel(\"files/RDD.xlsx\")\n# Create treated variable\ndata['treated'] = 0\ndata.loc[data['x'] >= 101, 'treated'] = 1\n# Define cut, band, and subset data\ncut = 100\nband = 50\nxlow = cut - band\nxhigh = cut + band\ndata = data[(data['x'] > xlow) & (data['x'] <= xhigh)][['x', 'y', 'treated']]\n# Generate line graph with two groups\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(10, 5))\nscatter_plot = sns.scatterplot(x='x', y='y', hue='treated', style='treated', data=data, s=50)\nscatter_plot.set_title(\"RDD example\", fontsize=25, fontweight='bold')\nscatter_plot.set_xlabel(\"\", fontsize=16, fontweight='bold')\nscatter_plot.set_ylabel(\"\", fontsize=16, fontweight='bold')\nscatter_plot.legend().set_title('')\nscatter_plot.legend(title='', loc='upper left', fontsize='small')\n# Add regression lines\nsns.regplot(x='x', y='y', data=data[data['treated'] == 0], scatter=False, line_kws={'color': 'blue'})\nsns.regplot(x='x', y='y', data=data[data['treated'] == 1], scatter=False, line_kws={'color': 'orange'})\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: false\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nimport excel \"files/RDD.xlsx\", firstrow\ngen treated = 0\nreplace treated = 1 if x >= 101\ngen cut = 100\ngen band = 50\ngen xlow = cut - band\ngen xhigh = cut + band\nkeep if x > xlow & x <= xhigh\ntwoway (scatter y x) (lfit y x if treated == 0) (lfit y x if treated == 1)\nquietly graph export figs/graph3.svg, replace\n```  \n\n![](figs/graph3.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Example RDD  {.smaller background=\"#edecc0\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(readxl)\nlibrary(ggplot2)\ndata  <- read_excel(\"files/RDD.xlsx\")\ndata$treated <- 0\ndata$treated[data$x >= 101] <- 1  \ncut <- 100\nband <- 50\nxlow = cut - band\nxhigh = cut + band\ndata <- subset(data, x > xlow & x <= xhigh, select=c(x, y,  treated))\n# Generating xhat - Now we are going to the RDD\ndata$xhat <- data$x - cut\n# Generating xhat * treated to allow different inclinations (we will use the findings of the last graph, i.e. that each group has a different trend.)\ndata$xhat_treated <- data$xhat * data$treated\n# RDD Assuming different trends\nrdd <- lm(y  ~ xhat + treated  + xhat_treated, data = data)\nsummary(rdd)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_excel(\"files/RDD.xlsx\")\n# Generate treated variable\ndata['treated'] = 0\ndata.loc[data['x'] >= 101, 'treated'] = 1\n# Define cut and band\ncut = 100\nband = 50\nxlow = cut - band\nxhigh = cut + band\n# Subset data\ndata = data[(data['x'] > xlow) & (data['x'] <= xhigh)]\n# Generate xhat and xhat_treated\ndata['xhat'] = data['x'] - cut\ndata['xhat_treated'] = data['xhat'] * data['treated']\n# Regression\nX = data[['xhat', 'treated', 'xhat_treated']]\nX = sm.add_constant(X)  # Add a constant term\ny = data['y']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nimport excel \"files/RDD.xlsx\", firstrow clear\ngen treated = 0\nreplace treated = 1 if x >= 101\ngen cut = 100\ngen band = 50\ngen xlow = cut - band\ngen xhigh = cut + band\nkeep if x > xlow & x <= xhigh\ngen xhat = x - cut\ngen xhat_treated = xhat * treated\nregress y xhat treated xhat_treated\n```  \n\n:::\n\n\n\n\n## Example RDD  {.smaller background=\"#edecc0\"}\n\nThe coefficient of x before the cut is 0.29 (t-stat 5.45), and after the cut, it is -0.51 (t-stat -6.75). \n\nWe also have the coefficient of the treatment, which is measured by the \"jump\" that occurs near the cut: **an estimated coefficient of 28.9 (t-stat 13.11). **\n\nIf this were a real example, this would be the causal effect of receiving the treatment (i.e., being beyond the cut).\n\n\n\n\n\n\n\n\n\n\n# Final comments RDD {.smaller background=\"#f0eeb4\"}\n\n## Final comments RDD {.smaller background=\"#f0eeb4\"}\n\n\n**Sensitivity to specification**\n\nMisspecification can lead to a \"spurious jump\". **Do not mix nonlinearity with a discontinuity**.\n\n. . . \n\n**Sensitivity to window**\n\nAlso, need to check many alternative $h$. \n\n. . . \n\n**Smoothness of the running around the cut**\n\nThe distribution of X should be smooth around the cut. If it is not, it might indicate that the treatment assignment was not random. \n\n. . . \n\n**Test comparability of units around the cut**\n\nCovariates balance. You can check whether there are jumps in control variables as an additional robustness.\n\n. . . \n\n**Placebo tests**\n\nTest whether the treatment is zero when it should be zero.\n\n\n\n\n\n\n\n\n\n\n# Synthetic Control {.smaller background=\"#f2c7c4\"}\n\n## Synthetic Control {.smaller background=\"#f2c7c4\"}\n\nIt is a method to estimate the effect of events or policy interventions, often at an **aggregate level** (cities, states, etc.)\n\nThe event occurs often in **only one unit**.\n\nIt compares the evolution of the  outcome for the treated unit to the evolution of the control group.\n\n- The control group contains many units. \n\n. . . \n\nThe limitation is often the selection of the control group. It is very **ambiguous**.\n\n\n\n\n\n\n## Synthetic Control {.smaller background=\"#f2c7c4\"}\n\n[Abadie, Diamond, and Hainmueller (2010)](https://doi.org/10.1198/jasa.2009.ap08746) apply synth  by using a cigarette tax in California called Proposition 99.\n\n*In 1988, California passed comprehensive tobacco control legislation called Proposition 99.*\n\n*Proposition 99 increased cigarette taxes by $0.25 a pack, spurred clean-air ordinances throughout the state, funded anti-smoking media campaigns, earmarked tax revenues to health and anti-smoking budgets, and produced more than $100 million a year in anti-tobacco projects.*\n\n*Other states had similar control programs, and they were dropped from their analysis.* [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#cuba-miami-and-the-mariel-boatlift)\n\n\n\n\n\n\n## Synthetic Control {.smaller background=\"#f2c7c4\"}\n\nThere was a trend before the treatment. **How can we estimate the causal effect?**\n\n![](figs/synth1.png)\n\n\n\n\n\n\n\n## Synthetic Control {.smaller background=\"#f2c7c4\"}\n\nThe goal is to  elect an optimal set of weights that when applied to the rest of the country produces the following figure: \n\n![](figs/synth3.png)\n\n\n## Synthetic Control {.smaller background=\"#f2c7c4\"}\n\nThe variables used for computing the weights are the following. You are creating weights such that weighting the other states, you can create a synthetic California. **Notice that, so far, the product of this analysis is only two data points per period.** \n\n\n![](figs/synth2.png)\n\n\n\n## Synthetic Control {.smaller background=\"#f2c7c4\"}\n\n**Tip**: Synth is also an graphical method, so graphs like the following are common. This is the difference between the two series.\n\n![](figs/synth4.png)\n\n\n\n\n# Inference {.smaller background=\"#d49692\"}\n\n## Inference {.smaller background=\"#d49692\"}\n\n\n**Notice that, so far, the product of this analysis is only two data points per period.** How can you stablish a \"significant\" causal effect?\n\n\n**Steps**\n\n1) Apply synth to each state in the control group (also called \"donor pool\"). \n\n2) Obtain a distribution of placebos.\n\n3) Compare the gap for California to the distribution of the placebo gaps.\n\n4) Then, test whether the effect for the treated unit is large enough relative to the placebos (i.e., to the effect estimated for a placebo unit randomly selected).\n\n\n\n\n\n\n\n## Inference {.smaller background=\"#d49692\"}\n\nNotice the bold line (treated unit) after the treatment. It is at the bottom. \n\n![](figs/synth5.png)\n\n## Inference {.smaller background=\"#d49692\"}\n\n*Abadie, Diamond, and Hainmueller (2010) recommend iteratively dropping the states whose pre-treatment RMSPE is considerably different than California’s because as you can see, they’re kind of blowing up the scale and making it hard to see what’s going on.* [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#cuba-miami-and-the-mariel-boatlift)\n\n![](figs/synth6.png)\n\n\n\n\n\n\n## Inference {.smaller background=\"#d49692\"}\n\nThe previous figure suggests the effect is large enough relative to the placebo effects.\n\n. . . \n\n\n1) The root mean squared prediction error (RMSPE) is:\n\n$$RMSPE = \\bigg (\\dfrac{1}{T-T_0} \\sum_{t=T_0+t}^T \\bigg (Y_{1t} - \\sum_{j=2}^{J+1} w_j^* Y_{jt} \\bigg )^2 \\bigg )^{\\tfrac{1}{2}}$$\n\n*It shows how far predictions fall from measured true values using Euclidean distance.*\n\n\n\n2) Sort the ratio post- to pre-treatment RMSPE in descending order\n\n3) Calculate the p-value as $\\frac{Rank}{Total}$.\n\n\n. . . \n\nBasically, these steps give how likely is the occurrence of the treated unit distance vis-a-vis the average placebo.\n\n\n\n\n\n\n\n\n\n\n\n## Inference {.smaller background=\"#d49692\"}\n\n**RMSPE**: in the previous example, California has the largest increase in the error after the treatment. Position 1 out of 39 states, implying an exact p-value of $\\frac{1}{38}=0.026$ (significant). \n\n\n\n![](figs/synth7.png)\n\n\n\n\n\n\n\n\n\n\n\n\n# Example Synth  {.smaller background=\"#f5a7a2\"}\n\n## Example Synth  {.smaller background=\"#f5a7a2\"}\n\n::: panel-tabset\n\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/synth_smoking.dta , clear\ntsset state year\nsynth cigsale beer(1984(1)1988) lnincome retprice age15to24 cigsale(1988) cigsale(1980) cigsale(1975), trunit(3) trperiod(1989) \n```  \n\n:::\n\n\n## Example Synth  {.smaller background=\"#f5a7a2\"}\n\n::: panel-tabset\n\n\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: false\n#| code-summary: \"Stata\"\nuse files/synth_smoking.dta , clear\nsynth cigsale beer lnincome(1980&1985) retprice cigsale(1988) cigsale(1980) cigsale(1975), trunit(3) trperiod(1989) fig\nquietly graph export figs/synth1.svg, replace\n```  \n\n![](figs/synth1.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n## Example Synth  {.smaller background=\"#f5a7a2\"}\n\n\n*Authors using synthetic control must do more than merely run the synth command when doing comparative case studies.* \n\n*They must find the exact-values through placebo-based inference, check for the quality of the pre-treatment fit, investigate the balance of the covariates used for matching, and check for the validity of the model through placebo estimation (e.g., rolling back the treatment date).*\n\n\n\n\n\n\n\n\n\n\n\n\n\n# [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#prison-construction-and-black-male-incarceration)  {.smaller background=\"#de938e\"}\n\n\n## [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#prison-construction-and-black-male-incarceration)  {.smaller background=\"#de938e\"}\n\nIn 1992, Texas expanded the prision system operational capacity.\n\n![](figs/synth8.png)\n\n\n\n\n\n\n\n## [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#prison-construction-and-black-male-incarceration)  {.smaller background=\"#de938e\"}\n\nThis is what happened.\n\n![](figs/synth9.png)\n\n\n\n\n\n\n## [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#prison-construction-and-black-male-incarceration)  {.smaller background=\"#de938e\"}\n\nSynthetic Texas.\n\n![](figs/synth10.png)\n\n\n\n\n## [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#prison-construction-and-black-male-incarceration)  {.smaller background=\"#de938e\"}\n\nGap between Synthetic Texas and Texas.\n\n![](figs/synth11.png)\n\n\n\n\n## [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#prison-construction-and-black-male-incarceration)  {.smaller background=\"#de938e\"}\n\nBuilding Placebos.\n\n![](figs/synth13.png)\n\n\n\n\n## [Mastering Metrics](https://mixtape.scunning.com/10-synthetic_control#prison-construction-and-black-male-incarceration)  {.smaller background=\"#de938e\"}\n\nTexas is the one in the far right tail. \n\n![](figs/synth12.png)\n\n\n\n\n\n\n\n# Introduction IV {.smaller background=\"#bcd3f7\"}\n\n## Introduction IV {.smaller background=\"#bcd3f7\"}\n\nImagine the following model\n\n$$Ln(wage)=\\alpha + \\beta_1 educ + \\epsilon$$\n\nWe can infer that *educ* is correlated with *ability*, but the latter is in the error term. \n\n*educ* in this case is \"endogenous\".\n\n$$Cov(educ, \\epsilon) \\neq 0$$\n\n\n\n## Introduction IV {.smaller background=\"#bcd3f7\"}\n\n**The setup of an IV is**\n\nSuppose that we have an observable variable z that satisfies these two assumptions: \n\n(1) z is uncorrelated with u:\n\n$$Cov(z, \\epsilon) = 0$$\n\n\n(2) z is correlated with x:\n\n$$Cov(z, x) \\neq 0$$\n\n\n\nThen, we call z an instrumental variable for x, or sometimes simply an instrument for x.\n\n\n\n\n\n## Introduction IV {.smaller background=\"#bcd3f7\"}\n\nBefore we continue,\n\nIV is not a model, it is an **estimation method** \n\nI'll call it a **Design**.\n\n\nDo not say, I estimated an IV model (more often than it should be).\n\n\n\n\n\n\n\n\n\n\n# Instrumental Variables {.smaller background=\"#8eadde\"}\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\nImagine that you have one independent variable that is \"endogenous\":\n\n- $Cov(x_k,\\mu)\\neq 0$\n\n- You may have many other independent variables not \"endogenous\"\n\n. . .\n\nIn this situation:\n\n- $B_k$ is biased\n\n- The other betas will likely be biased as well, since it is unlikely that all other Xs are not correlated with $x_k$\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n$x_k$ is the endogenous variable.\n\n- It has \"good\" variation: \n\n  - the part that varies that is not correlated with $\\mu$\n\n- It has \"bad\" variation: \n\n  - the part that varies that is  correlated with $\\mu$\n\n. . . \n\nLet's assume now that you can find an instrument $z$\n\n- The instrument $z$ is correlated with $X_k$, but only the \"good\" variation, not the \"bad\".\n\n- The instrument $z$ does not explain $y$ directly, only through $x_k$.\n\n  - **Only through** condition.\n\n\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n**Relevance Condition**\n\nThe instrument $z$ is correlated with $x_k$.\n\n- This assumption is easy to test. Simply run a regression of $x=f(z, all\\; Xs)$ and check the significance of the $\\beta_z$.\n\n- This is called the **first stage of an IV regression**\n\n  - **Tip**: Always show the beta coefficient and the R2 (even if low) of the first-stage.  \n\n. . . \n\n**Exclusion Condition**\n\nThe instrument $z$ is not correlated with $\\mu$.\n\n- That is $cov(z,\\mu) = 0$\n\n- As all correlations with $\\mu$, you cannot test this prediction. You have to rely on the theory, create a story about that.\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n**An example of IV** [Murray](https://doi.org/10.1257/jep.20.4.111).\n\n![](figs/iv2.png)\n\n\n\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n**An example of IV** [Murray](http://dx.doi.org/10.1016/j.jcorpfin.2013.12.013).\n\n![](figs/iv6.png)\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#8eadde\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#good-instruments-should-feel-weird)\n\n**Good instruments should feel weird**\n\nParents with two same-gender kids are more likely to try a third kid than a diverse-gender pair of parents.\n\nSo, you may use the gender of the kids as instrument for the likelihood of the mother go back to the labor market.\n\n\n\n\n\n\n# Angrist's example {.smaller background=\"#d2e2fc\"}\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\nRemember the **Fuzzy RDD**.\n\n- There is the *treatment*\n- There is the *position* (before or after the cut) \n\nThe *position* is an indication of receiving or not the treatment, but it is not definitive.\n\nThus, we can use the *position* as an IV for the treatment.\n\n\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#the-problem-of-weak-instruments)\n\n*One of the more seminal papers in instrumental variables for the modern period is Angrist and Krueger (1991).* \n\n*Their idea is simple and clever; a quirk in the United States educational system is that a child enters a grade on the basis of his or her birthday.* \n\n*For a long time, that cutoff was late December. If children were born on or before December 31, then they were assigned to the first grade. But if their birthday was on or after January 1, they were assigned to kindergarten. *\n\n*Thus two people—one born on December 31 and one born on January 1—were exogenously assigned different grades.*\n\nEveryone is forced to leave school when 16.\n\n\n\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#the-problem-of-weak-instruments)\n\n![](figs/iv3.png)\n\n*Angrist and Krueger had the insight that that small quirk was exogenously assigning more schooling to people born later in the year.*\n\n*The person born in December would reach age 16 with more education than the person born in January*\n\n\n\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#the-problem-of-weak-instruments)\n\n![](figs/iv4.jpg)\n\n\n**What is the instrument here?**\n\n\n\n## Angrist's example {.smaller background=\"#d2e2fc\"}\n\n[Mixtape](https://mixtape.scunning.com/07-instrumental_variables#the-problem-of-weak-instruments)\n\n\n**What is the instrument here?**\n\nThe instrument is the quarter of birth.\n\n- People born in the 3rd and 4th quarter receive more education than others due to **compulsory schooling**.\n\n\n\n\n\n\n\n\n# Two-stage least squares (2SLS) {.smaller background=\"#abc8f5\"}\n\n## Two-stage least squares  (2SLS){.smaller background=\"#abc8f5\"}\n\n\nOne of the more intuitive instrumental variables estimators is the 2SLS. \n\n\n\n**The first stage is**\n\n$$x_k = \\delta + \\delta_1 z + \\delta_2 x_1 + . . .+ \\delta_n x_n + \\mu$$\n\nThen, you predict $x_k$ using the first stage.\n\n- \"predict\" means that you are finding the \"response\" Y of the equation after estimating the coefficients\n\n $$\\hat{x_k} = \\hat{\\delta} + \\hat{\\delta_1} z + \\hat{\\delta_2} x_1 + . . . + \\hat{\\delta_n} x_n $$\n\n\n. . . \n\nThen, **the second stage** is:\n\n\n$$y = \\alpha + \\beta_1 \\hat{x_k} + \\beta_2 x_1 + . . .+  \\beta_n x_n + \\mu$$\n\n\nThe idea using $\\hat{x_k}$ is that it represents only the variation that is not correlated with $\\mu$.\n\n\n\n\n\n\n\n\n## Instrumental Variables {.smaller background=\"#abc8f5\"}\n\nWe can write that:\n\n$$\\beta_1= \\frac{Cov(z,y)}{Cov(z,x_k)}$$\n\n\n- It shows that $\\beta_1$ is the population covariance between z and y divided by the population covariance between z and x.\n\n- Notice how this fails if z and x are uncorrelated, that is, if $Cov(z, x) = 0$ \n\n\n\n\n. . . \n\n\n\n$$\\beta_1= \\frac{\\sum_{i=1}^n(z_i-\\bar{z})(y_i-\\bar{y})}{\\sum_{i=1}^n(z_i-\\bar{z})(x_i-\\bar{x})}$$\n\n\nNotice that if $z$ and $x$ are the same (i.e., perfect correlation), $\\beta_1$ above is the OLS $\\beta$\n\n\n$$\\beta_1= \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}$$\n\n\n\n# Weak Instruments  {.smaller background=\"#8da6cc\"}\n\n## Weak Instruments {.smaller background=\"#8da6cc\"}\n\nI did not demonstrate here, but we can say that (see pp; 517-18 Wooldridge):\n\n- though IV is consistent when $z$ and $u$ are uncorrelated and $z$ and $x$ have any positive or negative correlation, IV estimates can have large standard errors, especially if $z$ and $x$ are only weakly correlated.\n\nThis gives rise to the week instruments problem.\n\nWe can write the probability limit of the IV estimator:\n\n$$plim \\hat{\\beta_{iv}} = \\beta_1 + \\frac{Corr(z,\\mu)}{Corr(z,x)} \\times \\frac{\\sigma_{\\mu}}{\\sigma_x}$$\n\nIt shows that, even if $Corr(z,\\mu)$ is small, the inconsistency in the IV estimator can be very large if $Corr(z,x)$ is also small.\n\n\n\n\n\n\n\n## Weak Instruments {.smaller background=\"#8da6cc\"}\n\nTwo tips:\n\n1) Look at the F-stat of the first-stage. It should not be low.\n\n2) The sign of the coefficient in the first stage should be as expected.\n\n3) Look at the S.E. When the instrument is week, the S.E., are even larger than it should be.\n\n. . .\n\nOne more tip,\n\n4) Avoid computing the first stage by hand, it would give wrong estimates of the S.E. in the second stage.\n\n\n\n\n\n\n\n\n\n\n\n# Example {.smaller background=\"#95a9c7\"}\n\n## Example  {.smaller background=\"#95a9c7\"}\n\nOLS\n\n::: panel-tabset\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/mroz.dta , clear\nreg lwage educ \n```  \n\n:::\n\n\n\n\n\n\n\n\n## Example  {.smaller background=\"#95a9c7\"}\n\nThe IV estimate of the return to education is 5.9%, which is barely more than one-half of the  OLS estimate. This suggests that the OLS estimate is too high and is consistent with omitted ability bias.\n\n::: panel-tabset\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/mroz.dta , clear\nivreg lwage (educ =fatheduc ) , first\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Final Comments {.smaller background=\"#bcd3f7\"}\n\n## Final Comments {.smaller background=\"#bcd3f7\"}\n\nWhen you have only one Z, you can say that the model is **just identified**.\n\nBut you can have multiple IVs, in which case you will say that the model is **overidentified**.\n\n- You can implement IV design just as before\n\n- The relevance and exclusion assumptions are there as well.\n\n. . . \n\nAssuming that both conditions are satisfied, you will have more asymptotic efficiency in the IV estimates.\n\n\n\n\n\n\n## Final Comments {.smaller background=\"#bcd3f7\"}\n\n**It is rare to have multiple Zs. You should be happy if you have a good one!**\n\n. . . \n\n\nBut if you do have multiple IVs, you can test their quality...\n\n- If they are all valid, you should get consistent estimates...\n\n- ... even if you use only a subset of them.\n\n- So the test is about how similar the estimates are if you use subsets of IVs.\n\n\n**But this test does not give really an answer about whether the IVs are good.**\n\nThis always come from theory.\n\n\n\n\n## Final Comments {.smaller background=\"#bcd3f7\"}\n\n**Some more comments:**\n\n1) If you have an interaction term between $x_1$ and $x_2$, and $z$ is the instrument for $x_1$, you can \"create\" the instrument $zx_2$ for $x_2$.\n\n. . . \n\n2) GMM uses lagged variables as instruments. But, this is not a good decision if the variables are highly serially correlated.\n\n  - Lagged total assets is not a good instrument for total assets.\n  \n. . . \n\n3) Using the average-group of variable X is also problematic (i.e., the industry average own. concentration as IV of firm-level own. concentration)\n\n - This is no different than a group FE, making hard to believe in the exclusion restriction.\n\n\n\n\n## THANK YOU!\n\n::: columns\n::: {.column width=\"30%\"}\n![](figs/fgv.png){fig-align=\"right\"}\n:::\n\n::: {.column width=\"70%\"}\n**Henrique Castro Martins**\n\n-   [henrique.martins\\@fgv.br](henrique.martins@fgv.br)\n-   <https://eaesp.fgv.br/en/people/henrique-castro-martins>\n-   [henriquemartins.net](https://henriquemartins.net/)\n-   <https://www.linkedin.com/in/henriquecastror/>\n:::\n:::\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","css":["styles.css"],"output-file":"part_99.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.4.549","auto-stretch":true,"html":{"css":"webex.css","include-after-body":"webex.js"},"editor":"visual","title":"Inferência Causal","subtitle":"Part 2","author":"Henrique C. Martins","slideNumber":true,"theme":"simple","chalkboard":true,"previewLinks":"auto","logo":"figs/background2.png","footer":"<https://eaesp.fgv.br/>","multiplex":true}}},"projectFormats":["html"]}