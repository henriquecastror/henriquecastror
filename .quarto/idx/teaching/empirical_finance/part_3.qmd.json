{"title":"Empirical Methods in Finance","markdown":{"yaml":{"title":"Empirical Methods in Finance","subtitle":"Part 3","author":"Henrique C. Martins","format":{"revealjs":{"slide-number":true,"theme":"simple","chalkboard":true,"preview-links":"auto","logo":"figs/background8.png","css":"logo.css","footer":"**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  ","multiplex":true,"scrollable":true}},"title-slide-attributes":{"data-background-color":"#b1cafa"},"include-after":"<script type=\"text/javascript\">\n  Reveal.on('ready', event => {\n    if (event.indexh === 0) {\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n  });\n  Reveal.addEventListener('slidechanged', (event) => {\n    if (event.indexh === 0) {\n      Reveal.configure({ slideNumber: null });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n    if (event.indexh === 1) {\n      Reveal.configure({ slideNumber: 'c' });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n    }\n  });\n</script>\n"},"headingText":"library(reticulate)","containsRefs":false,"markdown":"\n\n\n\n```{r setup}\n#| include: false\n#| warning: false\n\n\n# use_python(\"C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe\")\nlibrary(reticulate)\nlibrary(Statamarkdown)\n#reticulate::py_install(\"matplotlib\")\n#reticulate::py_install(\"seaborn\")\n#reticulate::py_install(\"pyfinance\")\n#reticulate::py_install(\"xlrd\")\n#reticulate::py_install(\"quandl\")\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Conterfactuals {.smaller background=\"#b3eafc\"}\n\n## Conterfactuals {.smaller background=\"#b3eafc\"}\n\n-   Imagine that John and Mary are moving to the north of Canada.\n\n-   John has a history of respiratory disease and decide to buy insurance.\n\n-   Mary does not have a history of respiratory disease and decide not to buy insurance.\n\n-   What is the causal effect of buying insurance?\n\n| Default                     | John   |   Mary |\n|-----------------------------|:-------|-------:|\n| State of insurance          | 1      |      0 |\n| Situation without insurance | `n.o.` |      5 |\n| Situation with insurance    | 4      | `n.o.` |\n| Observed                    | 4      |      5 |\n| Effect                      | ?      |      ? |\n\n[Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845)\n\n\n\n\n\n\n\n\n## Conterfactuals {.smaller background=\"#b3eafc\"}\n\n**Na√Øve calculation: comparing John com Mary**\n\n$$Y_{john} - Y_{Mary} = 4 - 5 = -1$$\n\nConclusion: buying insurance has a negative effect on health.\n\n. . .\n\n**This is wrong!**\n\n[Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845)\n\n## Conterfactuals {.smaller background=\"#b3eafc\"}\n\n| Default                     | John | Mary |\n|-----------------------------|:-----|-----:|\n| State of insurance          | 1    |    0 |\n| Situation without insurance | `3`  |    5 |\n| Situation with insurance    | 4    |  `5` |\n| Observed                    | 4    |    5 |\n| Effect                      | ?    |    ? |\n\n$$(Y_{1,john} - Y_{0,john}) + (Y_{1,Mary}- Y_{0,Mary}) = 4 - 3 + 5 - 5 = 0.5$$\n\n**Conclusion:** buying insurance has a positive effect of 1 in John's health and average effect of 0.5 in the sample's health (i.e. averages conditional on insurance status).\n\n[Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Regressions {.smaller background=\"#dfe3f7\"}\n\n## Regression [Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n**Let's see how a regression could solve the problem.** Imagine that you have the following data on students' application. (**Decisions in bold**)\n\n| Student | Private   | Private   | Private   | Public    | Public    | Public    | Earnings |\n|---------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n|         | Ivy       | Leafy     | Smart     | State     | Tall      | Altered   | 110,000  |\n| 1       |           | Reject    | **Admit** |           | Admit     |           | 110,000  |\n| 2       |           | Reject    | **Admit** |           | Admit     |           | 100,000  |\n| 3       |           | Reject    | Admit     |           | **Admit** |           | 110,000  |\n| 4       | **Admit** |           | Admit     |           | Admit     | Admit     | 60,000   |\n| 5       | Admit     |           | Admit     |           | Admit     | **Admit** | 30,000   |\n| 6       |           | **Admit** |           |           |           |           | 115,000  |\n| 7       |           | **Admit** |           |           |           |           | 75,000   |\n| 8       | Reject    |           |           | **Admit** | Admit     |           | 90,000   |\n| 9       | Reject    |           |           | Admit     | **Admit** |           | 60,000   |\n\n\n\n\n\n\n\n\n## Regression [Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n**We can see from the table that:**\n\n-   Some students earn high salary, in both situations\n\n-   Some students earn low salary, in both situations\n\n-   There are clusters of students that applied for the same universities\n\n    -   How likely are they to be similar? Can we benefit from the fact they believe they are similar?\n\n. . .\n\n-   If we compare earnings from the first three individuals:\n\n    -   ((110 + 100)/ 2 - 11000) = -5.000\n\n-   If we compare earnings from individuals 4 and 5:\n\n    -   (60 - 30) = 30.000\n\n-   The average is:\n\n    -   25.000/2 = 12.500\n\n\n\n\n\n\n\n\n\n\n\n## Regression [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\nLet's create a dataframe to run regressions with the previous student's data.\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Create the data frame\ndata <- data.frame(\n  id = 1:9,\n  earnings = c(110000, 100000, 110000, 60000, 30000, 115000, 75000, 90000, 60000),\n  school = c(\"private\", \"private\", \"public\", \"private\", \"public\", \"private\", \"private\", \"public\", \"public\"),\n  private = c(1, 1, 0, 1, 0, 1, 1, 0, 0),\n  group = c(1, 1, 1, 2, 2, 3, 3, 4, 4)\n)\nprint(data)\n\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\n\nimport pandas as pd\ndata = pd.DataFrame({\n    'id': range(1, 10),\n    'earnings': [110000, 100000, 110000, 60000, 30000, 115000, 75000, 90000, 60000],\n    'school': [\"private\", \"private\", \"public\", \"private\", \"public\", \"private\", \"private\", \"public\", \"public\"],\n    'private': [1, 1, 0, 1, 0, 1, 1, 0, 0],\n    'group': [1, 1, 1, 2, 2, 3, 3, 4, 4]\n})\nprint(data)\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\ninput id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\nlist\n```\n:::\n\n\n\n\n\n\n\n\n## \"Naive\" regression all students [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\epsilon$$ **What is the benefit of private education here?**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Create the data frame\nmodel <- lm(earnings ~ private, data = data)\nsummary(model)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\n#pip install numpy scikit-learn statsmodels\nimport statsmodels.api as sm\nX = sm.add_constant(data['private'])  \ny = data['earnings']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nquiet input id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\n\nreg earnings private \n```\n:::\n\n\n\n\n\n\n\n\n## \"Naive\" regression all students [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\epsilon$$ **What is the benefit of private education here?**\n\nThe coefficient of `private` is 19500, meaning that those that have private education earn 19500 more.\n\n. . . \n\nThe problem with this design is that 1) we are including all students, even those that do not bring any \"information\", and 2) we are not controlling for the differences in students' profiles. \n\n\nLet's fix the first problem first. \n\n**What students should we not include in the model?**\n\n\n\n\n\n## Students id\\<=5 [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\epsilon \\;,\\; if\\; i <=5$$ **What is the benefit of private education here?**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n\nmodel2 <- lm(earnings ~ private , data = subset(data,id<=5))\nsummary(model2)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\n#pip install numpy scikit-learn statsmodels\n\nsubset_data = data[data['id'] <= 5]\nX = sm.add_constant(subset_data['private']) \ny = subset_data['earnings']\nmodel2 = sm.OLS(y, X).fit()\nprint(model2.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nquiet input id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\nreg earnings private if id<=5\n```\n:::\n\n\n\n\n\n## Students id\\<=5 [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\epsilon \\;,\\; if\\; i <=5$$ **What is the benefit of private education here?**\n\nStudents 6 and 7 only applied to Private, while students 8 and 9 did not really had a choice. So we should exclude them.\n\n. . .\n\nThe benefit of private is now 20000.\n\nThe coefficient did not change much, but the design improved partially.\n\n. . .\n\nWe still have an uncontrolled \"heterogeneity\" in the groups of students. **Students 1 to 3 seem to earn more no matter their decisions**.\n\n\n\n\n\n\n\n## Apples-to-Apples [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\beta_2 Group+ \\epsilon \\;,\\; if\\; i <=5$$ **This is the best we can do.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n\ndata$dummy <- ifelse(data$group == 1, 1, 0)\ndata$dummy[data$group == 2] <- 0\nmodel3 <- lm(earnings ~ private + dummy, data = subset(data,id<=5))\nsummary(model3)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\n#pip install numpy scikit-learn statsmodels\n\ndata['dummy'] = 1\ndata.loc[data['group'] == 2, 'dummy'] = 0\nsubset_data = data[data['id'] <= 5]\nX = sm.add_constant(subset_data[['private', 'dummy']])\ny = subset_data['earnings']\nmodel3 = sm.OLS(y, X).fit()\nprint(model3.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nquiet input id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\ngen \tdummy = 1 if group == 1\nreplace dummy = 0 if group == 2\nreg earnings private dummy if id<=5 \n```\n:::\n\n\n\n\n## Regression {.smaller background=\"#dfe3f7\"}\n\nThe previous regression assumes that students 1 to 3 are different that students 4 and 5. \n\nWe will find many instances like that in empirical research. E.g., industry. \n\n. . .\n\nThe private school coefficient, in this case 10,000, implies a private-public earnings differential of this value.\n\n. . .\n\n::: callout-important\nThe Y above is used in monetary values.\n\nUsing a logged y, ln(Y) or ln(earnings), allows estimates to be interpreted as a percent change.\n\nFor instance if $\\beta=0.05$, it means that the earnings differential is 5% for those studying in private schools (conditional on the controls included in the model). \n:::\n\n\n\n\n\n\n\n\n\n\n\n# OVB again {.smaller background=\"#f5caae\"}\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n\n\nRegression is a way to make other things equal (ceteris paribus), but equality  is generated only for variables included in the model as controls on the right-hand sided of the model.\n\nFailure to include enough controls of the right controls still leave us with selection bias.\n\nThe regression version of the selection bias generated by the inadequate controls is called **Omitted Variable Bias (OVB)**. \n\nThe inclusion of a control that should not be included is called \"**Bad Controls**\" problem.\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n**How could we calculate the OVB in this example?**\n\n\n$$earnings_i = 70.000 + 20.000\\times Private_i  \\epsilon $$\n\n$$earnings_i = 40.000 + 10.000 \\times Private_i + 60.000 \\times Group+ \\epsilon$$ \n\n\n- $\\beta$ (1st regression) - $\\beta$ (second regression).\n- The OVB here is 20.000 - 10.000 = 10.000.\n- Meaning that the $\\beta$ (1st regression) is 10.000 higher than what it should be.\n\n\n\n\n\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n**How could we calculate the OVB in this example?**\n\n\nWe could calculate the bias by estimating:\n\n$$Private=\\alpha + \\beta_{omitted} \\times Group + \\epsilon$$\n\nThen,\n\n$$\\beta_{omitted} \\times \\beta_{missing} = 0.1667 * 60.000 = 10.000$$\n\nThe OVB is 10.000, meaning that the first model (the one with the omitted variable) estimates a Beta that is 10.000 higher than it should be. \n\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nmodel4 <- lm(private ~ dummy , data = subset(data,id<=5))\nsummary(model4)\nmatrix2<- summary(model4)$coefficients\nsum(0.1667 * 60000 )\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nsubset_data = data[data['id'] <= 5]\nmodel4 = sm.OLS(subset_data['private'], sm.add_constant(subset_data[['dummy']])).fit()\nprint(model4.summary())\nbias = 0.1667 * 60000\nprint(bias)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nquiet input id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\ngen \tdummy = 1 if group == 1\nreplace dummy = 0 if group == 2\nreg private dummy if id<=5\ndi .1666667 *  60000 \n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n**So what?**\n\n- Anticipating the effect of the omitted variable on the non-omitted variable can tell you the sign of the bias.\n\n- Then you can know if the bias is attenuating or increasing the effect you are investigating.\n\n- If attenuating, the problem is smaller than if it is increasing\n\n\n\n\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n**Regressions**\n\n-   The previous examples show that we can run **regressions and find correlations** ...\n\n-   ... And we can run regressions and find **causal effects**.\n\n-   But we need to control for all relevant variables, otherwise we have the *OVB problem*.\n\n-   Should you not look careful to your data, you'd miss the inclusion of the variable `group`.\n\n-   The results show that you may estimate a spurious coefficient twice the size of the \"true\" coefficient.\n\n\n\n\n\n\n\n# Bad Controls Problem {.smaller background=\"#dff2c7\"}\n\n\n## Bad Controls Problem {.smaller background=\"#dff2c7\"}\n\n**Bad controls** are variables that are **also outcome of the treatment** being studied.\n\nA **Bad control** could very well be a **dependent variable** of the treatment as well. \n\n**Good controls** are variables that **you can think as being fixed** at the time of the treatment. \n\n. . .\n\nLet's return to the model.\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\beta_2 Group+ \\epsilon \\;,\\; if\\; i <=5$$ \n\n\nAssuming you also have the occupation of the students at the time of earnings. Should you include `occupation` in the model?\n\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\beta_2 Group + \\beta_3 Occupation + \\epsilon \\;,\\; if\\; i <=5$$ \n\nReasoning: \"*We should use occupation as control because it would be wise to look at the effect of education on earnings only for those within an occupation*\".\n\nWhat is the problem with this reasoning?\n\n\n\n\n\n\n\n\n## Bad Controls Problem {.smaller background=\"#dff2c7\"}\n\nThe problem is that studying in private would increase the chances of getting a white-collar occupation, i.e., *private education (treatment) affects the occupation (bad control)*.\n\nIn this case, should you include occupation as control, the coefficient of interest no longer has a causal interpretation.\n\n\n. . .\n\n**This is a very common problem in empirical research**.\n\nIt is not hard to come up with stories of why a control is a bad control.\n\n\n\n\n\n\n# Randomization {.smaller background=\"#ff9c6b\"}\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n**Now I want to discuss the idea of randomization**\n\nSuppose you have developed a treatment (e.g., a program) that you believe will increase the 'motivation' of employees of a factory.\n\nYou have 100 employees to use in an experiment to test your claim that the treatment will increase motivation.\n\n. . .\n\n- You randomly allocate 50 employees to receive the treatment. The other 50 are part of the control group.\n\n. . .\n\n- You treat all employees in the same manner, except for the treatment.\n\n\n\n. . .\n\nUsing the data available, this is the **difference in motivation between the treatment and control groups (next slide):**\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(dplyr)\ndata  <- read_excel(\"files/part_3_data.xlsx\", range = \"A1:C101\")\n# Box plot control vs treatment groups\nggplot(data, aes(y=motivation, fill=group)) +   \n  geom_boxplot()+\n  theme(plot.title = element_text(color=\"black\", size=30, face=\"bold\"),\n        panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n        axis.text.y = element_text(face=\"bold\", color=\"black\", size = 18),\n        axis.text.x = element_blank(),\n        legend.title = element_blank(),\n        legend.key.size = unit(3, \"cm\"))\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Read data from Excel file\ndata = pd.read_excel(\"files/part_3_data.xlsx\")\n\n# Create a box plot of control vs treatment groups using seaborn\nplt.figure(figsize=(7, 5))\nsns.set(style='whitegrid')\nsns.boxplot(x='group', y='motivation', data=data, palette='Set2')\nplt.title(\"Box Plot of Control vs Treatment Groups\", fontsize=18)\nplt.xlabel(\"Group\", fontsize=14)\nplt.ylabel(\"Motivation\", fontsize=14)\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\n\nimport excel \"files/part_3_data.xlsx\", cellrange(A1:C101) firstrow clear\ngraph box motivation , over(group) box(1, color(black)) \tytitle(\"Motivation\")  \n\nquietly graph export \"files/graph3_5.svg\", replace\n```       \n\n![](files/graph3_5.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\nThe calculated means are below. And they are statistically different.\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\ndata  <- read_excel(\"files/part_3_data.xlsx\", range = \"A1:C101\")\ntapply(data$motivation, data$group, summary)\nt.test(motivation ~ group, data = data)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\ndata = pd.read_excel(\"files/part_3_data.xlsx\")\ngroup_summary = data.groupby('group')['motivation'].describe()\nprint(group_summary)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nimport excel \"files/part_3_data.xlsx\", cellrange(A1:C101) firstrow clear\nbys group  : sum motivation\nestpost ttest motivation , by(group)\n```\n:::\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n**Is there evidence that the program has increased motivation?**\n\n. . .\n\n- well, if you randomly split a group of 100 people into two groups of 50, you certainly wouldn't get the same mean motivation in both groups even if you treated them exactly alike. \n\n- Maybe the difference that we see is just such a difference?\n\n**How can we test this hypothesis?**\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n**Solution**: \n\n- Suppose the treatment had no effect, and the employees developed their motivation  independently of the treatment. \n\n- What is the chance that the 50 employees randomly assigned to the treatment group would have an average at least 1.47 (22.27 - 20.80)  points higher than the average motivation of the employees randomly assigned to the control group?\n\n\n\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n**Steps**\n\n1) Randomly split the 100 employees that we observed in this experiment into two groups of 50.\n\n2) Note the difference in the mean motivation  between the two groups.\n\n3) Repeat 1 and 2 a total of 10,000 times.\n\n4) Note the proportion of times the difference is at least 1.47 (22.27 - 20.80).\n\n\n\n\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary libraries\ndata <- read_excel(\"files/part_3_data.xlsx\", range = \"A1:C101\")\ncomb <- 10000\ndf <- data.frame(matrix(ncol = 2, nrow = comb))\ncolnames(df) <- c(\"order\" ,\"diff\")\n# Create the loop for randomization:\nfor (i in seq(from = 1, to = comb)) {\n  set.seed(i)                               \n  data$temp <- runif(100, min = 0, max = 1)  # Creating 100 random numbers 0 to 1\n  data <- data[order(data$temp),]            # Sorting data by the random numbers generated in the previous row\n  data$rank <- rank(data$temp)               # Ranking by the random numbers\n# The row below defines the treatment group based on the random numbers generated. This is where we guarantee randomization\ndata$status_rank <- case_when(data$rank <= 50 ~ \"Control_rand\", data$rank > 50 ~ \"Treated_rand\")\n# Calculate the new means of the new groups. Need to transpose data.\nmeans <- t(as.data.frame(tapply(data$motivation, data$status_rank, mean)))\n# Moving the new means to df. Each row is the difference of means\ndf[i, 1] <- i\ndf[i, 2] <- means[1, 2] - means[1, 1]\nrm(means) # Deleting value\ndata = subset(data, select = -c(temp, rank, status_rank)) # Deleting variables\n}\n# Calculate a suitable binwidth for the histogram\nbinwidth <- (max(df$diff) - min(df$diff)) / sqrt(length(df$diff))\n# Create a histogram of the differences with the calculated binwidth\nggplot(df, aes(x = diff)) +\n  geom_histogram(binwidth = binwidth, fill = \"blue\", color = \"black\") +\n  labs(title = \"Distribution of Differences\", x = \"Difference\", y = \"Frequency\")\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nimport excel \"files/part_3_data.xlsx\", cellrange(A1:C101) firstrow clear\nset seed 472195 \t\t\nsort group\t\t \nset obs 10000 \t\t\t\t \negen fin_order = seq() \nsort fin_order \t\t\t\t \nsummarize \t\t\t\t\t\ngen av_diff=.\n\nlocal i = 1\nwhile `i'<=10000 {\n\n\tsort fin_order\n\tgen rand_num`i' = uniform() if !missing(motivation)\n\tegen ordering`i' = rank(rand_num`i')\n\tsort ordering`i'\n\n\tgen group`i' = \"\"\n\treplace group`i' = \"T\" if ordering <= 50\n\treplace group`i' = \"C\" if ordering > 50 & ordering<=100\n\t\n\tqui summ motivation if group`i'==\"T\"\n\tscalar avT = `r(mean)'\n\tqui summ motivation if group`i'==\"C\"\n\tscalar avC = `r(mean)'\n\t\n\tsort fin_order\n\treplace av_diff = avT-avC in `i'\n\t\n\tdrop rand_num`i' ordering`i' group`i'\n\tlocal i = `i' + 1\n}\nhistogram av_diff, frequency kdensity  \ngraph export \"files/graph3_6.png\" , replace\n\n```       \n\n![](files/graph3_6.png){ width=800px height=450px }\n:::\n\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\nThe mean difference was as far from 0 as 1.5 for only a few out of the 10,000 random divisions of the data into two groups of 50.\n\n- Thus, **the difference between the mean motivation would almost always be less than the observed difference of 1.47 (22.27 - 20.80) if the treatment had no effect.**\n\n- It seems reasonable to believe that the treatment caused the difference in motivation.\n\n\n\n\n\n\n\n\n\n# Measurement Error problem  {.smaller background=\"#f2e9b6\"}\n\n\n## Measurement Error problem  {.smaller background=\"#f2e9b6\"}\n\nThe measurement error problem has a similar statistical structure to the omitted variable bias (OVB).\n\n- \"Classical\" random measurement error for the $y$ will inflate standard errors but will not lead to biased coefficients. \n\n    - $y^{*} = y + \\sigma_{1}$\n    - If you estimante $y^{*} = f(x)$, you have $y + \\sigma_{1} = x + \\epsilon$ \n    - $y = x + u$ \n        - where $u = \\epsilon - \\sigma_{1}$ \n\n\n\n\n## Measurement Error problem  {.smaller background=\"#f2e9b6\"}\n\n- \"Classical‚Äù random measurement error in x‚Äôs will bias coefficient estimates toward zero.\n\n- $x^*=x+\\sigma_2$\n\n- Imagine that $x^*$ is a bunch of noise. It would not explain anything. Thus, your results are biased toward zero.\n\n\n\n\n\n## Measurement Error problem  {.smaller background=\"#f2e9b6\"}\n\nA example using one of the Wooldridge's datasets.\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(jtools)\ndata <- read.dta(\"files/CEOSAL1.dta\")\nset.seed(2)\ndata$salary_noise <- data$salary + runif(length((data$salary)), min=-100, max= 100)\ndata$roe_noise <- data$roe + runif(length((data$roe)), min=-100, max= 100)\n# OLS model \nmodel1 <- lm(data$salary ~ data$roe)\nmodel2 <- lm(data$salary ~ data$roe_noise)\nmodel3 <- lm(data$salary_noise ~ data$roe)\n#summary(model1)\n#summary(model2)\n#summary(model3)\nexport_summs(model1, model2, model3, digits = 3 , model.names = c(\"Roe\", \"Roe (X) with noise\", \"Salary (y) with noise\") )\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.iolib.summary2 import summary_col\n\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\nnp.random.seed(2)\n# Add noise to the 'salary' and 'roe' columns\ndata['salary_noise'] = data['salary'] + np.random.uniform(-100, 100, len(data))\ndata['roe_noise'] = data['roe'] + np.random.uniform(-100, 100, len(data))\n# OLS model\nmodel1 = smf.ols(formula='salary ~ roe', data=data).fit()\nmodel2 = smf.ols(formula='salary ~ roe_noise', data=data).fit()\nmodel3 = smf.ols(formula='salary_noise ~ roe', data=data).fit()\n# Create a summary table for all regressions\nresults = summary_col([model1, model2, model3], \n                      model_names=['Reg 1', 'Reg 2', 'Reg 3'],\n                      stars=True,\n                      float_format='%0.2f')\n# Print the summary table\nprint(results)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.dta\", clear\nset seed 2\ngen salary_noise = salary + runiform() * 200 - 100\ngen roe_noise = roe + runiform() * 200 - 100\neststo: qui reg salary roe\neststo: qui reg salary roe_noise\neststo: qui reg salary_noise roe\nesttab\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## üôã‚Äç‚ôÇÔ∏è **Any Questions?**{ .smaller  background=\"#fdf6e3\"}\n\n::: columns\n::: {.column width=\"50%\"}\n### Thank You!\n\n![](figs/qa2.png){width=\"110%\" style=\"box-shadow: none;\"}\n\n:::\n\n::: {.column width=\"50%\"}\n<div style=\"text-align:right;\">\n  <img src=\"figs/avatar.jpg\" width=\"120px\" style=\"border-radius:50%; box-shadow:0 4px 12px rgba(0,0,0,.25);\" />\n</div>\n\n### **Henrique C. Martins**\n\n- üåê [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)  \n- üíº [LinkedIn](https://www.linkedin.com/in/henriquecastror/)  \n- üß† [Google Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)  \n- üìÑ [Lattes CV](http://lattes.cnpq.br/6076997472159785)  \n- üè† [Personal Website](https://henriquemartins.net/)  \n:::\n:::\n\n","srcMarkdownNoYaml":"\n\n\n\n```{r setup}\n#| include: false\n#| warning: false\n\n\n# library(reticulate)\n# use_python(\"C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe\")\nlibrary(reticulate)\nlibrary(Statamarkdown)\n#reticulate::py_install(\"matplotlib\")\n#reticulate::py_install(\"seaborn\")\n#reticulate::py_install(\"pyfinance\")\n#reticulate::py_install(\"xlrd\")\n#reticulate::py_install(\"quandl\")\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Conterfactuals {.smaller background=\"#b3eafc\"}\n\n## Conterfactuals {.smaller background=\"#b3eafc\"}\n\n-   Imagine that John and Mary are moving to the north of Canada.\n\n-   John has a history of respiratory disease and decide to buy insurance.\n\n-   Mary does not have a history of respiratory disease and decide not to buy insurance.\n\n-   What is the causal effect of buying insurance?\n\n| Default                     | John   |   Mary |\n|-----------------------------|:-------|-------:|\n| State of insurance          | 1      |      0 |\n| Situation without insurance | `n.o.` |      5 |\n| Situation with insurance    | 4      | `n.o.` |\n| Observed                    | 4      |      5 |\n| Effect                      | ?      |      ? |\n\n[Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845)\n\n\n\n\n\n\n\n\n## Conterfactuals {.smaller background=\"#b3eafc\"}\n\n**Na√Øve calculation: comparing John com Mary**\n\n$$Y_{john} - Y_{Mary} = 4 - 5 = -1$$\n\nConclusion: buying insurance has a negative effect on health.\n\n. . .\n\n**This is wrong!**\n\n[Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845)\n\n## Conterfactuals {.smaller background=\"#b3eafc\"}\n\n| Default                     | John | Mary |\n|-----------------------------|:-----|-----:|\n| State of insurance          | 1    |    0 |\n| Situation without insurance | `3`  |    5 |\n| Situation with insurance    | 4    |  `5` |\n| Observed                    | 4    |    5 |\n| Effect                      | ?    |    ? |\n\n$$(Y_{1,john} - Y_{0,john}) + (Y_{1,Mary}- Y_{0,Mary}) = 4 - 3 + 5 - 5 = 0.5$$\n\n**Conclusion:** buying insurance has a positive effect of 1 in John's health and average effect of 0.5 in the sample's health (i.e. averages conditional on insurance status).\n\n[Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Regressions {.smaller background=\"#dfe3f7\"}\n\n## Regression [Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n**Let's see how a regression could solve the problem.** Imagine that you have the following data on students' application. (**Decisions in bold**)\n\n| Student | Private   | Private   | Private   | Public    | Public    | Public    | Earnings |\n|---------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n|         | Ivy       | Leafy     | Smart     | State     | Tall      | Altered   | 110,000  |\n| 1       |           | Reject    | **Admit** |           | Admit     |           | 110,000  |\n| 2       |           | Reject    | **Admit** |           | Admit     |           | 100,000  |\n| 3       |           | Reject    | Admit     |           | **Admit** |           | 110,000  |\n| 4       | **Admit** |           | Admit     |           | Admit     | Admit     | 60,000   |\n| 5       | Admit     |           | Admit     |           | Admit     | **Admit** | 30,000   |\n| 6       |           | **Admit** |           |           |           |           | 115,000  |\n| 7       |           | **Admit** |           |           |           |           | 75,000   |\n| 8       | Reject    |           |           | **Admit** | Admit     |           | 90,000   |\n| 9       | Reject    |           |           | Admit     | **Admit** |           | 60,000   |\n\n\n\n\n\n\n\n\n## Regression [Source: Mastering Metrics](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n**We can see from the table that:**\n\n-   Some students earn high salary, in both situations\n\n-   Some students earn low salary, in both situations\n\n-   There are clusters of students that applied for the same universities\n\n    -   How likely are they to be similar? Can we benefit from the fact they believe they are similar?\n\n. . .\n\n-   If we compare earnings from the first three individuals:\n\n    -   ((110 + 100)/ 2 - 11000) = -5.000\n\n-   If we compare earnings from individuals 4 and 5:\n\n    -   (60 - 30) = 30.000\n\n-   The average is:\n\n    -   25.000/2 = 12.500\n\n\n\n\n\n\n\n\n\n\n\n## Regression [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\nLet's create a dataframe to run regressions with the previous student's data.\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Create the data frame\ndata <- data.frame(\n  id = 1:9,\n  earnings = c(110000, 100000, 110000, 60000, 30000, 115000, 75000, 90000, 60000),\n  school = c(\"private\", \"private\", \"public\", \"private\", \"public\", \"private\", \"private\", \"public\", \"public\"),\n  private = c(1, 1, 0, 1, 0, 1, 1, 0, 0),\n  group = c(1, 1, 1, 2, 2, 3, 3, 4, 4)\n)\nprint(data)\n\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\n\nimport pandas as pd\ndata = pd.DataFrame({\n    'id': range(1, 10),\n    'earnings': [110000, 100000, 110000, 60000, 30000, 115000, 75000, 90000, 60000],\n    'school': [\"private\", \"private\", \"public\", \"private\", \"public\", \"private\", \"private\", \"public\", \"public\"],\n    'private': [1, 1, 0, 1, 0, 1, 1, 0, 0],\n    'group': [1, 1, 1, 2, 2, 3, 3, 4, 4]\n})\nprint(data)\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\ninput id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\nlist\n```\n:::\n\n\n\n\n\n\n\n\n## \"Naive\" regression all students [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\epsilon$$ **What is the benefit of private education here?**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Create the data frame\nmodel <- lm(earnings ~ private, data = data)\nsummary(model)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\n#pip install numpy scikit-learn statsmodels\nimport statsmodels.api as sm\nX = sm.add_constant(data['private'])  \ny = data['earnings']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nquiet input id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\n\nreg earnings private \n```\n:::\n\n\n\n\n\n\n\n\n## \"Naive\" regression all students [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\epsilon$$ **What is the benefit of private education here?**\n\nThe coefficient of `private` is 19500, meaning that those that have private education earn 19500 more.\n\n. . . \n\nThe problem with this design is that 1) we are including all students, even those that do not bring any \"information\", and 2) we are not controlling for the differences in students' profiles. \n\n\nLet's fix the first problem first. \n\n**What students should we not include in the model?**\n\n\n\n\n\n## Students id\\<=5 [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\epsilon \\;,\\; if\\; i <=5$$ **What is the benefit of private education here?**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n\nmodel2 <- lm(earnings ~ private , data = subset(data,id<=5))\nsummary(model2)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\n#pip install numpy scikit-learn statsmodels\n\nsubset_data = data[data['id'] <= 5]\nX = sm.add_constant(subset_data['private']) \ny = subset_data['earnings']\nmodel2 = sm.OLS(y, X).fit()\nprint(model2.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nquiet input id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\nreg earnings private if id<=5\n```\n:::\n\n\n\n\n\n## Students id\\<=5 [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\epsilon \\;,\\; if\\; i <=5$$ **What is the benefit of private education here?**\n\nStudents 6 and 7 only applied to Private, while students 8 and 9 did not really had a choice. So we should exclude them.\n\n. . .\n\nThe benefit of private is now 20000.\n\nThe coefficient did not change much, but the design improved partially.\n\n. . .\n\nWe still have an uncontrolled \"heterogeneity\" in the groups of students. **Students 1 to 3 seem to earn more no matter their decisions**.\n\n\n\n\n\n\n\n## Apples-to-Apples [Source](https://www.amazon.com.br/Mastering-Metrics-Path-Cause-Effect/dp/0691152845) {.smaller background=\"#dfe3f7\"}\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\beta_2 Group+ \\epsilon \\;,\\; if\\; i <=5$$ **This is the best we can do.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n\ndata$dummy <- ifelse(data$group == 1, 1, 0)\ndata$dummy[data$group == 2] <- 0\nmodel3 <- lm(earnings ~ private + dummy, data = subset(data,id<=5))\nsummary(model3)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\n#pip install numpy scikit-learn statsmodels\n\ndata['dummy'] = 1\ndata.loc[data['group'] == 2, 'dummy'] = 0\nsubset_data = data[data['id'] <= 5]\nX = sm.add_constant(subset_data[['private', 'dummy']])\ny = subset_data['earnings']\nmodel3 = sm.OLS(y, X).fit()\nprint(model3.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nquiet input id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\ngen \tdummy = 1 if group == 1\nreplace dummy = 0 if group == 2\nreg earnings private dummy if id<=5 \n```\n:::\n\n\n\n\n## Regression {.smaller background=\"#dfe3f7\"}\n\nThe previous regression assumes that students 1 to 3 are different that students 4 and 5. \n\nWe will find many instances like that in empirical research. E.g., industry. \n\n. . .\n\nThe private school coefficient, in this case 10,000, implies a private-public earnings differential of this value.\n\n. . .\n\n::: callout-important\nThe Y above is used in monetary values.\n\nUsing a logged y, ln(Y) or ln(earnings), allows estimates to be interpreted as a percent change.\n\nFor instance if $\\beta=0.05$, it means that the earnings differential is 5% for those studying in private schools (conditional on the controls included in the model). \n:::\n\n\n\n\n\n\n\n\n\n\n\n# OVB again {.smaller background=\"#f5caae\"}\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n\n\nRegression is a way to make other things equal (ceteris paribus), but equality  is generated only for variables included in the model as controls on the right-hand sided of the model.\n\nFailure to include enough controls of the right controls still leave us with selection bias.\n\nThe regression version of the selection bias generated by the inadequate controls is called **Omitted Variable Bias (OVB)**. \n\nThe inclusion of a control that should not be included is called \"**Bad Controls**\" problem.\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n**How could we calculate the OVB in this example?**\n\n\n$$earnings_i = 70.000 + 20.000\\times Private_i  \\epsilon $$\n\n$$earnings_i = 40.000 + 10.000 \\times Private_i + 60.000 \\times Group+ \\epsilon$$ \n\n\n- $\\beta$ (1st regression) - $\\beta$ (second regression).\n- The OVB here is 20.000 - 10.000 = 10.000.\n- Meaning that the $\\beta$ (1st regression) is 10.000 higher than what it should be.\n\n\n\n\n\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n**How could we calculate the OVB in this example?**\n\n\nWe could calculate the bias by estimating:\n\n$$Private=\\alpha + \\beta_{omitted} \\times Group + \\epsilon$$\n\nThen,\n\n$$\\beta_{omitted} \\times \\beta_{missing} = 0.1667 * 60.000 = 10.000$$\n\nThe OVB is 10.000, meaning that the first model (the one with the omitted variable) estimates a Beta that is 10.000 higher than it should be. \n\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nmodel4 <- lm(private ~ dummy , data = subset(data,id<=5))\nsummary(model4)\nmatrix2<- summary(model4)$coefficients\nsum(0.1667 * 60000 )\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nsubset_data = data[data['id'] <= 5]\nmodel4 = sm.OLS(subset_data['private'], sm.add_constant(subset_data[['dummy']])).fit()\nprint(model4.summary())\nbias = 0.1667 * 60000\nprint(bias)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nquiet input id earnings str7 school private group\n1 110000 \"private\" 1 1\n2 100000 \"private\" 1 1\n3 110000 \"public\" 0 1\n4 60000 \"private\" 1 2\n5 30000 \"public\" 0 2\n6 115000 \"private\" 1 3\n7 75000 \"private\" 1 3\n8 90000 \"public\" 0 4\n9 60000 \"public\" 0 4\nend\ngen \tdummy = 1 if group == 1\nreplace dummy = 0 if group == 2\nreg private dummy if id<=5\ndi .1666667 *  60000 \n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n**So what?**\n\n- Anticipating the effect of the omitted variable on the non-omitted variable can tell you the sign of the bias.\n\n- Then you can know if the bias is attenuating or increasing the effect you are investigating.\n\n- If attenuating, the problem is smaller than if it is increasing\n\n\n\n\n\n\n\n\n\n\n\n\n## OVB again {.smaller background=\"#f5caae\"}\n\n**Regressions**\n\n-   The previous examples show that we can run **regressions and find correlations** ...\n\n-   ... And we can run regressions and find **causal effects**.\n\n-   But we need to control for all relevant variables, otherwise we have the *OVB problem*.\n\n-   Should you not look careful to your data, you'd miss the inclusion of the variable `group`.\n\n-   The results show that you may estimate a spurious coefficient twice the size of the \"true\" coefficient.\n\n\n\n\n\n\n\n# Bad Controls Problem {.smaller background=\"#dff2c7\"}\n\n\n## Bad Controls Problem {.smaller background=\"#dff2c7\"}\n\n**Bad controls** are variables that are **also outcome of the treatment** being studied.\n\nA **Bad control** could very well be a **dependent variable** of the treatment as well. \n\n**Good controls** are variables that **you can think as being fixed** at the time of the treatment. \n\n. . .\n\nLet's return to the model.\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\beta_2 Group+ \\epsilon \\;,\\; if\\; i <=5$$ \n\n\nAssuming you also have the occupation of the students at the time of earnings. Should you include `occupation` in the model?\n\n\n$$earnings_i = \\alpha + \\beta_1 Private_i + \\beta_2 Group + \\beta_3 Occupation + \\epsilon \\;,\\; if\\; i <=5$$ \n\nReasoning: \"*We should use occupation as control because it would be wise to look at the effect of education on earnings only for those within an occupation*\".\n\nWhat is the problem with this reasoning?\n\n\n\n\n\n\n\n\n## Bad Controls Problem {.smaller background=\"#dff2c7\"}\n\nThe problem is that studying in private would increase the chances of getting a white-collar occupation, i.e., *private education (treatment) affects the occupation (bad control)*.\n\nIn this case, should you include occupation as control, the coefficient of interest no longer has a causal interpretation.\n\n\n. . .\n\n**This is a very common problem in empirical research**.\n\nIt is not hard to come up with stories of why a control is a bad control.\n\n\n\n\n\n\n# Randomization {.smaller background=\"#ff9c6b\"}\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n**Now I want to discuss the idea of randomization**\n\nSuppose you have developed a treatment (e.g., a program) that you believe will increase the 'motivation' of employees of a factory.\n\nYou have 100 employees to use in an experiment to test your claim that the treatment will increase motivation.\n\n. . .\n\n- You randomly allocate 50 employees to receive the treatment. The other 50 are part of the control group.\n\n. . .\n\n- You treat all employees in the same manner, except for the treatment.\n\n\n\n. . .\n\nUsing the data available, this is the **difference in motivation between the treatment and control groups (next slide):**\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(dplyr)\ndata  <- read_excel(\"files/part_3_data.xlsx\", range = \"A1:C101\")\n# Box plot control vs treatment groups\nggplot(data, aes(y=motivation, fill=group)) +   \n  geom_boxplot()+\n  theme(plot.title = element_text(color=\"black\", size=30, face=\"bold\"),\n        panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n        axis.text.y = element_text(face=\"bold\", color=\"black\", size = 18),\n        axis.text.x = element_blank(),\n        legend.title = element_blank(),\n        legend.key.size = unit(3, \"cm\"))\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Read data from Excel file\ndata = pd.read_excel(\"files/part_3_data.xlsx\")\n\n# Create a box plot of control vs treatment groups using seaborn\nplt.figure(figsize=(7, 5))\nsns.set(style='whitegrid')\nsns.boxplot(x='group', y='motivation', data=data, palette='Set2')\nplt.title(\"Box Plot of Control vs Treatment Groups\", fontsize=18)\nplt.xlabel(\"Group\", fontsize=14)\nplt.ylabel(\"Motivation\", fontsize=14)\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\n\nimport excel \"files/part_3_data.xlsx\", cellrange(A1:C101) firstrow clear\ngraph box motivation , over(group) box(1, color(black)) \tytitle(\"Motivation\")  \n\nquietly graph export \"files/graph3_5.svg\", replace\n```       \n\n![](files/graph3_5.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\nThe calculated means are below. And they are statistically different.\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\ndata  <- read_excel(\"files/part_3_data.xlsx\", range = \"A1:C101\")\ntapply(data$motivation, data$group, summary)\nt.test(motivation ~ group, data = data)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\ndata = pd.read_excel(\"files/part_3_data.xlsx\")\ngroup_summary = data.groupby('group')['motivation'].describe()\nprint(group_summary)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nimport excel \"files/part_3_data.xlsx\", cellrange(A1:C101) firstrow clear\nbys group  : sum motivation\nestpost ttest motivation , by(group)\n```\n:::\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n**Is there evidence that the program has increased motivation?**\n\n. . .\n\n- well, if you randomly split a group of 100 people into two groups of 50, you certainly wouldn't get the same mean motivation in both groups even if you treated them exactly alike. \n\n- Maybe the difference that we see is just such a difference?\n\n**How can we test this hypothesis?**\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n**Solution**: \n\n- Suppose the treatment had no effect, and the employees developed their motivation  independently of the treatment. \n\n- What is the chance that the 50 employees randomly assigned to the treatment group would have an average at least 1.47 (22.27 - 20.80)  points higher than the average motivation of the employees randomly assigned to the control group?\n\n\n\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n**Steps**\n\n1) Randomly split the 100 employees that we observed in this experiment into two groups of 50.\n\n2) Note the difference in the mean motivation  between the two groups.\n\n3) Repeat 1 and 2 a total of 10,000 times.\n\n4) Note the proportion of times the difference is at least 1.47 (22.27 - 20.80).\n\n\n\n\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary libraries\ndata <- read_excel(\"files/part_3_data.xlsx\", range = \"A1:C101\")\ncomb <- 10000\ndf <- data.frame(matrix(ncol = 2, nrow = comb))\ncolnames(df) <- c(\"order\" ,\"diff\")\n# Create the loop for randomization:\nfor (i in seq(from = 1, to = comb)) {\n  set.seed(i)                               \n  data$temp <- runif(100, min = 0, max = 1)  # Creating 100 random numbers 0 to 1\n  data <- data[order(data$temp),]            # Sorting data by the random numbers generated in the previous row\n  data$rank <- rank(data$temp)               # Ranking by the random numbers\n# The row below defines the treatment group based on the random numbers generated. This is where we guarantee randomization\ndata$status_rank <- case_when(data$rank <= 50 ~ \"Control_rand\", data$rank > 50 ~ \"Treated_rand\")\n# Calculate the new means of the new groups. Need to transpose data.\nmeans <- t(as.data.frame(tapply(data$motivation, data$status_rank, mean)))\n# Moving the new means to df. Each row is the difference of means\ndf[i, 1] <- i\ndf[i, 2] <- means[1, 2] - means[1, 1]\nrm(means) # Deleting value\ndata = subset(data, select = -c(temp, rank, status_rank)) # Deleting variables\n}\n# Calculate a suitable binwidth for the histogram\nbinwidth <- (max(df$diff) - min(df$diff)) / sqrt(length(df$diff))\n# Create a histogram of the differences with the calculated binwidth\nggplot(df, aes(x = diff)) +\n  geom_histogram(binwidth = binwidth, fill = \"blue\", color = \"black\") +\n  labs(title = \"Distribution of Differences\", x = \"Difference\", y = \"Frequency\")\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: false\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nimport excel \"files/part_3_data.xlsx\", cellrange(A1:C101) firstrow clear\nset seed 472195 \t\t\nsort group\t\t \nset obs 10000 \t\t\t\t \negen fin_order = seq() \nsort fin_order \t\t\t\t \nsummarize \t\t\t\t\t\ngen av_diff=.\n\nlocal i = 1\nwhile `i'<=10000 {\n\n\tsort fin_order\n\tgen rand_num`i' = uniform() if !missing(motivation)\n\tegen ordering`i' = rank(rand_num`i')\n\tsort ordering`i'\n\n\tgen group`i' = \"\"\n\treplace group`i' = \"T\" if ordering <= 50\n\treplace group`i' = \"C\" if ordering > 50 & ordering<=100\n\t\n\tqui summ motivation if group`i'==\"T\"\n\tscalar avT = `r(mean)'\n\tqui summ motivation if group`i'==\"C\"\n\tscalar avC = `r(mean)'\n\t\n\tsort fin_order\n\treplace av_diff = avT-avC in `i'\n\t\n\tdrop rand_num`i' ordering`i' group`i'\n\tlocal i = `i' + 1\n}\nhistogram av_diff, frequency kdensity  \ngraph export \"files/graph3_6.png\" , replace\n\n```       \n\n![](files/graph3_6.png){ width=800px height=450px }\n:::\n\n\n\n\n\n\n## Randomization {.smaller background=\"#ff9c6b\"}\n\nThe mean difference was as far from 0 as 1.5 for only a few out of the 10,000 random divisions of the data into two groups of 50.\n\n- Thus, **the difference between the mean motivation would almost always be less than the observed difference of 1.47 (22.27 - 20.80) if the treatment had no effect.**\n\n- It seems reasonable to believe that the treatment caused the difference in motivation.\n\n\n\n\n\n\n\n\n\n# Measurement Error problem  {.smaller background=\"#f2e9b6\"}\n\n\n## Measurement Error problem  {.smaller background=\"#f2e9b6\"}\n\nThe measurement error problem has a similar statistical structure to the omitted variable bias (OVB).\n\n- \"Classical\" random measurement error for the $y$ will inflate standard errors but will not lead to biased coefficients. \n\n    - $y^{*} = y + \\sigma_{1}$\n    - If you estimante $y^{*} = f(x)$, you have $y + \\sigma_{1} = x + \\epsilon$ \n    - $y = x + u$ \n        - where $u = \\epsilon - \\sigma_{1}$ \n\n\n\n\n## Measurement Error problem  {.smaller background=\"#f2e9b6\"}\n\n- \"Classical‚Äù random measurement error in x‚Äôs will bias coefficient estimates toward zero.\n\n- $x^*=x+\\sigma_2$\n\n- Imagine that $x^*$ is a bunch of noise. It would not explain anything. Thus, your results are biased toward zero.\n\n\n\n\n\n## Measurement Error problem  {.smaller background=\"#f2e9b6\"}\n\nA example using one of the Wooldridge's datasets.\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(jtools)\ndata <- read.dta(\"files/CEOSAL1.dta\")\nset.seed(2)\ndata$salary_noise <- data$salary + runif(length((data$salary)), min=-100, max= 100)\ndata$roe_noise <- data$roe + runif(length((data$roe)), min=-100, max= 100)\n# OLS model \nmodel1 <- lm(data$salary ~ data$roe)\nmodel2 <- lm(data$salary ~ data$roe_noise)\nmodel3 <- lm(data$salary_noise ~ data$roe)\n#summary(model1)\n#summary(model2)\n#summary(model3)\nexport_summs(model1, model2, model3, digits = 3 , model.names = c(\"Roe\", \"Roe (X) with noise\", \"Salary (y) with noise\") )\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.iolib.summary2 import summary_col\n\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\nnp.random.seed(2)\n# Add noise to the 'salary' and 'roe' columns\ndata['salary_noise'] = data['salary'] + np.random.uniform(-100, 100, len(data))\ndata['roe_noise'] = data['roe'] + np.random.uniform(-100, 100, len(data))\n# OLS model\nmodel1 = smf.ols(formula='salary ~ roe', data=data).fit()\nmodel2 = smf.ols(formula='salary ~ roe_noise', data=data).fit()\nmodel3 = smf.ols(formula='salary_noise ~ roe', data=data).fit()\n# Create a summary table for all regressions\nresults = summary_col([model1, model2, model3], \n                      model_names=['Reg 1', 'Reg 2', 'Reg 3'],\n                      stars=True,\n                      float_format='%0.2f')\n# Print the summary table\nprint(results)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.dta\", clear\nset seed 2\ngen salary_noise = salary + runiform() * 200 - 100\ngen roe_noise = roe + runiform() * 200 - 100\neststo: qui reg salary roe\neststo: qui reg salary roe_noise\neststo: qui reg salary_noise roe\nesttab\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## üôã‚Äç‚ôÇÔ∏è **Any Questions?**{ .smaller  background=\"#fdf6e3\"}\n\n::: columns\n::: {.column width=\"50%\"}\n### Thank You!\n\n![](figs/qa2.png){width=\"110%\" style=\"box-shadow: none;\"}\n\n:::\n\n::: {.column width=\"50%\"}\n<div style=\"text-align:right;\">\n  <img src=\"figs/avatar.jpg\" width=\"120px\" style=\"border-radius:50%; box-shadow:0 4px 12px rgba(0,0,0,.25);\" />\n</div>\n\n### **Henrique C. Martins**\n\n- üåê [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)  \n- üíº [LinkedIn](https://www.linkedin.com/in/henriquecastror/)  \n- üß† [Google Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)  \n- üìÑ [Lattes CV](http://lattes.cnpq.br/6076997472159785)  \n- üè† [Personal Website](https://henriquemartins.net/)  \n:::\n:::\n\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","css":["logo.css"],"output-file":"part_3.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.4.549","auto-stretch":true,"html":{"css":"webex.css","include-after-body":"webex.js"},"editor":"visual","title":"Empirical Methods in Finance","subtitle":"Part 3","author":"Henrique C. Martins","title-slide-attributes":{"data-background-color":"#b1cafa"},"include-after":["<script type=\"text/javascript\">\n  Reveal.on('ready', event => {\n    if (event.indexh === 0) {\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n  });\n  Reveal.addEventListener('slidechanged', (event) => {\n    if (event.indexh === 0) {\n      Reveal.configure({ slideNumber: null });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n    if (event.indexh === 1) {\n      Reveal.configure({ slideNumber: 'c' });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n    }\n  });\n</script>\n"],"slideNumber":true,"theme":"simple","chalkboard":true,"previewLinks":"auto","logo":"figs/background8.png","footer":"**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  ","multiplex":true,"scrollable":true}}},"projectFormats":["html"]}