{"title":"Econometria Aplicada a Finanças","markdown":{"yaml":{"title":"Econometria Aplicada a Finanças","subtitle":"Part 6","author":"Henrique C. Martins","format":{"revealjs":{"slide-number":true,"theme":"simple","chalkboard":true,"preview-links":"auto","logo":"figs/background2.png","css":"styles.css","footer":"<https://eaesp.fgv.br/>","multiplex":true}}},"headingText":"library(reticulate)","containsRefs":false,"markdown":"\n\n```{r setup}\n#| include: false\n#| warning: false\n\n# use_python(\"C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe\")\nlibrary(reticulate)\nlibrary(Statamarkdown)\n#reticulate::py_install(\"matplotlib\")\n#reticulate::py_install(\"seaborn\")\n#reticulate::py_install(\"pyfinance\")\n#reticulate::py_install(\"xlrd\")\n#reticulate::py_install(\"quandl\")\n#reticulate::py_install(\"linearmodels\")\n```\n\n\n\n\n\n\n\n\n\n\n# Multivariable models {.smaller background=\"#bdc7c9\"}\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\nIt is quite uncommon that you will have a model with only one independent variable.\n\nThe most frequent type of model in research is multivariate.\n\n\n$$y_i = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + . . . + \\beta_k x_k+ \\mu$$\n\n. . . \n\nThe estimation of $\\alpha$ refers to the predicted value of Y when all X's are zero (it might not make much sense if the variables cannot assume the value of zero). \n\n\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\nUsually, we think of $\\beta_k$ as having partial effects interpretations.\n\n- Meaning that we think $\\beta$ as the change in Y ($\\Delta y$) given a change in x ($\\Delta x_1$), ceteribus paribus \n\n    - i.e., holding all other changes as zero ($\\Delta x_2 = \\Delta x_3 = . . . = \\Delta x_k = 0$)\n\n- Thus, the \"effect\" or the \"association\" is $\\beta_1$, holding all else constant.\n\n\n\n\n. . . \n\nWe can predict the value of $y$ just like before.\n\n$$\\hat{y_i} = \\hat{\\alpha} + \\hat{\\beta_1} x_1 + \\hat{\\beta_2} x_2 + \\hat{\\beta_3} x_3 + . . . + \\hat{\\beta_k} x_k $$\n\n\n\n\n\n\n\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\nLike before, we will need some assumptions.\n\n$$E(\\mu | x_1,x_2, ... , x_k) = 0$$\n\nImplying no correlation between $\\mu$ and the X's.\n\n\n\n\n\n\n\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(sandwich)\nlibrary(foreign) \nlibrary(stargazer)\ndata <- read.dta(\"files/CEOSAL1.DTA\")\nmodel1 <- lm(salary ~ roe , data = data)\nmodel2 <- lm(salary ~ roe + lsales, data = data)\nstargazer(model1, model2 ,title = \"Regression Results\", type = \"text\")\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.iolib.summary2 import summary_col\n\ndata = pd.read_stata(\"files/CEOSAL1.DTA\")\nmodel1 = sm.OLS.from_formula(\"salary ~ roe\", data=data).fit()\nmodel2 = sm.OLS.from_formula(\"salary ~ roe + lsales\", data=data).fit()\nsummary = summary_col([model1, model2], stars=True)\nprint(summary)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\neststo: qui reg salary roe \neststo: qui reg salary roe lsales\nesttab\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\n**Goodness-of-fit**\n\nAs defined before\n\n$$R^2 = \\frac{SSE}{SST} = 1-\\frac{SSR}{SST}$$\n\nOne consequence of measuring $R^2$ this way is that it never goes down when you include more variables. \n\n- It is intuitive, if you are including more variables, you are taking stuff from the residual, increasing $R^2$.\n\n. . .\n\nIf you have multivariate models, that could be a problem, especially if you want to compare the $R^2$ of different models.\n\nWe often use:\n\n$$Adj\\;R^2 = 1-(1-R^2)\\frac{(n-1)}{n-k-1}$$\n\nWhere n is the number of observations and k is the number of independent variables, excluding the constant.\n\nAdj-R^2 can go up, but it can actually go down as well. \n\n\n\n\n# Multicollinearity {.smaller background=\"#7e96d6\"}\n\n## Multicollinearity {.smaller background=\"#7e96d6\"}\n\nWhen control variables show high correlation, the model may present **Multicollinearity**.\n\n- Highly collinear variables can inflate SEs\n\n- But, multicollinearity does not create a bias.\n\n. . .\n\nConsider a model such as:\n\n$$y_i= \\alpha + \\beta_1x1+\\beta_2x2+\\beta_3x3+ \\mu$$\n\n$x_2$ and $x3$ might be highly collinear. \n\n- This makes $var(\\beta_2)$ and  $var(\\beta_3)$ increase. \n\n- But that changes nothing on  $var(\\beta_1)$\n\n\n\n## Multicollinearity {.smaller background=\"#7e96d6\"}\n\nAt the end of the day, **multicollinearity is a non-problem**. It is very rare that I need to test it in my own research!\n\n\n- The tip is to not include controls that are too collinear with the variable of interest (i.e., the treatment).\n\n- Of course, if you need them for stablish causality, you need to include them, ignoring multicollinearity\n\n\n\n\n# Scaling {.smaller background=\"#f0e299\" }\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\n**Multiplying or dividing by a constant does not change your inference.**\n\nLet's say you multiply the Y by 1.000. What will change?\n\n. . . \n\nIn the example from before:\n\n- $\\alpha=963.19$\n- $\\beta=18.50$\n\nIf you multiply the Y (earnings) by 1.000, the new coefficients will be:\n\n- $\\alpha=963,190$\n- $\\beta=18,500$\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nScaling y by a constant c just causes all the estimates to be scaled by the same constant\n\n\n$$y=\\alpha + \\beta x + \\mu$$\n\n$$c.y = c.\\alpha + c.\\beta x + c.\\mu$$\n\n- new alpha: $c.\\alpha$\n- new beta: $c.\\beta$\n\n\n\n\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\n**The scaling has no effect on the relationship between X and Y.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(sandwich)\nlibrary(foreign) \nlibrary(lmtest)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\ndata$salary = data$salary * 1000\nmodel <- lm(salary ~ roe, data = data)\nsummary(model)\n\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\n\nmydata = pd.read_stata(\"files/CEOSAL1.DTA\")\narray1 = np.array([1000])\nmydata['salary'] = np.multiply(mydata['salary'], array1)\nX = sm.add_constant(mydata['roe'])  # Adding a constant (intercept) term\ny = mydata['salary'] \nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nreplace salary = salary * 1000\nqui reg salary roe \nesttab\n```  \n\n:::\n\n\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nWhat if, instead, we multiply the x (ROE) by a constant 1000.\n\n$$y=\\alpha + \\beta x + \\mu$$\n\n$$y = \\alpha + \\frac{\\beta}{1.000} (1.000 x)+ \\mu$$\n\n\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\n**Only $\\beta$ changes.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(sandwich)\nlibrary(foreign) \nlibrary(lmtest)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\ndata$roe = data$roe * 1000\nmodel <- lm(salary ~ roe, data = data)\nsummary(model)\n\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\n\nmydata = pd.read_stata(\"files/CEOSAL1.DTA\")\narray1 = np.array([1000])\nmydata['roe'] = np.multiply(mydata['roe'], array1)\nX = sm.add_constant(mydata['roe'])  # Adding a constant (intercept) term\ny = mydata['salary'] \nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nreplace roe = roe * 1000\nqui reg salary roe \nesttab\n```  \n\n:::\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nScaling is useful when we estimate very large or very small coefficients.\n\n- 0.000000185\n- 185000000.00\n\nSuch coefficients are hard to read.\n\n**Scaling affect the $\\beta$ and S.E. but do not affect the t-stat.**\n\n\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nTo write a better story in your article in terms of  magnitudes, it could be helpful to scale the variables by their sample standard deviation.\n\n- Let's say that $\\sigma_x$ and  $\\sigma_y$ are the s.d. of x and y, respectively. \n\n- Let's say that you divide X by $\\sigma_x$ ($k=\\frac{1}{\\sigma_x}$) and y by $\\sigma_y$ ($c=\\frac{1}{\\sigma_y}$).\n\n- Now, **units of x and y are standard deviations.**\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nYou would have:\n\n- $\\alpha$ scaled by ($c=\\frac{1}{\\sigma_x}$)\n\n- $\\beta$ scaled by ($\\frac{c=\\frac{1}{\\sigma_x}}{k=\\frac{1}{\\sigma_y}}$)\n\n$$c  y = c \\alpha + \\frac{c \\beta}{k} (k x)+ c \\mu$$\n\n\n$$\\frac{1}{\\sigma_y}  y = \\frac{1}{\\sigma_y} \\alpha + \\frac{\\sigma_x}{\\sigma_y} \\beta (\\frac{x}{\\sigma_x} )+ \\frac{1}{\\sigma_y} \\mu$$\n\n. . .\n\n\nSo, if you estimate a $\\beta$ of 0.2, it means that a 1 s.d. increase in x leads to a 0.2 s.d. increase in y.\n\n\n\n\n\n\n\n\n\n## Functional form of relationships {.smaller background=\"#abc8f7\"}\n\nIn many cases, you want to use the logarithm of a variable. This changes the interpretation of the coefficients you estimate.\n\n. . . \n\n\n- **log-log regression**: both Y and X are in log values $ln(Y) = \\alpha + \\beta \\times ln(X) + \\epsilon$. The interpretation of $\\beta$ in this case is: **one percent increase of $x$** leads to **$\\beta$ percent increase in $y$**.   \n\n. . . \n\n- **log-level regression**: both Y and X are in log values $ln(Y) = \\alpha + \\beta \\times X + \\epsilon$. The interpretation of $\\beta$ in this case is: **one unit  increase of $x$** leads to **$\\beta$ percent increase in $y$**.   \n\n\n. . . \n\n- **level-log regression**: both Y and X are in log values $Y = \\alpha + \\beta \\times ln(X) + \\epsilon$. The interpretation of $\\beta$ in this case is: **one percent increase of $x$** leads to **$\\frac{\\beta}{100}$ units increase in $y$**.   \n\n\n**Important note:** the misspecification of x’s is similar to the omitted variable bias (OVB).\n\n\n\n\n\n\n\n\n\n# Winsorization {.smaller background=\"#89faa7\"}\n\n\n## Winsorization {.smaller background=\"#89faa7\"}\n\nIn real research, one very common problem is when you have outliers.\n\nOutliers are observations very far from the mean. For instance, companies that have 800% of leverage ($\\frac{Debt}{TA}$). Clearly, situations like this are typing errors in the original dataset. And this is more common that one should expect.\n\nResearchers avoid excluding such variables. We only exclude when it is totally necessary.\n\nTo avoid using these weird values, we winsorize.\n\nUsually, 1% at both tails.\n\n\n\n\n\n## Winsorization {.smaller background=\"#89faa7\"}\n\n**Look at the following dispersion graphs.** Something weird?\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(ggplot2)\nlibrary(foreign) \nmydata <- read.dta(\"files/CEOSAL1.DTA\")\noptions(repr.plot.width=6, repr.plot.height=4) \nggplot(mydata, aes(x = roe, y = salary)) +\n  geom_point() +\n  labs(title = \"Salary vs. ROE\", x = \"ROE\", y = \"Salary\") +\n  theme_minimal()\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nmydata = pd.read_stata(\"files/CEOSAL1.DTA\")\nplt.figure(figsize=(6, 4))  \nsns.scatterplot(x = \"roe\", y = \"salary\", data=mydata)\nsns.despine(trim=True)\nplt.title(\"Salary vs. ROE\")\nplt.xlabel(\"ROE\")\nplt.ylabel(\"Salary\")\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\ntwoway scatter salary roe\nqui graph export \"files/graph6_1.svg\", replace\n```  \n\n![](files/graph6_1.svg) \n\n:::\n\n\n\n\n\n\n\n\n\n\n## Winsorization {.smaller background=\"#89faa7\"}\n\n**Take a look on the extreme values now.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(ggplot2)\nlibrary(foreign) \nlibrary(DescTools)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\ndata$w_salary     <- Winsorize(data$salary   , probs = c(0.05, 0.95) , na.rm = TRUE) \ndata$w_roe     <- Winsorize(data$roe   , probs = c(0.05, 0.95) , na.rm = TRUE) \noptions(repr.plot.width=6, repr.plot.height=4) \nggplot(data, aes(y = w_salary, x = w_roe)) +\n  geom_point() +\n  theme_minimal()\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport numpy as np\nimport pandas as pd\n\nmydata = pd.read_stata(\"files/CEOSAL1.DTA\")\nquantiles = [0.05, 0.95]\nmydata['w_salary'] = mydata['salary'].clip(np.percentile(mydata['salary'], quantiles[0]), np.percentile(mydata['salary'], quantiles[1]))\nmydata['w_roe'] = mydata['roe'].clip(np.percentile(mydata['roe'], quantiles[0]), np.percentile(mydata['roe'], quantiles[1]))\nplt.figure(figsize=(6, 4))  \nplt.scatter(mydata['w_roe'], mydata['w_salary'])\nplt.xlabel('Winsorized ROE')\nplt.ylabel('Winsorized Salary')\nplt.title('Scatter Plot of Winsorized Salary vs. Winsorized ROE')\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nwinsor salary , gen(w_salary) p(0.05)\nwinsor roe , gen(w_roe) p(0.05)\ntwoway scatter w_salary w_roe\nqui graph export \"files/graph6_2.svg\", replace\n```  \n\n![](files/graph6_2.svg) \n\n:::\n\n\n\n\n\n## Winsorization {.smaller background=\"#89faa7\"}\n\n**Finally, take a look at the statistics.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(ggplot2)\nlibrary(foreign) \nlibrary(DescTools)\nlibrary(haven)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\ndata$w_salary     <- Winsorize(data$salary   , probs = c(0.05, 0.95) , na.rm = TRUE) \ndata$w_roe     <- Winsorize(data$roe   , probs = c(0.05, 0.95) , na.rm = TRUE) \nsummary_stats <- summary(data[c(\"salary\", \"w_salary\", \"roe\", \"w_roe\")])\nprint(summary_stats)\n```\n\n### Python\n\n**Python for some reason is no good.**\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\n\ndata = pd.read_stata(\"files/CEOSAL1.DTA\")\nquantiles = [0.005, 0.095]\ndata['w_salary'] = data['salary'].clip(np.percentile(data['salary'], quantiles[0]), np.percentile(data['salary'], quantiles[1]))\ndata['w_roe'] = data['roe'].clip(np.percentile(data['roe'], quantiles[0]), np.percentile(data['roe'], quantiles[1]))\nsummary_stats = data[[\"salary\", \"w_salary\", \"roe\", \"w_roe\"]].describe()\nprint(summary_stats)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nwinsor salary , gen(w_salary) p(0.05)\nwinsor roe , gen(w_roe) p(0.05)\nestpost tabstat salary w_salary roe w_roe , s(min, mean, max, sd, count) c(s)\n```  \n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n# Models with binary variables {.smaller background=\"#ed95d5\" }\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\nA binary variable is quite simple to understand: it takes the value of 0 for one group, and 1 for the other.\n\n- 0 for men\n- 1 for women\n\nWe can explore many interesting types of binary variables in most cases of corporate finance. \n\nFor instance, whether the firm is included in \"Novo Mercado\", if the firm has high levels of ESG, etc. \n\n\n\n\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\nThe interpretation is a bit trickier. \n\nLet's think about the example 7.1 of Wooldridge. He estimates the following equation:\n\n$$wage = \\beta_0 + \\delta_1 female + \\beta_1 educ + \\mu$$\n\n- In model (7.1), only two observed factors affect wage: gender and education. \n\n- Because $female = 1$ when the person is female, and $female = 0 $ when the person is male, the parameter $\\delta_1$ has the following interpretation: \n\n\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\n- $\\delta_1$  is the difference in hourly wage between females and males, given the same amount of education (and the same error term u). \n\nThus, the coefficient $\\delta_1$  determines whether there is discrimination against women: \n\n- if $\\delta_1<0$, then, for the same level of other factors, women earn less than men on average.\n\n. . .\n\nIn terms of expectations, if we assume the zero conditional mean assumption E($\\mu$ | female,educ) = 0, then \n\n- $\\delta_1 = E(wage | female = 1, educ) - E(wage | female = 0, educ)$, or\n\n- $\\delta_1 = E(wage | female, educ) - E(wage | male, educ)$ \n\n- The key here is that the level of education is the same in both expectations; the difference, $\\delta_1$ , is due to gender only.\n\n\n\n\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\nThe visual interpretation is as follows. The situation can be depicted graphically as an intercept shift between males and females. The interpretation relies on $\\delta_1$. We can observe that $delta_1 < 0$; this is an argument for existence of a gender gap in wage.\n \n\n![](figs/wooldridge_7_1B.png)\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\n\n![](figs/wooldridge_7_1.png)\n\n\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\n\ndata <- read_dta(\"files/wage1.dta\")\nmodel <- lm(wage ~ female +  educ + exper + tenure , data)\nstargazer(model,title = \"Regression Results\", type = \"text\")\n```\n\n\n\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/wage1.dta\")\nX = data[['female', 'educ', 'exper', 'tenure']]\ny = data['wage']\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/wage1.dta\" , replace\neststo: qui reg wage female  educ exper  tenure\nesttab\n```  \n\n:::\n\n\n\n\n\n\n\n\n# Models with quadratic terms {.smaller background=\"#32a89d\"  }\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\n\nLet’s say you have a variable that should not show a clear linear relationship with another variable.\n\nFor instance, consider **ownership concentration and firm value**.There is a case to be made the relationship between these variable is not linear. \n\n. . .\n\n- In low levels of ownership concentration (let’s say 5% of shares), a small increase in it might lead to an increase in firm value. The argument is that, in such levels, an increase in ownership concentration will lead the shareholder to monitor more the management maximizing the likelihood of value increasing decisions.\n\n. . . \n\n- But consider now the case where the shareholder has 60% or more of the firm’s outstanding shares. If you increase further the concentration it might signals the market that this shareholder is too powerful that might start using the firm to personal benefits (which will not be shared with minorities).\n\n. . .\n\n**If this story is true, the relationship is (inverse) u-shaped**. That is, at first the relationship is positive, then becomes negative.\n\n\n\n\n\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\nTheoretically, I could make an argument for a non-linear relationship between several variables of interest in finance. Let’s say size and leverage. Small firms might not be able to issue too much debt as middle size firms. At the same time, huge firms might not need debt. The empirical relationship might be non-linear.\n\n. . . \n\nAs noted before, misspecifying the functional form of a model can create biases. \n\nBut, in this specific case, the problem seems minor since we have the data to fix it.\n\n\n. . . \n\nThe model is:\n\n$$y_i = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon$$\n\n\n\n\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\n**How to include quadratic terms?** Create the variable and include as control. \n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(stargazer)\n\ndata <- read.dta(\"files/CEOSAL1.dta\")\n\ndata$roe_sq = data$roe * data$roe\nmodel1 <- lm(salary ~ roe, data=data)\nmodel2 <- lm(salary ~ roe + roe_sq, data=data)\n\nstargazer(model1, model2, title = \"Regression Results\", type = \"text\")\n```\n\n\n\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.iolib.summary2 import summary_col\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\ndata['roe_sq'] = data['roe'] ** 2\n# OLS model\nmodel1 = sm.OLS(data['salary'], sm.add_constant(data['roe'])).fit()\nmodel2 = sm.OLS(data['salary'], sm.add_constant(data[['roe', 'roe_sq']])).fit()\nsummary = summary_col([model1, model2], stars=True)\nprint(summary)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\ngen roe_sq = roe * roe\neststo: qui reg salary roe \neststo: qui reg salary roe roe_sq\nesttab\n```  \n\n:::\n\n\n\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\nIn the previous example, the quadratic term is not significant, suggesting the relationship is not quadratic.\n\nAlso, notice that the linear term is also not significant anymore.\n\n\n\n\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\n**Here, the association is non-linear? What does it mean?**\n\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\n\ndata <- read_dta(\"files/wage1.dta\")\nmodel <- lm(lwage ~ female +  educ + exper + expersq + tenure + tenursq, data)\nstargazer(model,title = \"Regression Results\", type = \"text\")\n```\n\n\n\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/wage1.dta\")\nX = data[['female', 'educ', 'exper', 'expersq', 'tenure', 'tenursq']]\ny = data['lwage']\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/wage1.dta\" , replace\neststo: qui reg lwage female  educ exper expersq tenure tenursq\nesttab\n```  \n\n:::\n\n\n\n\n\n\n\n\n# Models with Interactions {.smaller background=\"#e3b174\"}\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\nIn some specific cases, you want to interact variables to test if the interacted effect is significant. \n\nFor instance, you might believe that, using Wooldridge very traditional example 7.4., women that are married are yet more discriminated in the job market than single women. \n\nSo, you may prefer to estimate the following equation to follow your intuition.\n\n$$wage = \\beta_0 + \\beta_1 female + \\beta_2 married + \\beta_3 female.married + \\mu$$\n\nWhere $maried$ is a binary variable marking all married people with 1, and 0 otherwise. $female$ marks 1 to women and 0 otherwise. \n\n\n\n\n\n\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\nIn this setting\n\n- The group of single men is the base case and is represented by $\\beta_0$. That is, both female and married are 0.\n\n- The group of single women is represented by $\\beta_0 + \\beta_1$. That is, female is 1 but married is 0.\n\n- The group of married men is represented by $\\beta_0 + \\beta_2$. That is, female is 0 but married is 1.\n\n- Finally, the group of married women is represented by $\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3$. That is, female and married are 1.\n\n\n\n\n\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\nUsing a random sample taken from the U.S. Current Population Survey for the year 1976, Wooldridge estimates that \n\n- $female<0$ \n\n- $married>0$\n\n- $female.married<0$\n\nThis result makes sense for the 70s. \n\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\n![](figs/wooldridge_7_4.png)\n\n\n\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\n\ndata <- read_dta(\"files/wage1.dta\")\ndata$fem_mar <- data$female * data$married\nmodel <- lm(lwage ~ female + married + fem_mar + educ + exper + expersq + tenure + tenursq, data)\nstargazer(model,title = \"Regression Results\", type = \"text\")\n```\n\n\n\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/wage1.dta\")\ndata['fem_mar'] = data['female'] * data['married']\nX = data[['female', 'married', 'fem_mar', 'educ', 'exper', 'expersq', 'tenure', 'tenursq']]\ny = data['lwage']\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/wage1.dta\" , replace\ngen fem_mar = female * married\neststo: qui reg lwage female married fem_mar educ exper expersq tenure tenursq\nesttab\n```  \n\n:::\n\n\n\n\n\n\n# Linear probability model {.smaller background=\"#7ae7eb\"}\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\nWhen the dependent variable is binary we cannot rely on linear models as those discussed so far. \n\nWe need a **linear probability model**. \n\nIn such models, we are interested in how the probability of the occurrence of an event depends on the values of x. That is, we want to know $P[y=1|x]$.\n\n\n. . .\n\n\nImagine that $y$ is employment status, 0 for unemployed, 1 for employed. \n\nImagine that we are interested in estimating the probability that a person start working after a training program. \n\nFor these types of problem, we need a linear probability model.\n\n$$P[y=1|x] = \\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_kx_k + \\epsilon$$\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\nThe mechanics of estimating these model is similar to before, except that $Y$ is binary.\n\nThe interpretation of coefficients change. That is, **a unit change in $x$ changes the probability of y = 1**. \n\nSo, let's say that $\\beta_1$ is 0.05. It means that changing $x_1$ by one unit will change the probability of $y = 1$ (i.e., getting a job) in 5%, ceteris paribus. \n\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\n\nUsing Wooldridge's example 7.29:\n\n\n![](figs/wooldridge_7_29.png)\n\n\nwhere:\n\n- $inlf$   =1 if in labor force, 1975\n\n\n\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\nThe relationship between the probability of labor force participation and $educ$ is plotted in the figure below. \n\nFixing the other independent variables at 50, 5, 30, 1 and 6, respectively, the predicted probability is negative until education equals 3.84 years. This is odd, since the model is predicting negative probability of employment given a set of specific values. \n\n\n![](figs/wooldridge_7_3.png)\n\n\n\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\n**Another example**\n\nThe model is predicting that *going from 0 to 4 kids less than 6 years old* reduces the probability of working by $4\\times 0.262 = 1.048$, which is impossible since it is higher than 1.\n\n. . .\n\n**The takeaway**\n\nThat is, one important caveat of a linear probability model is that probabilities might falls off of expected empirical values. \n\nIf this is problematic to us, we might need a different solution.\n\n\n\n\n\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\n\ndata <- read_dta(\"files/mroz.dta\")\nlpm <- lm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = data)\nstargazer(lpm,title = \"Regression Results\", type = \"text\")\n```\n\n\n\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\ndata = pd.read_stata(\"files/mroz.dta\")\nformula = \"inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6\"\nmodel = smf.ols(formula, data=data).fit()\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/mroz.dta\" , replace\n\neststo: qui reg inlf nwifeinc educ exper expersq age kidslt6 kidsge6\nesttab\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n# Logit and Probit {.smaller background=\"#ebe0ae\" }\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\nAlthough the linear probability model is simple to estimate and use, it has some limitations as discussed.\n\nIf that problem is important to us, we need a solution that addresses the problem of  negative or higher than 1 probability. \n\nThat is, **we need a binary response model**.\n\n. . .\n\nIn a binary response model, interest relies on the response probability.\n\n$$P(y =1 | x) = P(y=1| x_1,x_2,x_3,...)$$\n\nThat is, we have a group of X variables explaining Y, which is binary. In a LPM, we assume that the response probability is linear in the parameters $\\beta$. \n\n- This is the assumption that created the problem discussed above.\n\n\n\n\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\nWe can change that assumption to a different function. \n\n- A **logit** model assumes a logistic function ($G(Z)=\\frac{exp(z)}{[1+exp(z)]}$)\n- A **probit** model assumes a standard normal cumulative distribution function ($\\int_{-inf}^{+z}\\phi(v)dv$)\n\n. . . \n\nThe adjustment is something as follows.\n\n$$P(y =1 | x) = G(\\beta_0 + \\beta_1 x_1+ \\beta_2 x_2 + \\beta_3 x_3)$$\n\nWhere G is either the logistic (logit) or the normal (probit) function. \n\n\nWe don't need to memorize the functions, but we need to understand the adjustment that assuming a different function makes. \n\nBasically, we will not have predicted negative values anymore because the function adjusts at very low and very high values. \n\n\n\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\n![](figs/wooldridge_17_1.png)\n\n\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\ndata <- read_dta(\"files/mroz.dta\")\nlpm <- lm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data )\nlogit <- glm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data ,family = binomial)\nprobit <- glm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data , family = binomial(link = probit))\nstargazer(lpm , logit, probit,title = \"Regression Results\", type = \"text\")\n```\n\n\n\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.iolib.summary2 import summary_col\n\ndata = pd.read_stata(\"files/mroz.dta\")\nlpm_model = smf.ols(\"inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6\", data=data).fit(disp=False)\nlogit_model = smf.logit(\"inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6\", data=data).fit(disp=False)\nprobit_model = smf.probit(\"inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6\", data=data).fit(disp=False)\nsummary = summary_col([lpm_model, logit_model,probit_model], stars=True)\nprint(summary)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/mroz.dta\" , replace\neststo: qui regress inlf nwifeinc educ exper expersq age kidslt6 kidsge6\neststo: qui logit inlf nwifeinc educ exper expersq age kidslt6 kidsge6\neststo: qui probit inlf nwifeinc educ exper expersq age kidslt6 kidsge6\nesttab\n```  \n\n:::\n\n\n\n\n\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\nImportantly, in a LPM model, the coefficients have similar interpretations as usual. \n\nBut logit and probit models lead to harder to interpret coefficients. \n\nIn fact, often we do not make any interpretation of these coefficients. \n\nInstead, we usually transform them to arrive at an interpretation that is similar to what we have in LPM. \n\nTo make the magnitudes of probit and logit roughly comparable, we can multiply the probit coefficients by 1.6, or we can multiply the logit estimates by .625. \n\nAlso, the probit slope estimates can be divided by 2.5 to make them comparable to the LPM estimates.\n\nAfter these adjustments, the interpretation of the logit and probit outputs are similar to LPM's.  \n\n\n\n\n\n\n# Tobit {.smaller background=\"#ae83f2\" }\n\n\n\n## Tobit {.smaller background=\"#ae83f2\"}\n\nAnother problem in the dependent variable occurs when we have a **limited dependent variable** with a corner solution. \n\nThat is, a variable that ranges from zero to all positive values. \n\nFor instance, hours working. \n\n- Nobody works less than zero hours, but individuals in the population can work many number of positive hours. \n\nWhen we have such type of dependent variable, we need to estimate a **tobit** model.\n\n\n\n\n## Tobit {.smaller background=\"#ae83f2\"}\n\n**Tobit can make a huge difference to the LPM model.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\nlibrary(AER)\ndata <- read_dta(\"files/mroz.dta\")\nlpm <- lm(hours ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = data)\ntobit <- tobit(hours ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = data)\nstargazer(lpm , tobit ,title = \"Regression Results\", type = \"text\")\n```\n\n\n\n### Python\n\n\n### Stata\n\n\n\n:::\n\n\n\n\n\n\n\n# Economic vs statistical significance {.smaller background=\"#a1f7e3\"}\n\n\n## Economic vs statistical significance {.smaller background=\"#a1f7e3\" }\n\n**Economic is not the same as statistical significance!**\n\n- coefficients may be significantly different from 0 from a statistical perspective. \n\n- but their economic significance may be very small. \n\n- Conversely, coefficients might be economically large but with no statistical significance. \n\n\n\n\n\n## Economic vs statistical significance {.smaller background=\"#a1f7e3\" }\n\nThere is considerable variation in this regard in empirical research.\n\n**But we should always try to show economic significance alongside the statistical significance.** \n\n- How large is the variation in $y$ given a unit variation in $x$? \n\n- Try to put that on economic figures. A thousand? A million?\n\n- Perhaps more importantly, are these figures realistic? \n\n  - e.g., How realistic is a predicted increase in salary of 1 Million?\n\n\n\n\n## THANK YOU!\n\n::: columns\n::: {.column width=\"30%\"}\n![](figs/fgv.png){fig-align=\"right\"}\n:::\n\n::: {.column width=\"70%\"}\n**Henrique Castro Martins**\n\n-   [henrique.martins\\@fgv.br](henrique.martins@fgv.br)\n-   <https://eaesp.fgv.br/en/people/henrique-castro-martins>\n-   [henriquemartins.net](https://henriquemartins.net/)\n-   <https://www.linkedin.com/in/henriquecastror/>\n:::\n:::\n","srcMarkdownNoYaml":"\n\n```{r setup}\n#| include: false\n#| warning: false\n\n# library(reticulate)\n# use_python(\"C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe\")\nlibrary(reticulate)\nlibrary(Statamarkdown)\n#reticulate::py_install(\"matplotlib\")\n#reticulate::py_install(\"seaborn\")\n#reticulate::py_install(\"pyfinance\")\n#reticulate::py_install(\"xlrd\")\n#reticulate::py_install(\"quandl\")\n#reticulate::py_install(\"linearmodels\")\n```\n\n\n\n\n\n\n\n\n\n\n# Multivariable models {.smaller background=\"#bdc7c9\"}\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\nIt is quite uncommon that you will have a model with only one independent variable.\n\nThe most frequent type of model in research is multivariate.\n\n\n$$y_i = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + . . . + \\beta_k x_k+ \\mu$$\n\n. . . \n\nThe estimation of $\\alpha$ refers to the predicted value of Y when all X's are zero (it might not make much sense if the variables cannot assume the value of zero). \n\n\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\nUsually, we think of $\\beta_k$ as having partial effects interpretations.\n\n- Meaning that we think $\\beta$ as the change in Y ($\\Delta y$) given a change in x ($\\Delta x_1$), ceteribus paribus \n\n    - i.e., holding all other changes as zero ($\\Delta x_2 = \\Delta x_3 = . . . = \\Delta x_k = 0$)\n\n- Thus, the \"effect\" or the \"association\" is $\\beta_1$, holding all else constant.\n\n\n\n\n. . . \n\nWe can predict the value of $y$ just like before.\n\n$$\\hat{y_i} = \\hat{\\alpha} + \\hat{\\beta_1} x_1 + \\hat{\\beta_2} x_2 + \\hat{\\beta_3} x_3 + . . . + \\hat{\\beta_k} x_k $$\n\n\n\n\n\n\n\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\nLike before, we will need some assumptions.\n\n$$E(\\mu | x_1,x_2, ... , x_k) = 0$$\n\nImplying no correlation between $\\mu$ and the X's.\n\n\n\n\n\n\n\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(sandwich)\nlibrary(foreign) \nlibrary(stargazer)\ndata <- read.dta(\"files/CEOSAL1.DTA\")\nmodel1 <- lm(salary ~ roe , data = data)\nmodel2 <- lm(salary ~ roe + lsales, data = data)\nstargazer(model1, model2 ,title = \"Regression Results\", type = \"text\")\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.iolib.summary2 import summary_col\n\ndata = pd.read_stata(\"files/CEOSAL1.DTA\")\nmodel1 = sm.OLS.from_formula(\"salary ~ roe\", data=data).fit()\nmodel2 = sm.OLS.from_formula(\"salary ~ roe + lsales\", data=data).fit()\nsummary = summary_col([model1, model2], stars=True)\nprint(summary)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\neststo: qui reg salary roe \neststo: qui reg salary roe lsales\nesttab\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Multivariable models {.smaller background=\"#bdc7c9\"}\n\n**Goodness-of-fit**\n\nAs defined before\n\n$$R^2 = \\frac{SSE}{SST} = 1-\\frac{SSR}{SST}$$\n\nOne consequence of measuring $R^2$ this way is that it never goes down when you include more variables. \n\n- It is intuitive, if you are including more variables, you are taking stuff from the residual, increasing $R^2$.\n\n. . .\n\nIf you have multivariate models, that could be a problem, especially if you want to compare the $R^2$ of different models.\n\nWe often use:\n\n$$Adj\\;R^2 = 1-(1-R^2)\\frac{(n-1)}{n-k-1}$$\n\nWhere n is the number of observations and k is the number of independent variables, excluding the constant.\n\nAdj-R^2 can go up, but it can actually go down as well. \n\n\n\n\n# Multicollinearity {.smaller background=\"#7e96d6\"}\n\n## Multicollinearity {.smaller background=\"#7e96d6\"}\n\nWhen control variables show high correlation, the model may present **Multicollinearity**.\n\n- Highly collinear variables can inflate SEs\n\n- But, multicollinearity does not create a bias.\n\n. . .\n\nConsider a model such as:\n\n$$y_i= \\alpha + \\beta_1x1+\\beta_2x2+\\beta_3x3+ \\mu$$\n\n$x_2$ and $x3$ might be highly collinear. \n\n- This makes $var(\\beta_2)$ and  $var(\\beta_3)$ increase. \n\n- But that changes nothing on  $var(\\beta_1)$\n\n\n\n## Multicollinearity {.smaller background=\"#7e96d6\"}\n\nAt the end of the day, **multicollinearity is a non-problem**. It is very rare that I need to test it in my own research!\n\n\n- The tip is to not include controls that are too collinear with the variable of interest (i.e., the treatment).\n\n- Of course, if you need them for stablish causality, you need to include them, ignoring multicollinearity\n\n\n\n\n# Scaling {.smaller background=\"#f0e299\" }\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\n**Multiplying or dividing by a constant does not change your inference.**\n\nLet's say you multiply the Y by 1.000. What will change?\n\n. . . \n\nIn the example from before:\n\n- $\\alpha=963.19$\n- $\\beta=18.50$\n\nIf you multiply the Y (earnings) by 1.000, the new coefficients will be:\n\n- $\\alpha=963,190$\n- $\\beta=18,500$\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nScaling y by a constant c just causes all the estimates to be scaled by the same constant\n\n\n$$y=\\alpha + \\beta x + \\mu$$\n\n$$c.y = c.\\alpha + c.\\beta x + c.\\mu$$\n\n- new alpha: $c.\\alpha$\n- new beta: $c.\\beta$\n\n\n\n\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\n**The scaling has no effect on the relationship between X and Y.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(sandwich)\nlibrary(foreign) \nlibrary(lmtest)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\ndata$salary = data$salary * 1000\nmodel <- lm(salary ~ roe, data = data)\nsummary(model)\n\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\n\nmydata = pd.read_stata(\"files/CEOSAL1.DTA\")\narray1 = np.array([1000])\nmydata['salary'] = np.multiply(mydata['salary'], array1)\nX = sm.add_constant(mydata['roe'])  # Adding a constant (intercept) term\ny = mydata['salary'] \nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nreplace salary = salary * 1000\nqui reg salary roe \nesttab\n```  \n\n:::\n\n\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nWhat if, instead, we multiply the x (ROE) by a constant 1000.\n\n$$y=\\alpha + \\beta x + \\mu$$\n\n$$y = \\alpha + \\frac{\\beta}{1.000} (1.000 x)+ \\mu$$\n\n\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\n**Only $\\beta$ changes.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(sandwich)\nlibrary(foreign) \nlibrary(lmtest)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\ndata$roe = data$roe * 1000\nmodel <- lm(salary ~ roe, data = data)\nsummary(model)\n\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\n\nmydata = pd.read_stata(\"files/CEOSAL1.DTA\")\narray1 = np.array([1000])\nmydata['roe'] = np.multiply(mydata['roe'], array1)\nX = sm.add_constant(mydata['roe'])  # Adding a constant (intercept) term\ny = mydata['salary'] \nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nreplace roe = roe * 1000\nqui reg salary roe \nesttab\n```  \n\n:::\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nScaling is useful when we estimate very large or very small coefficients.\n\n- 0.000000185\n- 185000000.00\n\nSuch coefficients are hard to read.\n\n**Scaling affect the $\\beta$ and S.E. but do not affect the t-stat.**\n\n\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nTo write a better story in your article in terms of  magnitudes, it could be helpful to scale the variables by their sample standard deviation.\n\n- Let's say that $\\sigma_x$ and  $\\sigma_y$ are the s.d. of x and y, respectively. \n\n- Let's say that you divide X by $\\sigma_x$ ($k=\\frac{1}{\\sigma_x}$) and y by $\\sigma_y$ ($c=\\frac{1}{\\sigma_y}$).\n\n- Now, **units of x and y are standard deviations.**\n\n\n\n\n## Scaling {.smaller background=\"#f0e299\"}\n\nYou would have:\n\n- $\\alpha$ scaled by ($c=\\frac{1}{\\sigma_x}$)\n\n- $\\beta$ scaled by ($\\frac{c=\\frac{1}{\\sigma_x}}{k=\\frac{1}{\\sigma_y}}$)\n\n$$c  y = c \\alpha + \\frac{c \\beta}{k} (k x)+ c \\mu$$\n\n\n$$\\frac{1}{\\sigma_y}  y = \\frac{1}{\\sigma_y} \\alpha + \\frac{\\sigma_x}{\\sigma_y} \\beta (\\frac{x}{\\sigma_x} )+ \\frac{1}{\\sigma_y} \\mu$$\n\n. . .\n\n\nSo, if you estimate a $\\beta$ of 0.2, it means that a 1 s.d. increase in x leads to a 0.2 s.d. increase in y.\n\n\n\n\n\n\n\n\n\n## Functional form of relationships {.smaller background=\"#abc8f7\"}\n\nIn many cases, you want to use the logarithm of a variable. This changes the interpretation of the coefficients you estimate.\n\n. . . \n\n\n- **log-log regression**: both Y and X are in log values $ln(Y) = \\alpha + \\beta \\times ln(X) + \\epsilon$. The interpretation of $\\beta$ in this case is: **one percent increase of $x$** leads to **$\\beta$ percent increase in $y$**.   \n\n. . . \n\n- **log-level regression**: both Y and X are in log values $ln(Y) = \\alpha + \\beta \\times X + \\epsilon$. The interpretation of $\\beta$ in this case is: **one unit  increase of $x$** leads to **$\\beta$ percent increase in $y$**.   \n\n\n. . . \n\n- **level-log regression**: both Y and X are in log values $Y = \\alpha + \\beta \\times ln(X) + \\epsilon$. The interpretation of $\\beta$ in this case is: **one percent increase of $x$** leads to **$\\frac{\\beta}{100}$ units increase in $y$**.   \n\n\n**Important note:** the misspecification of x’s is similar to the omitted variable bias (OVB).\n\n\n\n\n\n\n\n\n\n# Winsorization {.smaller background=\"#89faa7\"}\n\n\n## Winsorization {.smaller background=\"#89faa7\"}\n\nIn real research, one very common problem is when you have outliers.\n\nOutliers are observations very far from the mean. For instance, companies that have 800% of leverage ($\\frac{Debt}{TA}$). Clearly, situations like this are typing errors in the original dataset. And this is more common that one should expect.\n\nResearchers avoid excluding such variables. We only exclude when it is totally necessary.\n\nTo avoid using these weird values, we winsorize.\n\nUsually, 1% at both tails.\n\n\n\n\n\n## Winsorization {.smaller background=\"#89faa7\"}\n\n**Look at the following dispersion graphs.** Something weird?\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(ggplot2)\nlibrary(foreign) \nmydata <- read.dta(\"files/CEOSAL1.DTA\")\noptions(repr.plot.width=6, repr.plot.height=4) \nggplot(mydata, aes(x = roe, y = salary)) +\n  geom_point() +\n  labs(title = \"Salary vs. ROE\", x = \"ROE\", y = \"Salary\") +\n  theme_minimal()\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nmydata = pd.read_stata(\"files/CEOSAL1.DTA\")\nplt.figure(figsize=(6, 4))  \nsns.scatterplot(x = \"roe\", y = \"salary\", data=mydata)\nsns.despine(trim=True)\nplt.title(\"Salary vs. ROE\")\nplt.xlabel(\"ROE\")\nplt.ylabel(\"Salary\")\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\ntwoway scatter salary roe\nqui graph export \"files/graph6_1.svg\", replace\n```  \n\n![](files/graph6_1.svg) \n\n:::\n\n\n\n\n\n\n\n\n\n\n## Winsorization {.smaller background=\"#89faa7\"}\n\n**Take a look on the extreme values now.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(ggplot2)\nlibrary(foreign) \nlibrary(DescTools)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\ndata$w_salary     <- Winsorize(data$salary   , probs = c(0.05, 0.95) , na.rm = TRUE) \ndata$w_roe     <- Winsorize(data$roe   , probs = c(0.05, 0.95) , na.rm = TRUE) \noptions(repr.plot.width=6, repr.plot.height=4) \nggplot(data, aes(y = w_salary, x = w_roe)) +\n  geom_point() +\n  theme_minimal()\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport numpy as np\nimport pandas as pd\n\nmydata = pd.read_stata(\"files/CEOSAL1.DTA\")\nquantiles = [0.05, 0.95]\nmydata['w_salary'] = mydata['salary'].clip(np.percentile(mydata['salary'], quantiles[0]), np.percentile(mydata['salary'], quantiles[1]))\nmydata['w_roe'] = mydata['roe'].clip(np.percentile(mydata['roe'], quantiles[0]), np.percentile(mydata['roe'], quantiles[1]))\nplt.figure(figsize=(6, 4))  \nplt.scatter(mydata['w_roe'], mydata['w_salary'])\nplt.xlabel('Winsorized ROE')\nplt.ylabel('Winsorized Salary')\nplt.title('Scatter Plot of Winsorized Salary vs. Winsorized ROE')\nplt.show()\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nwinsor salary , gen(w_salary) p(0.05)\nwinsor roe , gen(w_roe) p(0.05)\ntwoway scatter w_salary w_roe\nqui graph export \"files/graph6_2.svg\", replace\n```  \n\n![](files/graph6_2.svg) \n\n:::\n\n\n\n\n\n## Winsorization {.smaller background=\"#89faa7\"}\n\n**Finally, take a look at the statistics.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(ggplot2)\nlibrary(foreign) \nlibrary(DescTools)\nlibrary(haven)\n\ndata <- read.dta(\"files/CEOSAL1.DTA\")\ndata$w_salary     <- Winsorize(data$salary   , probs = c(0.05, 0.95) , na.rm = TRUE) \ndata$w_roe     <- Winsorize(data$roe   , probs = c(0.05, 0.95) , na.rm = TRUE) \nsummary_stats <- summary(data[c(\"salary\", \"w_salary\", \"roe\", \"w_roe\")])\nprint(summary_stats)\n```\n\n### Python\n\n**Python for some reason is no good.**\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport numpy as np\n\ndata = pd.read_stata(\"files/CEOSAL1.DTA\")\nquantiles = [0.005, 0.095]\ndata['w_salary'] = data['salary'].clip(np.percentile(data['salary'], quantiles[0]), np.percentile(data['salary'], quantiles[1]))\ndata['w_roe'] = data['roe'].clip(np.percentile(data['roe'], quantiles[0]), np.percentile(data['roe'], quantiles[1]))\nsummary_stats = data[[\"salary\", \"w_salary\", \"roe\", \"w_roe\"]].describe()\nprint(summary_stats)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\nwinsor salary , gen(w_salary) p(0.05)\nwinsor roe , gen(w_roe) p(0.05)\nestpost tabstat salary w_salary roe w_roe , s(min, mean, max, sd, count) c(s)\n```  \n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n# Models with binary variables {.smaller background=\"#ed95d5\" }\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\nA binary variable is quite simple to understand: it takes the value of 0 for one group, and 1 for the other.\n\n- 0 for men\n- 1 for women\n\nWe can explore many interesting types of binary variables in most cases of corporate finance. \n\nFor instance, whether the firm is included in \"Novo Mercado\", if the firm has high levels of ESG, etc. \n\n\n\n\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\nThe interpretation is a bit trickier. \n\nLet's think about the example 7.1 of Wooldridge. He estimates the following equation:\n\n$$wage = \\beta_0 + \\delta_1 female + \\beta_1 educ + \\mu$$\n\n- In model (7.1), only two observed factors affect wage: gender and education. \n\n- Because $female = 1$ when the person is female, and $female = 0 $ when the person is male, the parameter $\\delta_1$ has the following interpretation: \n\n\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\n- $\\delta_1$  is the difference in hourly wage between females and males, given the same amount of education (and the same error term u). \n\nThus, the coefficient $\\delta_1$  determines whether there is discrimination against women: \n\n- if $\\delta_1<0$, then, for the same level of other factors, women earn less than men on average.\n\n. . .\n\nIn terms of expectations, if we assume the zero conditional mean assumption E($\\mu$ | female,educ) = 0, then \n\n- $\\delta_1 = E(wage | female = 1, educ) - E(wage | female = 0, educ)$, or\n\n- $\\delta_1 = E(wage | female, educ) - E(wage | male, educ)$ \n\n- The key here is that the level of education is the same in both expectations; the difference, $\\delta_1$ , is due to gender only.\n\n\n\n\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\nThe visual interpretation is as follows. The situation can be depicted graphically as an intercept shift between males and females. The interpretation relies on $\\delta_1$. We can observe that $delta_1 < 0$; this is an argument for existence of a gender gap in wage.\n \n\n![](figs/wooldridge_7_1B.png)\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\n\n![](figs/wooldridge_7_1.png)\n\n\n\n\n\n## Models with binary variables {.smaller background=\"#ed95d5\" }\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\n\ndata <- read_dta(\"files/wage1.dta\")\nmodel <- lm(wage ~ female +  educ + exper + tenure , data)\nstargazer(model,title = \"Regression Results\", type = \"text\")\n```\n\n\n\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/wage1.dta\")\nX = data[['female', 'educ', 'exper', 'tenure']]\ny = data['wage']\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/wage1.dta\" , replace\neststo: qui reg wage female  educ exper  tenure\nesttab\n```  \n\n:::\n\n\n\n\n\n\n\n\n# Models with quadratic terms {.smaller background=\"#32a89d\"  }\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\n\nLet’s say you have a variable that should not show a clear linear relationship with another variable.\n\nFor instance, consider **ownership concentration and firm value**.There is a case to be made the relationship between these variable is not linear. \n\n. . .\n\n- In low levels of ownership concentration (let’s say 5% of shares), a small increase in it might lead to an increase in firm value. The argument is that, in such levels, an increase in ownership concentration will lead the shareholder to monitor more the management maximizing the likelihood of value increasing decisions.\n\n. . . \n\n- But consider now the case where the shareholder has 60% or more of the firm’s outstanding shares. If you increase further the concentration it might signals the market that this shareholder is too powerful that might start using the firm to personal benefits (which will not be shared with minorities).\n\n. . .\n\n**If this story is true, the relationship is (inverse) u-shaped**. That is, at first the relationship is positive, then becomes negative.\n\n\n\n\n\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\nTheoretically, I could make an argument for a non-linear relationship between several variables of interest in finance. Let’s say size and leverage. Small firms might not be able to issue too much debt as middle size firms. At the same time, huge firms might not need debt. The empirical relationship might be non-linear.\n\n. . . \n\nAs noted before, misspecifying the functional form of a model can create biases. \n\nBut, in this specific case, the problem seems minor since we have the data to fix it.\n\n\n. . . \n\nThe model is:\n\n$$y_i = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2 + \\epsilon$$\n\n\n\n\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\n**How to include quadratic terms?** Create the variable and include as control. \n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(stargazer)\n\ndata <- read.dta(\"files/CEOSAL1.dta\")\n\ndata$roe_sq = data$roe * data$roe\nmodel1 <- lm(salary ~ roe, data=data)\nmodel2 <- lm(salary ~ roe + roe_sq, data=data)\n\nstargazer(model1, model2, title = \"Regression Results\", type = \"text\")\n```\n\n\n\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.iolib.summary2 import summary_col\ndata = pd.read_stata(\"files/CEOSAL1.dta\")\ndata['roe_sq'] = data['roe'] ** 2\n# OLS model\nmodel1 = sm.OLS(data['salary'], sm.add_constant(data['roe'])).fit()\nmodel2 = sm.OLS(data['salary'], sm.add_constant(data[['roe', 'roe_sq']])).fit()\nsummary = summary_col([model1, model2], stars=True)\nprint(summary)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CEOSAL1.DTA\" , replace\ngen roe_sq = roe * roe\neststo: qui reg salary roe \neststo: qui reg salary roe roe_sq\nesttab\n```  \n\n:::\n\n\n\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\nIn the previous example, the quadratic term is not significant, suggesting the relationship is not quadratic.\n\nAlso, notice that the linear term is also not significant anymore.\n\n\n\n\n\n## Models with quadratic terms {.smaller background=\"#32a89d\"}\n\n**Here, the association is non-linear? What does it mean?**\n\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\n\ndata <- read_dta(\"files/wage1.dta\")\nmodel <- lm(lwage ~ female +  educ + exper + expersq + tenure + tenursq, data)\nstargazer(model,title = \"Regression Results\", type = \"text\")\n```\n\n\n\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/wage1.dta\")\nX = data[['female', 'educ', 'exper', 'expersq', 'tenure', 'tenursq']]\ny = data['lwage']\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/wage1.dta\" , replace\neststo: qui reg lwage female  educ exper expersq tenure tenursq\nesttab\n```  \n\n:::\n\n\n\n\n\n\n\n\n# Models with Interactions {.smaller background=\"#e3b174\"}\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\nIn some specific cases, you want to interact variables to test if the interacted effect is significant. \n\nFor instance, you might believe that, using Wooldridge very traditional example 7.4., women that are married are yet more discriminated in the job market than single women. \n\nSo, you may prefer to estimate the following equation to follow your intuition.\n\n$$wage = \\beta_0 + \\beta_1 female + \\beta_2 married + \\beta_3 female.married + \\mu$$\n\nWhere $maried$ is a binary variable marking all married people with 1, and 0 otherwise. $female$ marks 1 to women and 0 otherwise. \n\n\n\n\n\n\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\nIn this setting\n\n- The group of single men is the base case and is represented by $\\beta_0$. That is, both female and married are 0.\n\n- The group of single women is represented by $\\beta_0 + \\beta_1$. That is, female is 1 but married is 0.\n\n- The group of married men is represented by $\\beta_0 + \\beta_2$. That is, female is 0 but married is 1.\n\n- Finally, the group of married women is represented by $\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3$. That is, female and married are 1.\n\n\n\n\n\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\nUsing a random sample taken from the U.S. Current Population Survey for the year 1976, Wooldridge estimates that \n\n- $female<0$ \n\n- $married>0$\n\n- $female.married<0$\n\nThis result makes sense for the 70s. \n\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\n![](figs/wooldridge_7_4.png)\n\n\n\n\n## Models with Interactions {.smaller background=\"#e3b174\"}\n\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\n\ndata <- read_dta(\"files/wage1.dta\")\ndata$fem_mar <- data$female * data$married\nmodel <- lm(lwage ~ female + married + fem_mar + educ + exper + expersq + tenure + tenursq, data)\nstargazer(model,title = \"Regression Results\", type = \"text\")\n```\n\n\n\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/wage1.dta\")\ndata['fem_mar'] = data['female'] * data['married']\nX = data[['female', 'married', 'fem_mar', 'educ', 'exper', 'expersq', 'tenure', 'tenursq']]\ny = data['lwage']\nX = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/wage1.dta\" , replace\ngen fem_mar = female * married\neststo: qui reg lwage female married fem_mar educ exper expersq tenure tenursq\nesttab\n```  \n\n:::\n\n\n\n\n\n\n# Linear probability model {.smaller background=\"#7ae7eb\"}\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\nWhen the dependent variable is binary we cannot rely on linear models as those discussed so far. \n\nWe need a **linear probability model**. \n\nIn such models, we are interested in how the probability of the occurrence of an event depends on the values of x. That is, we want to know $P[y=1|x]$.\n\n\n. . .\n\n\nImagine that $y$ is employment status, 0 for unemployed, 1 for employed. \n\nImagine that we are interested in estimating the probability that a person start working after a training program. \n\nFor these types of problem, we need a linear probability model.\n\n$$P[y=1|x] = \\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_kx_k + \\epsilon$$\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\nThe mechanics of estimating these model is similar to before, except that $Y$ is binary.\n\nThe interpretation of coefficients change. That is, **a unit change in $x$ changes the probability of y = 1**. \n\nSo, let's say that $\\beta_1$ is 0.05. It means that changing $x_1$ by one unit will change the probability of $y = 1$ (i.e., getting a job) in 5%, ceteris paribus. \n\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\n\nUsing Wooldridge's example 7.29:\n\n\n![](figs/wooldridge_7_29.png)\n\n\nwhere:\n\n- $inlf$   =1 if in labor force, 1975\n\n\n\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\nThe relationship between the probability of labor force participation and $educ$ is plotted in the figure below. \n\nFixing the other independent variables at 50, 5, 30, 1 and 6, respectively, the predicted probability is negative until education equals 3.84 years. This is odd, since the model is predicting negative probability of employment given a set of specific values. \n\n\n![](figs/wooldridge_7_3.png)\n\n\n\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\n**Another example**\n\nThe model is predicting that *going from 0 to 4 kids less than 6 years old* reduces the probability of working by $4\\times 0.262 = 1.048$, which is impossible since it is higher than 1.\n\n. . .\n\n**The takeaway**\n\nThat is, one important caveat of a linear probability model is that probabilities might falls off of expected empirical values. \n\nIf this is problematic to us, we might need a different solution.\n\n\n\n\n\n\n\n## Linear probability model {.smaller background=\"#7ae7eb\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\n\ndata <- read_dta(\"files/mroz.dta\")\nlpm <- lm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = data)\nstargazer(lpm,title = \"Regression Results\", type = \"text\")\n```\n\n\n\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\ndata = pd.read_stata(\"files/mroz.dta\")\nformula = \"inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6\"\nmodel = smf.ols(formula, data=data).fit()\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/mroz.dta\" , replace\n\neststo: qui reg inlf nwifeinc educ exper expersq age kidslt6 kidsge6\nesttab\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n# Logit and Probit {.smaller background=\"#ebe0ae\" }\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\nAlthough the linear probability model is simple to estimate and use, it has some limitations as discussed.\n\nIf that problem is important to us, we need a solution that addresses the problem of  negative or higher than 1 probability. \n\nThat is, **we need a binary response model**.\n\n. . .\n\nIn a binary response model, interest relies on the response probability.\n\n$$P(y =1 | x) = P(y=1| x_1,x_2,x_3,...)$$\n\nThat is, we have a group of X variables explaining Y, which is binary. In a LPM, we assume that the response probability is linear in the parameters $\\beta$. \n\n- This is the assumption that created the problem discussed above.\n\n\n\n\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\nWe can change that assumption to a different function. \n\n- A **logit** model assumes a logistic function ($G(Z)=\\frac{exp(z)}{[1+exp(z)]}$)\n- A **probit** model assumes a standard normal cumulative distribution function ($\\int_{-inf}^{+z}\\phi(v)dv$)\n\n. . . \n\nThe adjustment is something as follows.\n\n$$P(y =1 | x) = G(\\beta_0 + \\beta_1 x_1+ \\beta_2 x_2 + \\beta_3 x_3)$$\n\nWhere G is either the logistic (logit) or the normal (probit) function. \n\n\nWe don't need to memorize the functions, but we need to understand the adjustment that assuming a different function makes. \n\nBasically, we will not have predicted negative values anymore because the function adjusts at very low and very high values. \n\n\n\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\n![](figs/wooldridge_17_1.png)\n\n\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\ndata <- read_dta(\"files/mroz.dta\")\nlpm <- lm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data )\nlogit <- glm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data ,family = binomial)\nprobit <- glm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data , family = binomial(link = probit))\nstargazer(lpm , logit, probit,title = \"Regression Results\", type = \"text\")\n```\n\n\n\n\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.iolib.summary2 import summary_col\n\ndata = pd.read_stata(\"files/mroz.dta\")\nlpm_model = smf.ols(\"inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6\", data=data).fit(disp=False)\nlogit_model = smf.logit(\"inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6\", data=data).fit(disp=False)\nprobit_model = smf.probit(\"inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6\", data=data).fit(disp=False)\nsummary = summary_col([lpm_model, logit_model,probit_model], stars=True)\nprint(summary)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/mroz.dta\" , replace\neststo: qui regress inlf nwifeinc educ exper expersq age kidslt6 kidsge6\neststo: qui logit inlf nwifeinc educ exper expersq age kidslt6 kidsge6\neststo: qui probit inlf nwifeinc educ exper expersq age kidslt6 kidsge6\nesttab\n```  \n\n:::\n\n\n\n\n\n\n## Logit and Probit {.smaller background=\"#ebe0ae\"}\n\nImportantly, in a LPM model, the coefficients have similar interpretations as usual. \n\nBut logit and probit models lead to harder to interpret coefficients. \n\nIn fact, often we do not make any interpretation of these coefficients. \n\nInstead, we usually transform them to arrive at an interpretation that is similar to what we have in LPM. \n\nTo make the magnitudes of probit and logit roughly comparable, we can multiply the probit coefficients by 1.6, or we can multiply the logit estimates by .625. \n\nAlso, the probit slope estimates can be divided by 2.5 to make them comparable to the LPM estimates.\n\nAfter these adjustments, the interpretation of the logit and probit outputs are similar to LPM's.  \n\n\n\n\n\n\n# Tobit {.smaller background=\"#ae83f2\" }\n\n\n\n## Tobit {.smaller background=\"#ae83f2\"}\n\nAnother problem in the dependent variable occurs when we have a **limited dependent variable** with a corner solution. \n\nThat is, a variable that ranges from zero to all positive values. \n\nFor instance, hours working. \n\n- Nobody works less than zero hours, but individuals in the population can work many number of positive hours. \n\nWhen we have such type of dependent variable, we need to estimate a **tobit** model.\n\n\n\n\n## Tobit {.smaller background=\"#ae83f2\"}\n\n**Tobit can make a huge difference to the LPM model.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign) \nlibrary(stargazer)\nlibrary(haven)\nlibrary(AER)\ndata <- read_dta(\"files/mroz.dta\")\nlpm <- lm(hours ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = data)\ntobit <- tobit(hours ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = data)\nstargazer(lpm , tobit ,title = \"Regression Results\", type = \"text\")\n```\n\n\n\n### Python\n\n\n### Stata\n\n\n\n:::\n\n\n\n\n\n\n\n# Economic vs statistical significance {.smaller background=\"#a1f7e3\"}\n\n\n## Economic vs statistical significance {.smaller background=\"#a1f7e3\" }\n\n**Economic is not the same as statistical significance!**\n\n- coefficients may be significantly different from 0 from a statistical perspective. \n\n- but their economic significance may be very small. \n\n- Conversely, coefficients might be economically large but with no statistical significance. \n\n\n\n\n\n## Economic vs statistical significance {.smaller background=\"#a1f7e3\" }\n\nThere is considerable variation in this regard in empirical research.\n\n**But we should always try to show economic significance alongside the statistical significance.** \n\n- How large is the variation in $y$ given a unit variation in $x$? \n\n- Try to put that on economic figures. A thousand? A million?\n\n- Perhaps more importantly, are these figures realistic? \n\n  - e.g., How realistic is a predicted increase in salary of 1 Million?\n\n\n\n\n## THANK YOU!\n\n::: columns\n::: {.column width=\"30%\"}\n![](figs/fgv.png){fig-align=\"right\"}\n:::\n\n::: {.column width=\"70%\"}\n**Henrique Castro Martins**\n\n-   [henrique.martins\\@fgv.br](henrique.martins@fgv.br)\n-   <https://eaesp.fgv.br/en/people/henrique-castro-martins>\n-   [henriquemartins.net](https://henriquemartins.net/)\n-   <https://www.linkedin.com/in/henriquecastror/>\n:::\n:::\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","css":["styles.css"],"output-file":"part_6.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.4.549","auto-stretch":true,"editor":"visual","title":"Econometria Aplicada a Finanças","subtitle":"Part 6","author":"Henrique C. Martins","slideNumber":true,"theme":"simple","chalkboard":true,"previewLinks":"auto","logo":"figs/background2.png","footer":"<https://eaesp.fgv.br/>","multiplex":true}}},"projectFormats":["html"]}