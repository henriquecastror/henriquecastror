{"title":"Empirical Methods in Finance","markdown":{"yaml":{"title":"Empirical Methods in Finance","subtitle":"Part 5","author":"Henrique C. Martins","format":{"revealjs":{"slide-number":true,"theme":"simple","chalkboard":true,"preview-links":"auto","logo":"figs/background8.png","css":"logo.css","footer":"**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  ","multiplex":true,"scrollable":true}},"title-slide-attributes":{"data-background-color":"#b1cafa"},"include-after":"<script type=\"text/javascript\">\n  Reveal.on('ready', event => {\n    if (event.indexh === 0) {\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n  });\n  Reveal.addEventListener('slidechanged', (event) => {\n    if (event.indexh === 0) {\n      Reveal.configure({ slideNumber: null });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n    if (event.indexh === 1) {\n      Reveal.configure({ slideNumber: 'c' });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n    }\n  });\n</script>\n"},"headingText":"library(reticulate)","containsRefs":false,"markdown":"\n\n\n\n```{r setup}\n#| include: false\n#| warning: false\n\n\n# use_python(\"C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe\")\nlibrary(reticulate)\nlibrary(Statamarkdown)\n#reticulate::py_install(\"matplotlib\")\n#reticulate::py_install(\"seaborn\")\n#reticulate::py_install(\"pyfinance\")\n#reticulate::py_install(\"xlrd\")\n#reticulate::py_install(\"quandl\")\n\n```\n\n\n\n\n\n\n\n# Panel Data {.smaller background=\"#dff5ce\"}\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\nAs explained previously, OVB is a significant source of \"endogeneity\" in empirical research.\n\nOVB is a problem because of the considerable heterogeneity in many empirical settings. \n\n**Many of the omitted variables are unobservable to the researcher.**\n\nPanel data can sometimes offer a partial.\n\n\n\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\nWe start defining the following:\n\n\n$$y_{i,t} = \\alpha + \\beta_1 x_{i,t} + \\epsilon_{i,t}$$\n\nWhere: \n\n  - $i = 1, . . . , N$\n  - $t = 1, . . . , T$\n\n. . . \n\n\nImagine that the residual can be decomposed in: \n\n$$\\epsilon_{i,t} = c_i + \\mu_{i,t}$$\n\nThe term $c_i$ is constant.\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\nThe term $c_i$ is constant.\n\n**It captures the aggregate effect of all of the unobservable, time-invariant explanatory variables for $y_{it}$.**\n\nTo focus attention on the issues specific to panel data, we assume that $e_{it}$ has a zero mean conditional on $x_{it}$ and $c_i$ for all $t$.\n\n. . .\n\nThe most important thing here is whether $x_{it}$ and $c_i$ are correlated.\n\n**Why?**\n\n\n\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\nThe most important thing here is whether $x_{it}$ and $c_i$ are correlated.\n\n\n- If $x_{it}$ and $c_i$ are correlated, then $c_i$  is referred to as a “fixed effect”.\n  \n  - It there is correlation, there is violation of the *Conditional Mean Independence* (CMI) assumption.\n\n    \n- If $x_{it}$ and $c_i$ are not correlated, then $c_i$  is referred to as a “random effect”.\n\n  - Endogeneity is not a concern; however, the computation of standard errors is affected.\n\n\n\n\n\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\n**Why might fixed effects arise?**\n\nFE are any time-invariant unit characteristic that cannot be observed in the data.\n\n- education level,\n- firm's culture,\n- technology,\n- managerial talent,\n- investment opportunities,\n- location (economic development, institutions, etc.),\n- etc.\n\n\n\n\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\n**We say things like (you have to understand that they refer to FE):** \n\n- \"*Time-invariant heterogeneity at the unit-level*\"\n- \"*Unobserved variation that occur at the unit-level that do not vary over time*\"\n\n**Important**: with FE, you are capturing **all** unobserved heterogeneity that do not vary over time.\n\n\n\n\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\nDefinition of *Panel Data*:\n\nYou have multiple observations per unit (individual, firm, etc.)\n\nIn datasets, it is \"one panel below the other\" not \"one panel beside the other\".\n\n. . . \n\n\n**Four main topics in Panel Data:**\n\n1) Pooled cross-sectional\n\n2) Fixed Effect models (including multidimensional FE)\n\n3) Random Effects model\n\n4) First differences\n\n5) Lagged models\n\n\n\n\n\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\nFormal definition\n\n$$y_{i,t} = \\alpha + \\beta_1 x_{i,t} + \\delta FE +  \\epsilon_{i,t}$$\n\n- $E(\\epsilon_{i,t}) = 0$\n\n- $corr(x_{i,t},FE) \\neq 0$\n\n- $corr(FE, \\epsilon_{i,t}) = 0$\n\n- $corr(x_{i,t},epsilon_{i,t}) = 0$, for all t\n\nThe last assumption is called *strict exogeneity assumption* and means that the residual of any t is uncorrelated with x of any t.\n\n*That is, under a strict exogeneity assumption on the explanatory variables, the fixed effects estimator is unbiased: the idiosyncratic error should be uncorrelated with each explanatory variable across all time periods.*\n\n\n. . .\n\n**Remember that if we ignore FE, we have OVB.**\n\n\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\n**Before we continue...**\n\n**Comment #1**\n\n*The standard errors in this framework must be “clustered” by panel unit (e.g., individual) to allow for correlation in the residual for the same person over time. This yields valid inference as long as the number of clusters is “large.\"*\n\n. . . \n\n**Comment #2**\n\n*FE cannot solve reverse causality, it might help you with OVB.*\n\n. . . \n\n**Comment #3**\n\n*Three main types of FE:*\n\n- Pooled\n- Within-transformation (when someone says FE, it is usually this one)\n- Random Effects\n\n\n\n\n\n\n# Pooling Cross-sections  {.smaller background=\"#e0cafc\"}\n\n## Pooling Cross-Sections  {.smaller background=\"#e0cafc\"}\n\nWhen you have two periods of the same unit, but the periods are not consecutive, you have a pooled cross-sectional data.\n\nThis is common in survey data.\n\nIf you use only one period, you might find biased results.\n\n. . .\n\nLet's practice with the dataset CRIME2 from Wooldridge. \n\nThis dataset contains data (many cities) on the crime rate, unemployment rate and many other city-related variables.\n\nThere are two years, 82 and 87 (this is pooled cross-section). \n\n\n\n\n\n## Pooling Cross-Sections  {.smaller background=\"#e0cafc\"}\n\nIf we estimate only using the year 87, we would interpret that unemployment leads to lower crime rate.\n\n::: panel-tabset\n\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(haven) \ndata <- read_dta(\"files/CRIME2.dta\")\ndata1 <- subset(data, year == 87)\nmodel <- lm(crmrte ~ unem, data = data1)\nsummary(model)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\n\ndata = pd.read_stata(\"files/CRIME2.dta\")\ndata1 = data[data['year'] == 87]\nmodel = sm.OLS(data1['crmrte'], sm.add_constant(data1['unem'])).fit()\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CRIME2.dta\" , clear\nreg crmrte une if year ==87\n```  \n\n:::\n\n\n\n\n\n\n\n\n## Pooling Cross-Sections  {.smaller background=\"#e0cafc\"}\n\nWhen we consider a panel, we get the expected positive sign. This is evidence that the previous model suffered from OVB. Still, the coefficient of unem is not significant probably because of time-invariant unobserved heterogeneity in the cities.\n\n::: panel-tabset\n\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(haven) \ndata <- read_dta(\"files/CRIME2.dta\")\nmodel <- lm(crmrte ~ d87+ unem, data = data)\nsummary(model)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\n\ndata = pd.read_stata(\"files/CRIME2.dta\")\nmodel = sm.OLS(data['crmrte'], sm.add_constant(data[['d87','unem']])).fit()\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CRIME2.dta\" , clear\nreg crmrte  d87 une \n```  \n\n:::\n\n\n\n## Pooling Cross-Sections  {.smaller background=\"#e0cafc\"}\n\nThis shows us that we should also control for the year variable. \n\nWe call this, **Year Fixed Effects.**\n\nWe still most likely have OVB due to the unobserved heterogeneity in cities, that is, we still would need to include **cities FE**. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Demeaned variables  {.smaller background=\"#fccad9\"}\n\n## Demeaned variables  {.smaller background=\"#fccad9\"}\n\n**A first way to eliminate the FE is by demeaning the data.**\n\nConsider the following:\n\n$$\\bar{y_i} = \\alpha +\\beta \\bar{x_i} + \\delta FE + \\bar{\\epsilon_i}$$\n\n$$\\frac{1}{T}\\sum{y_{i,t}} = \\alpha +\\beta \\frac{1}{T}\\sum{x_{i,t}} + \\delta FE + \\frac{1}{T}\\sum{\\epsilon_{i,t}}$$\n\n. . .\n\nIf we subtract the mean of each variable, we have:\n\n$$(y_{i,t} - \\bar{y_i}) = \\beta (x_{i,t} - \\bar{x_i}) + (\\epsilon_{i,t} - \\bar{\\epsilon_i})$$\n\nBecause the FE does not vary over time, each value is equal to the mean.\n\nThus, when you demean, you eliminate the FE from the equation. You also eliminate the intercept $\\alpha$.\n\n. . .\n\n**Takeaway**: OLS will estimate unbiased coefficients if you demean the variables.\n\nThis is called **within-transformation** because you are demeaning \"within\" the group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Demeaned variables  {.smaller background=\"#fccad9\"}\n\nLet's use the dataset WAGEPAN to estimate the following equation.\n\n$$Ln(wage)=\\alpha + \\beta_1 exper^2 + \\beta_2 married + \\beta_3 union + \\epsilon$$\n\n\nSome variables in the dataset do not vary over time. These variables cannot be included in this equation. \n\n\n\n\n\n\n\n## Demeaned variables  {.smaller background=\"#fccad9\"}\n\nSee page 495 Wooldridge.\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign)\nlibrary(stargazer)\nlibrary(sandwich)\n\ndata <- read.dta(\"files/WAGEPAN.dta\")\n# Calculate mean by nr for lwage, expersq, married, and union\ndata <- data[order(data$nr), ]  # Sort data by nr for by-group operations\ndata$lwage_mean <- ave(data$lwage, data$nr, FUN = mean)\ndata$expersq_mean <- ave(data$expersq, data$nr, FUN = mean)\ndata$married_mean <- ave(data$married, data$nr, FUN = mean)\ndata$union_mean <- ave(data$union, data$nr, FUN = mean)\n\ndata$lwage_demean <- data$lwage - data$lwage_mean\ndata$expersq_demean <- data$expersq - data$expersq_mean\ndata$married_demean <- data$married - data$married_mean\ndata$union_demean <- data$union - data$union_mean\n\nmodel1 <- lm(lwage ~ educ + black + hisp + exper + expersq + married + union + d81 + d82 + d83 + d84 + d85 + d86 + d87, data = data)\nmodel2 <- lm(lwage_demean ~ expersq_demean + married_demean + union_demean + d81 + d82 + d83 + d84 + d85 + d86 + d87, data = data)\n\nstargazer(model1, model2 ,title = \"Regression Results\", column.labels=c(\"OLS\",\"Demean\"),  type = \"text\")\n\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.iolib.summary2 import summary_col\n\ndata = pd.read_stata(\"files/WAGEPAN.dta\")\n\ndata = data.sort_values(by='nr')  # Sort data by nr for by-group operations\ndata['lwage_mean'] = data.groupby('nr')['lwage'].transform('mean')\ndata['expersq_mean'] = data.groupby('nr')['expersq'].transform('mean')\ndata['married_mean'] = data.groupby('nr')['married'].transform('mean')\ndata['union_mean'] = data.groupby('nr')['union'].transform('mean')\n\ndata['lwage_demean'] = data['lwage'] - data['lwage_mean']\ndata['expersq_demean'] = data['expersq'] - data['expersq_mean']\ndata['married_demean'] = data['married'] - data['married_mean']\ndata['union_demean'] = data['union'] - data['union_mean']\n\nmodel1 = sm.OLS(data['lwage'], sm.add_constant(data[['educ', 'black', 'hisp', 'exper', 'expersq', 'married', 'union', 'd81', 'd82', 'd83', 'd84', 'd85', 'd86', 'd87']])).fit()\nmodel2 = sm.OLS(data['lwage_demean'], sm.add_constant(data[['expersq_demean', 'married_demean', 'union_demean', 'd81', 'd82', 'd83', 'd84', 'd85', 'd86', 'd87']])).fit()\n\n# Display regression results using stargazer\nsummary = summary_col([model1, model2], stars=True)\nprint(summary)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/WAGEPAN.dta\" , clear\n\nbys nr:  egen lwage_mean = mean(lwage) \nbys nr:  egen expersq_mean = mean(expersq) \nbys nr:  egen married_mean = mean(married) \nbys nr:  egen union_mean = mean(union)\n\ngen lwage_demean = lwage - lwage_mean\ngen expersq_demean = expersq - expersq_mean\ngen married_demean = married - married_mean\ngen union_demean = union - union_mean\n\neststo: qui reg lwage        educ black hisp exper expersq       married        union d81 d82 d83 d84 d85 d86 d87\neststo: qui reg lwage_demean expersq_demean married_demean union_demean d81 d82 d83 d84 d85 d86 d87\nesttab , mtitles(\"OLS\" \"Demean\") compress\n\n```  \n\n:::\n\n\n\n\n\n\n# Practical Tips  {.smaller background=\"#fce0cc\"}\n\n\n## Practical Tips  {.smaller background=\"#fce0cc\"}\n\nYou will not need to demean the variables every time you want to estimate a fixed effect models.\n\nThe statistical softwares have packages that do that.\n\nYou only need to know that **Fixed effects model** is a **demeaned model**, i.e., a **within-transformation model**. \n\nBut notice that you will have many different Fixed Effects together:\n\n- Firm Fixed Effects\n- Year Fixed Effects\n- Individual Fixed Effects (if individuals change between firms)\n\n. . . \n\nI am calling a **multidimensional fixed effects design** if you expand the FE to interactions of FE. Most common:\n\n- Year-Industry Fixed Effects.\n- CEO-Firm Fixed Effects.\n\n\n\n\n\n\n\n## Practical Tips  {.smaller background=\"#fce0cc\"}\n\nNotice the number of dummies in the last two columns.\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign)\nlibrary(stargazer)\nlibrary(sandwich)\nlibrary(plm)\n\ndata <- read.dta(\"files/WAGEPAN.dta\")\n# Calculate mean by nr for lwage, expersq, married, and union\ndata <- data[order(data$nr), ]  # Sort data by nr for by-group operations\ndata$lwage_mean <- ave(data$lwage, data$nr, FUN = mean)\ndata$expersq_mean <- ave(data$expersq, data$nr, FUN = mean)\ndata$married_mean <- ave(data$married, data$nr, FUN = mean)\ndata$union_mean <- ave(data$union, data$nr, FUN = mean)\n\ndata$lwage_demean <- data$lwage - data$lwage_mean\ndata$expersq_demean <- data$expersq - data$expersq_mean\ndata$married_demean <- data$married - data$married_mean\ndata$union_demean <- data$union - data$union_mean\n\n# set panel data\npdata <- pdata.frame(data, index = c(\"nr\", \"year\"))\n\n# Random effects regression using plm\nmodel_de <- lm(lwage_demean ~  expersq_demean + married_demean + union_demean +  d81 +d82+ d83+ d84+ d85 +d86 +d87 , data = data)\nmodel_fe <- plm(lwage ~  expersq + married + union + factor(year)              + educ + black + hisp + exper, data = pdata, model = \"within\")\nmodel_du <- lm( lwage ~  expersq + married + union + factor(year) + factor(nr) + educ + black + hisp + exper, data = data)\n\n# Display regression results using stargazer\n#summary(model_de)\n#summary(model_fe)\n#summary(model_du)\nstargazer(model_de, model_fe, model_du ,title = \"Regression Results\",  type = \"text\")\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/WAGEPAN.dta\" , clear\n\nbys nr:  egen lwage_mean = mean(lwage) \nbys nr:  egen expersq_mean = mean(expersq) \nbys nr:  egen married_mean = mean(married) \nbys nr:  egen union_mean = mean(union)\n\ngen lwage_demean = lwage - lwage_mean\ngen expersq_demean = expersq - expersq_mean\ngen married_demean = married - married_mean\ngen union_demean = union - union_mean\n\nxtset nr year \neststo: qui reg lwage_demean expersq_demean married_demean union_demean i.year\neststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , fe\neststo: qui reg lwage expersq married union i.year i.nr  educ black hisp exper \nesttab , mtitles(\"Demean\" \"FE\" \"LSDV\") compress\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n## Practical Tips  {.smaller background=\"#fce0cc\"}\n\nNotice that the parameter $\\delta$ does not have meaning. \n\n$$y_{i,t} = \\alpha + \\beta_1 x_{i,t} + \\delta FE +  \\epsilon_{i,t}$$\n\nIn fact, the previous slides have shown that you will find the same results of a FE model if you include the dummies for the units in the panel (i.e., dummies for the firms or individuals, etc.).\n\nThis is called **least squares dummy variable (LSDV) model**.\n\n- the SE are also identical to the within-transformation model.\n\n- But the R2 of the LSDV will be very high because you are including a lot of \"explanatory variables\".\n\n::: {.callout-note}\nAt the end of the day, you will use the package for the unit's FE (i.e., the firm), and will include the additional FE as dummies, just like a LSDV model.\n:::\n\n\n\n\n## Practical Tips  {.smaller background=\"#fce0cc\"}\n\nWhen you estimate a LSDV, the software will inform an $\\alpha$. \n\nBut this coefficient **has no interpretation whatsoever.** \n\n- it will be FE for the dropped unit of FE. \n\nYou can simply ignore it, you even don't need to include in your final table. \n\nNo problem if you do, just **don't make inferences from it**.\n\n\n\n\n\n## Practical Tips  {.smaller background=\"#fce0cc\"}\n\nA FE model helps a lot, but it only does what it can do.\n\nThat is, FE models do not capture **time-variant unobserved heterogeneity**.\n\n. . .\n\nAlso, if you have constant Xs in your model, you will have to drop them.\n\n- More technically, if there is no within-variation in a X, you cannot include it (the software will drop them).\n\n- For instance, the software will drop $year_{birth}$ below if you include CEO FE.\n\n$$Y_{i,t} = \\alpha + \\beta_1 year_{birth} + CEO \\;FE + ... + \\epsilon_{i,t}$$\n\nIf you attempt to include the CEO FE manually, the software will drop a random CEO FE or the variable $year_{birth}$. If you get a beta for $year_{birth}$ it has no meaning.\n\n\n\n\n\n\n## Practical Tips  {.smaller background=\"#fce0cc\"}\n\nAdding many FE can demand a lot of computational power.\n\nConsider the multidimensional model as follows:\n\n$$Y_{i,t} = \\alpha + \\beta_1 X_{i,t} + Firm \\;FE + Year\\; FE + Year.Industry \\;FE + CEO \\;FE + ... + \\epsilon_{i,t}$$\n\nIt would take a while to estimate in an average computer.\n\n\n\n\n\n\n\n\n# Random Effects  {.smaller background=\"#c6f7ec\"}\n\n## Random Effects  {.smaller background=\"#c6f7ec\"}\n\nRemember that:\n\n$$\\epsilon_{i,t} = c_i + \\mu_{i,t}$$\n\nThe most important thing here is whether $x_{it}$ and $c_i$ are correlated.\n    \n- If they are, you should estimate Fixed Effects\n\n- If $x_{it}$ and $c_i$ are not correlated, then $c_i$  is referred to as a **random effect**.\n\n  - Endogeneity is not a concern; however, the computation of standard errors is affected.\n\nBut, if the $x_{it}$ and $c_i$ are not correlated, there is **no endogeneity concern**. \n\n$c_i$ can be let as part of the $\\epsilon_{i,t}$ without bias in the estimated betas.\n\n\n\n\n\n\n\n## Random Effects  {.smaller background=\"#c6f7ec\"}\n\nAdditionally, the assumption that $x_{it}$ and $c_i$ are not correlated is rather strong and not practical to most applications of corporate finance, economics or public policy.\n\nRE is a model not used often. Cunningham does not even discuss it.\n\n*If the key explanatory variable is constant over time, we cannot use FE to estimate its effect on y.*\n\n*Of course, we can only use RE because we are willing to assume the unobserved effect is uncorrelated with all explanatory variables.*\n\n*Typically, if one uses RE, and as many time-constant controls as possible are included among the explanatory variables (with an FE analysis, it is not necessary to include such controls) RE is preferred to pooled OLS because RE is generally more efficient.*\n\n(Wooldridge, p.496)\n\n\n\n\n\n\n\n\n## Random Effects  {.smaller background=\"#c6f7ec\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary packages\nlibrary(plm)\nlibrary(jtools)\nlibrary(foreign)\ndata <- read.dta(\"files/WAGEPAN.dta\")\npdata <- pdata.frame(data, index = c(\"nr\", \"year\"))\n\npo_model <- lm(lwage ~ expersq + married + union + factor(year) + educ + black + hisp + exper, data = data)\nfe_model <- plm(lwage ~ expersq + married + union + factor(year) + educ + black + hisp + exper, data = pdata, model = \"within\")\nre_model <- plm(lwage ~ expersq + married + union + factor(year) + educ + black + hisp + exper, data = pdata, model = \"random\")\n\nstargazer(po_model, fe_model , re_model ,title = \"Regression Results\", column.labels=c(\"OLS\",\"FE\",\"RE\"),  type = \"text\")\n\n```\n\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/WAGEPAN.dta\" , clear\n\nxtset nr year \neststo: qui reg   lwage expersq married union i.year  educ black hisp exper \neststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , fe\neststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , re\n\nesttab , mtitles(\"OLS\" \"FE\" \"RE\") compress\n\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n# FE vs. RE    {.smaller background=\"#5c97f7\" }\n\n\n## FE vs. RE    {.smaller background=\"#5c97f7\" }\n\n*The idea is that one uses the random effects estimates unless the Hausman test rejects.* \n\n*In practice, a failure to reject means either that the RE and FE estimates are sufficiently close so that it does not matter which is used, or the sampling variation is so large in the FE estimates that one cannot conclude practically significant differences are statistically significant.* (Wooldridge)\n\n\n**If the p-value of the Hausman test is significant then use FE, if not use RE.**\n\n\n\n\n\n## FE vs. RE   {.smaller background=\"#5c97f7\" }\n\n\n::: panel-tabset\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/WAGEPAN.dta\", clear\nxtset nr year\nqui xtreg lwage expersq married union i.year educ black hisp exper, fe\nestimates store fe_model\nqui xtreg lwage expersq married union i.year educ black hisp exper, re\nestimates store re_model\nhausman fe_model re_model\n```  \n\n:::\n\n\n\n\n\n\n\n# First differences   {.smaller background=\"#e3e2b8\"}\n\n## First differences   {.smaller background=\"#e3e2b8\"}\n\nIn most applications, the main reason for collecting panel data is **to allow for the unobserved effect, $c_i$, to be correlated with the explanatory variables**. \n\nFor example, in the crime equation, we want to allow the unmeasured city factors in $c_i$ that affect the crime rate also to be correlated with the unemployment rate. \n\nIt turns out that this is simple to allow: **because $c_i$ is constant over time, we can difference the data across the two years.** \n\nMore precisely, for a cross-sectional observation $i$, write the two years as:\n\n\n$$y_{i,1} = \\beta_0 + \\beta_1 x_{i,1} + c_i + \\mu_{i,1}, t=1$$ \n\n$$y_{i,2} = (\\beta_0 + \\delta_0) + \\beta_1 x_{i,2} + c_i + \\mu_{i,2}, t=2$$ \n\nIf we subtract the second equation from the first, we obtain\n\n$$(y_{i,2} - y_{i,1}) = \\delta_0 + \\beta_1 (x_{i,2} - x_{i,1}) + (\\mu_{i,2}-\\mu_{i,1})$$ \n\n\n$$\\Delta y_{i} = \\delta_0 + \\beta_1 \\Delta x_{i} + \\Delta \\mu_{i}$$ \n\n\n\n\n\n\n\n\n\n## First differences   {.smaller background=\"#e3e2b8\"}\n\n**So, rather than subtracting the group mean of each variable, you  subtract the lagged observation.**\n\nNot hard to see that, when t=2, FE and FD will give identical solutions\n\n. . .\n\n- FE is more efficient if disturbances $\\mu_{i,t}$ have low serial correlation\n\n- FD is more efficient if disturbance $\\mu_{i,t}$ follow a random walk\n\nAt the end of the day, you can estimate both. \n\nEmpirical research usually estimate FD only in specific circumstances, when they are interested in how changes of X affect changes of Y.\n\nThings like stationarity or trends are often not concerns in panel data\n\n- where N is 10 to 20 \n\n\n\n\n\n\n## First differences   {.smaller background=\"#e3e2b8\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary packages\n# Load necessary libraries\nlibrary(plm)\nlibrary(lmtest)\nlibrary(stargazer)\n\ndata <- read.dta(\"files/WAGEPAN.dta\")\npdata <- pdata.frame(data, index = c(\"nr\", \"year\"))\n\nols_model <- lm(lwage ~ expersq + married + union + factor(year) + educ + black + hisp + exper, data = pdata)\nfe_model <- plm(lwage ~ expersq + married + union + educ + black + hisp + exper, data = pdata, model = \"within\")\nre_model <- plm(lwage ~ expersq + married + union + educ + black + hisp + exper, data = pdata, model = \"random\")\nfd_model <- plm(lwage ~ expersq + married + union + educ + black + hisp + exper, data = pdata, model = \"fd\")\n\nstargazer(ols_model, fe_model ,re_model, fd_model,title = \"Regression Results\",   type = \"text\")\n\n```\n\n\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/WAGEPAN.dta\" , clear\n\nxtset nr year \neststo: qui reg   lwage expersq married union i.year  educ black hisp exper \neststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , fe\neststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , re\neststo: qui reg D.lwage D.expersq D.married D.union i.year  D.educ D.black D.hisp D.exper \n\nesttab , mtitles(\"OLS\" \"FE\" \"RE\" \"FD\") compress\n\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n# Lagged independent variables   {.smaller background=\"#e3bfc3\"}\n\n## Lagged independent variables   {.smaller background=\"#e3bfc3\"}\n\nWhen you have a panel data and are concerned with simultaneity between Y and X, you can endeavor in lagging the Xs.\n\n\n$$y_{i,t} = \\beta_0 + \\beta_1 x_{i,t-1} + c_i + \\mu_{i,t}$$ \n\nAs a matter of fact, this is often expected in finance research. \n\n. . . \n\nThere is a limitation, however.\n\nThe usual proxy of corporate finance research is highly autocorrelated. \n \n - e.g., total assets do not vary much throughout  time. \n \nThus, lagging the X often does not make much of a difference. \n \n::: {.callout-tip}\nAlways do it. Otherwise, you will have to explain why you didn't do it.\n:::\n\n\n\n\n\n\n\n# Lagged dependent variables   {.smaller background=\"#d6cbf5\"}\n\n## Lagged dependent variables   {.smaller background=\"#d6cbf5\"}\n\nSometimes you may have something like\n\n$$y_{i,t} = \\beta_0 + \\beta_1 y_{i,t-1}+ \\beta_2 x_{i,t} + c_i + \\mu_{i,t}$$ \n\nThis is called a **Dynamic Panel Model**. It includes $y_{i,t-1}$ as X.\n\n. . .\n\nConsider a FE model.\n\n$$y_{i,t} - \\bar{y_i} = \\beta_0 + \\gamma_1 (y_{i,t-1} - \\bar{y}_{i,t-1}) + \\omega_2 (x_{i,t-1} - \\bar{x_i} )   + (FE_i - \\bar{FE}_i)  + (\\mu_{i,t} - \\bar{\\mu}_i )$$ \n\nThe within transformation removes the time-invariant unobserved heterogeneity from the model. \n\nHowever, it introduces a correlation between the transformed lag $(y_{i,t−1}−\\bar{y}_{i,t-1})$ and the transformed error $(\\mu_{i,t−1}−\\bar{\\mu}_{i,t-1})$ because the average error ($\\bar{\\mu} = \\sum_{i=1}^{T} \\mu_{i,t}$) includes $\\mu_{i,t-1}$, which is also \"included\" in $y_{i,t−1}$ \n\n- $y_{i,t-1} = \\beta_0 + \\beta_1 y_{i,t-2}+ \\beta_2 x_{i,t-1} + c_i + \\mu_{i,t-1}$ \n\n\n\n\n\n\n\n\n## Lagged dependent variables   {.smaller background=\"#d6cbf5\"}\n\nThe bias declines with panel length because $\\epsilon_{i,t−1}$ becomes a smaller component of the average error term as T increases. \n\nIn other words, with higher T the correlation between the lagged dependent variable and the regression errors becomes smaller.\n\n**[Flannery and Hankins (2013)](https://doi.org/10.1016/j.jcorpfin.2012.09.004)** have a good review with applications in corporate finance.\n\nThey conclude that FE is biased when estimating these models.\n\nThey suggest to estimate **Sys-GMM** or **Least Squares Dummy Variable Correction**. We do not discuss these models in the course.\n\n\n\n\n\n\n\n\n\n\n# Selection Bias {.smaller background=\"#e3e2b8\"}\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nBack to the selection bias example of before.\n\n-   Imagine that John and Mary are moving to the north of Canada.\n\n-   John has a history of respiratory disease and decide to buy insurance.\n\n-   Mary does not have a history of respiratory disease and decide not to buy insurance.\n\n\n\n| Default                     | John | Mary |\n|-----------------------------|:-----|-----:|\n| State of insurance          | 1    |    0 |\n| Situation without insurance | `3`  |    5 |\n| Situation with insurance    | 4    |  `5` |\n| Observed                    | 4    |    5 |\n| Effect                      | ?    |    ? |\n\n$$(Y_{1,john} - Y_{0,john}) + (Y_{1,Mary}- Y_{0,Mary}) = 4 - 3 + 5 - 5 = 0.5$$\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nRearranging the terms:\n\n\n$$(Y_{1,john} - Y_{0,Mary})   + (Y_{1,Mary}  - Y_{0,john})  = (4 - 5) + (5 - 3)  = 0.5$$\n$$We\\;see   + We\\;do\\;not\\;see  = (4 - 5) + (5 - 3)  = 0.5$$\n\nThe term $(Y_{1,Mary}  - Y_{0,john}) =  (5 - 3) = 2$ is the **selection bias**.\n\nIt exists because we are comparing two people that should not be compared.\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nSome notation:\n\n$d=1$ for the treated units (treatment group)\n\n$d=0$ for the treated units (control group)\n\n\n. . . \n\n\n$Y_{i}$ = Potential outcome of individual *i*.\n\n$Y_{i,1}$ or  $Y(1)$ = Potential outcome of individual *i*, treatement group.\n\n$Y_{i,0}$ or  $Y(0)$ = Potential outcome of individual *i*, control group.\n\n\n\n\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nSome notation:\n\nThese are the representations of the **causal effect** we often want to estimate.\n\n**Average Treatment Effect:**\n\nATE = $\\frac{1}{N} (E[Y_{i,1}] - E[Y_{i,0}])$\n\n. . . \n\n**Average Treatment Effect on the treated:**\n\nATET = $\\frac{1}{N} (E[Y_{i,1}|D_i=1] - E[Y_{i,0}|D_i=1])$\n\n. . . \n\n**Average Treatment Effect on the untreated:**\n\nATEU = $\\frac{1}{N} (E[Y_{i,1}|D_i=0] - E[Y_{i,0}|D_i=0])$\n\n. . . \n\nOf course, again, we cannot observe both potential outcomes of the same unit *i*.\n\n\n\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nWhen dealing with **causal inference**, we have to find ways to approximate what the hidden potential outcome of the treated units is. \n\nThat is, the challenge in identifying causal effects is that the untreated potential outcomes, $Y_{i,0}$, are never\nobserved for the treated group ($D_i= 1$). The \"second\" term in the following equation:\n\nATET = $\\frac{1}{N} (E[Y_{i,1}|D_i=1] - E[Y_{i,0}|D_i=1])$\n\n\nWe need an empirical design to **\"observe\"** what we do not really observe (i.e., the counterfactual). \n\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nMany options:\n\n- Matching/Balancing\n- Difference-in-differences (DiD)\n- Instrumental variables\n- Regression discontinuity design (RDD)\n- Synthetic control (Synth)\n\n\n\n\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nThe process of finding units that are comparable is called **matching**.\n\n. . .\n\n**Before we continue...**\n\n**We will match on observables. We cannot be on unobservables.**\n\nThus, you may want to write in your article \"selection bias due to observables\".\n\n. . .\n\n**Cunningham:**\n\n*Propensity score matching has not seen as wide adoption among economists as in other nonexperimental methods like regression discontinuity or difference-in-differences. The most common reason given for this is that economists are oftentimes skeptical that CIA can be achieved in any dataset almost as an article of faith. This is because for many applications, economists as a group are usually more concerned about selection on unobservables than they are selection on observables, and as such, they reach for matching methods less often.*\n\nCIA = CMI\n\n\n\n\n\n\n\n\n# Matching  {.smaller background=\"#e0cafc\"}\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\n**Matching** aims to compare the outcomes between observations that have the same values of all control variables, except that one unit is treated and the other is not. \n\n. . .\n\nIn this literature, the control variables used to matched are often called **covariates**.\n\nThat is, for each treated unit, the researcher finds an untreated unit that is similar in all covariates.\n\nThe implication is that the researcher can argue that \"*units are comparable after matching*\". \n\n\n\n\n\n\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\nThe easiest to see is **exact matching**: *it matches observations that have the exact same values*. \n\n- It might be doable if you have only one covariate. \n\n- Naturally, if you have only one covariate, you might still be left with some selection bias.\n\n  - In the previous example, health history is one important covariate that makes John and Mary different. \n  \n  - But what about life style? Nutrition? Etc. \n  \n\nAs the number of covariates grow, you cannot pursue exact matching. That is the job of PSM.\n\n\n\n\n\n\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\n**In exact matching, the causal effect estimator (ATET) is:**\n\n$$ATET = \\frac{1}{N} \\sum (E[Y_{i}] - E[Y_{j(i)}] | D_i=1)$$\n\nWhere $Y_{j(i)}$ is the j-th unit matched to the i-th unit based on the j-th being “closest to” the i-th unit for some  covariate. \n\nFor instance, let’s say that a unit in the treatment group has a covariate with a value of 2 and we find another unit in the control group (exactly one unit) with a covariate value of 2. \n\nThen we will impute the treatment unit’s missing counterfactual with the matched unit’s, and take a difference.\n\n\n\n\n\n\n\n## Matching {.smaller background=\"#e0cafc\"}\n\nConsider the following dataset from Cunningham:\n\n![](figs/scott.png)\n\n\n\n\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\n::: panel-tabset\n### R Averages\n\nAverage ages are very different. The salary of a 24 yrs old person is quite different than the salary of a 32 yrs person.\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(knitr)\nlibrary(kableExtra)\n\nread_data <- function(df)\n{\n  full_path <- paste(\"https://github.com/scunning1975/mixtape/raw/master/\",df, sep = \"\")\n  df <- read_dta(full_path)\n  return(df)\n}\ntraining_example <- read_data(\"training_example.dta\") %>% slice(1:20)\nsummary(training_example$age_treat)\nsummary(training_example$age_control)\n```\n\n\n### R Treated\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(knitr)\nlibrary(kableExtra)\n\nread_data <- function(df)\n{\n  full_path <- paste(\"https://github.com/scunning1975/mixtape/raw/master/\",df, sep = \"\")\n  df <- read_dta(full_path)\n  return(df)\n}\n\ntraining_example <- read_data(\"training_example.dta\") %>% slice(1:20)\n\nggplot(training_example, aes(x=age_treat)) +\n  stat_bin(bins = 10, na.rm = TRUE)\n\n```\n\n### R Control\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(knitr)\nlibrary(kableExtra)\n\nread_data <- function(df)\n{\n  full_path <- paste(\"https://github.com/scunning1975/mixtape/raw/master/\",df, sep = \"\")\n  df <- read_dta(full_path)\n  return(df)\n}\n\ntraining_example <- read_data(\"training_example.dta\") %>% slice(1:20)\n\nggplot(training_example, aes(x=age_control)) +\n  stat_bin(bins = 10, na.rm = TRUE)\n\n```\n\n\n### R Matched\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(knitr)\nlibrary(kableExtra)\n\nread_data <- function(df)\n{\n  full_path <- paste(\"https://github.com/scunning1975/mixtape/raw/master/\",df, sep = \"\")\n  df <- read_dta(full_path)\n  return(df)\n}\n\ntraining_example <- read_data(\"training_example.dta\") %>% slice(1:20)\n\nggplot(training_example, aes(x=age_matched)) +\n  stat_bin(bins = 10, na.rm = TRUE)\n\n```\n\n\n:::\n\n\n\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\nIn this example, you are literally finding the units in the control group that have the same age as the units in the treatment group.\n\nYou are exact matching 1-by-1 in this example.\n\nYou have only one covariate, i.e., age.\n\n\n\n\n\n\n\n\n\n\n\n# Distance Matching  {.smaller background=\"#c6f7ec\"}\n\n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\nThe last example was simple because you could *exact match*.\n\nIf you cannot find one exact match, you need an approximate match. \n\n. . .\n\nIn order to do that, you have to use distance matching.\n\n**Distance matching** minimizes the distance (i.e., how far the covariates are from each other) between the treatment and control groups.\n\n\n\n\n\n\n\n\n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\n**Euclidean distance** = $|X_i-X_j|=\\sqrt{(X_i-X_j)'(X_i-X_j)}=\\sqrt{\\sum_{n=1}^k(X_{n,i}-X_{n,j})^2}$\n\n![](figs/euclidian.png)\n\n\n\n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\n**Normalized Euclidean distance** = $|X_i-X_j|=\\sqrt{(X_i-X_j)'\\hat{V}^{-1}(X_i-X_j)}=\\sqrt{\\sum_{n=1}^k\\frac{(X_{n,i}-X_{n,j})}{\\sigma^2_n}}$\n\nThe problem with this measure of distance is that the distance measure itself depends on the **scale of the variables themselves**. \n\nFor this reason, researchers typically will use some modification of the Euclidean distance, such as the **normalized Euclidean distance**, or they’ll use a wholly different alternative distance. \n\nThe normalized Euclidean distance is a commonly used distance, and what makes it different is that the distance of each variable is scaled by the variable’s variance. \n\n\n \n \n \n \n \n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\n**Mahalanobis  distance** = $|X_i-X_j|=\\sqrt{(X_i-X_j)'\\hat{\\sum_x}^{-1}(X_i-X_j)}$\n\nWhere $\\hat{\\sum_x}$ is the sample covariance matrix of X.\n\n. . . \n\n![](figs/malahanobis_king_nielsen.png)\n\n\n\n\n\n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\nDistance matching only goes so far...\n\n... **the larger the dimensionality, the harder is to use distance matching**.\n\nAs sample size increases, for a given N of covariates, the matching discrepancies tend to zero.\n\nBut, the more covariates, the longer it takes.\n\n. . . \n\nAt the end of the day, it is preferable to have many covariates, but it is makes distance matching harder.\n\n\n\n\n\n\n\n\n# Coarsened Exact Matching (CER)  {.smaller background=\"#fce0cc\"}\n\n## Coarsened Exact Matching (CER)  {.smaller background=\"#fce0cc\"}\n\nIn coarsened exact matching, something only counts as a match if it exactly matches on each matching variable. \n\n**The “coarsened” part comes in because, if you have any continuous variables to match on, you need to “coarsen” them first by putting them into bins, rather than matching on exact values.**\n\nCoarsening means creating bins. Fewer bins makes exact matches more likely. \n\n. . .\n\nCER is not used much in empirical research in finance. It is used more in the big data realm when you have many variables to match. \n\n\n\n\n\n\n\n# Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n**PSM is one way to matching using many covariates.** \n\n**PSM aggregates all covariates into one score (propensity-score), which is the likelihood of receiving the treatment.**\n\nThe idea is to match units that, based on observables, have the same probability (called propensity-score) of being treated. \n\n. . .\n\nThe idea is to estimate a probit (default in stata) or logit model (fist stage):\n\n$$P(D=1|X)$$\n\n**The propensity-score is the predicted probability of a unit being treated given all covariates X**. The p-score is just a single number.\n\n\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nConsiderations in PSM.\n\n1) How many neighbors to match?\n\n- Nearest neighbor, radius or kernel?\n\n2) With or without replacement?\n\n3) With or without common support?\n\n- *Common support*: imposes a common support by dropping treatment observations whose pscore is higher than the maximum or less than the minimum pscore of the controls.\n\n4) It is expected that, after PSM, you show the overlap of propensity-scores.\n\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**The y-axis is the propensity-score**.\n\n![](figs/ani_katchova1.png)\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**Nearest matching:** Find the observation closest to ($min|p_i-p_j|$)\n\n![](figs/ani_katchova3.png)\n\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**Kernel matching:** Each treated observation i is matched with several control observations, with weights inversely proportional to the distance between treated and control observations.\n\n![](figs/ani_katchova2.png)\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**Radius matching**: Each treated observation i is matched with control observations j that fall within a specified radius.\n\n$$|p_i-p_j| <r$$\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**Common support:** Restrict matching only based on the common range of propensity scores.\n\n![](figs/ani_katchova5.png)\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nSeems good overlap, but \"good\" is arbitrary.\n\n![](figs/psm1.png)\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nSeems bad overlap\n\n![](figs/psm2.png)\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nSeems good overlap, but \"good\" is arbitrary.\n\n![](figs/psm_graph1.png)\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nSeems bad overlap\n\n![](figs/psm_graph2.png)\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n![](figs/psm_bias.png)\n\n\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n![](figs/psm_ttest1.png)\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n![](figs/psm_ttest2.png)\n\n\n\n\n\n\n\n\n\n\n# Example  {.smaller background=\"#dff5ce\"}\n\n## Example  {.smaller background=\"#dff5ce\"}\n\nLet's practice with an example. 185 treated units vs 15,992 control units. \n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary packages\n# Load necessary libraries\nlibrary(haven)\nlibrary(psych)\ndata <- read_dta(\"files/cps1re74.dta\")\nsummary_stats <- by(data, data$treat, FUN = function(group) {\n  c(\n    mean = mean(group$age, na.rm = TRUE),\n    variance = var(group$age, na.rm = TRUE),\n    skewness = skew(group$age, na.rm = TRUE),\n    count = length(group$age)\n  )\n})\nsummary_df <- as.data.frame(do.call(rbind, summary_stats))\ncolnames(summary_df) <- c(\"mean\", \"variance\", \"skewness\", \"count\")\nprint(summary_df)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nfrom scipy.stats import skew\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/cps1re74.dta\")\ngrouped_data = data.groupby('treat')['age'].agg(['mean', 'var', lambda x: skew(x, nan_policy='omit'), 'count']).reset_index()\ngrouped_data.columns = ['treat', 'mean', 'variance', 'skewness', 'count']\nprint(grouped_data)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/cps1re74.dta, clear\nqui estpost tabstat age black educ , by(treat) c(s) s(me v sk n) nototal\nesttab . \t,varwidth(20) cells(\"mean(fmt(3)) variance(fmt(3)) skewness(fmt(3)) count(fmt(0))\") noobs nonumber compress \n```  \n\n:::\n\n\n\n## Example  {.smaller background=\"#dff5ce\"}\n\nClearly, the treated group is younger, mainly black, and less educated.\n\nAlso note that the **variance and skewness** of the two subsamples are **different**.\n\nIf we were to use these two subsamples in any econometric analysis **without preprocessing to make them comparable**, we would likely have coefficients biased by **selection bias**.\n\nTherefore, it is important to perform some matching method.\n\nLet's start with Propensity Score Matching (PSM). We will use the simplest matching, that is, without using any additional functions.\n\n\n\n\n\n\n\n\n\n\n## Example  {.smaller background=\"#dff5ce\"}\n\n**Nearest with noreplacement.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# install.packages(\"MatchIt\")\nlibrary(haven)\nlibrary(psych)\nlibrary(MatchIt)\ndata <- read_dta(\"files/cps1re74.dta\")\nmodel <- matchit(treat ~ age + black + educ, data = data, method = \"nearest\")\nsummary(model)\n```\n\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/cps1re74.dta, clear\npsmatch2 treat age black educ , n(1) noreplacement\nsum _weight , d\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n## Example  {.smaller background=\"#dff5ce\"}\n\n**Notice that we are creating weights now**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# install.packages(\"MatchIt\")\nlibrary(haven)\nlibrary(MatchIt)\ndata <- read_dta(\"files/cps1re74.dta\")\nmodel <- matchit(treat ~ age + black + educ, data = data, method = \"exact\")\nsummary(model$weights)\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/cps1re74.dta, clear\nqui psmatch2 treat age black educ , kernel\nsum _weight , d\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n## Example  {.smaller background=\"#dff5ce\"}\n\n**Now, the descriptive statistics are much closer**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(haven)\nlibrary(MatchIt)\n#install.packages(\"e1071\")\nlibrary(e1071)\ndata <- read_dta(\"files/cps1re74.dta\")\nmodel <- matchit(treat ~ age + black + educ, data = data, method = \"exact\")\nmatched_data <- match.data(model)\nsummary_stats <- by(matched_data, matched_data$treat, function(x) {\n  c(mean(x$age), var(x$age), skewness(x$age), length(x$age))\n})\n\nresult_df <- data.frame(\n  Treatment = c(\"Control\", \"Treated\"),\n  Mean_Age = sapply(summary_stats, function(x) x[1]),\n  Variance_Age = sapply(summary_stats, function(x) x[2]),\n  Skewness_Age = sapply(summary_stats, function(x) x[3]),\n  Count = sapply(summary_stats, function(x) x[4])\n)\nprint(result_df)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/cps1re74.dta, clear\nqui psmatch2 treat age black educ , kernel\nqui estpost tabstat age black educ [aweight = _weight], by(treat) c(s) s(me v sk n) nototal\nesttab . \t,varwidth(20) cells(\"mean(fmt(3)) variance(fmt(3)) skewness(fmt(3)) count(fmt(0))\") noobs  nonumber compress \n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Entropy Balancing  {.smaller background=\"#fccad9\"}\n\n## Entropy Balancing  {.smaller background=\"#fccad9\"}\n\n**Here, instead of matching units, we reweight the observations such that the moments of the distributions (mean, variance, skewness) are similar.**\n\n- The ebalance function implements a reweighting scheme. The user starts by choosing the covariates that should be included in the reweighting. \n\n- For each covariate, the user then specifies a set of balance constraints (in Equation 5) to equate the moments of the covariate distribution between the treatment and the reweighted control group. \n\n- The moment constraints may include the mean (first moment), the variance (second moment), and the skewness (third moment).\n\n**The outcome is a vector containing the weights to weight the observations, such that the weighted average, weighted variance, and weighted skewness of the covariates in control group are similar to those in the treatment group**\n\n\n\n\n\n\n\n\n\n\n## Entropy Balancing  {.smaller background=\"#fccad9\"}\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(haven)\n#install.packages(\"ebal\")\nlibrary(ebal)\ndata <- read_dta(\"files/cps1re74.dta\")\ntreatment <-cbind(data$treat)\nvars <-cbind(data$age, data$educ, data$black)\neb <- ebalance(treatment, vars)\n# means in treatment group data\napply(vars[treatment==1,],2,mean)\n# means in reweighted control group data\napply(vars[treatment==0,],2,weighted.mean,w=eb$w)\n# means in raw data control group data\napply(vars[treatment==0,],2,mean)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/cps1re74.dta, clear\nebalance treat age black educ, targets(3)\n```  \n\n:::\n\n\n\n\n\n\n## Entropy Balancing  {.smaller background=\"#fccad9\"}\n\n\n::: panel-tabset\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/cps1re74.dta, clear\nqui ebalance treat age black educ, targets(3)\nqui estpost tabstat age black educ [aweight = _webal], by(treat) c(s) s(me v sk n) nototal\nesttab . \t,varwidth(20) cells(\"mean(fmt(3)) variance(fmt(3)) skewness(fmt(3)) count(fmt(0))\") noobs  nonumber compress \n```  \n\n:::\n\n\n\n\n\n## **THANK YOU!** {background=\"#b1cafa\"}\n\n::: columns\n::: {.column width=\"60%\"}\n**QUESTIONS?**\n\n![](figs/qa2.png){width=\"150%\" heigth=\"150%\"}\n:::\n\n::: {.column width=\"40%\"}\n**Henrique C. Martins**\n\n-   [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)\n-   [Personal Website](https://henriquemartins.net/)\n-   [LinkedIn](https://www.linkedin.com/in/henriquecastror/)\n-   [Lattes](http://lattes.cnpq.br/6076997472159785)\n-   [Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)\\\n:::\n:::\n\n::: footer\n:::\n","srcMarkdownNoYaml":"\n\n\n\n```{r setup}\n#| include: false\n#| warning: false\n\n\n# library(reticulate)\n# use_python(\"C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe\")\nlibrary(reticulate)\nlibrary(Statamarkdown)\n#reticulate::py_install(\"matplotlib\")\n#reticulate::py_install(\"seaborn\")\n#reticulate::py_install(\"pyfinance\")\n#reticulate::py_install(\"xlrd\")\n#reticulate::py_install(\"quandl\")\n\n```\n\n\n\n\n\n\n\n# Panel Data {.smaller background=\"#dff5ce\"}\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\nAs explained previously, OVB is a significant source of \"endogeneity\" in empirical research.\n\nOVB is a problem because of the considerable heterogeneity in many empirical settings. \n\n**Many of the omitted variables are unobservable to the researcher.**\n\nPanel data can sometimes offer a partial.\n\n\n\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\nWe start defining the following:\n\n\n$$y_{i,t} = \\alpha + \\beta_1 x_{i,t} + \\epsilon_{i,t}$$\n\nWhere: \n\n  - $i = 1, . . . , N$\n  - $t = 1, . . . , T$\n\n. . . \n\n\nImagine that the residual can be decomposed in: \n\n$$\\epsilon_{i,t} = c_i + \\mu_{i,t}$$\n\nThe term $c_i$ is constant.\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\nThe term $c_i$ is constant.\n\n**It captures the aggregate effect of all of the unobservable, time-invariant explanatory variables for $y_{it}$.**\n\nTo focus attention on the issues specific to panel data, we assume that $e_{it}$ has a zero mean conditional on $x_{it}$ and $c_i$ for all $t$.\n\n. . .\n\nThe most important thing here is whether $x_{it}$ and $c_i$ are correlated.\n\n**Why?**\n\n\n\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\nThe most important thing here is whether $x_{it}$ and $c_i$ are correlated.\n\n\n- If $x_{it}$ and $c_i$ are correlated, then $c_i$  is referred to as a “fixed effect”.\n  \n  - It there is correlation, there is violation of the *Conditional Mean Independence* (CMI) assumption.\n\n    \n- If $x_{it}$ and $c_i$ are not correlated, then $c_i$  is referred to as a “random effect”.\n\n  - Endogeneity is not a concern; however, the computation of standard errors is affected.\n\n\n\n\n\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\n**Why might fixed effects arise?**\n\nFE are any time-invariant unit characteristic that cannot be observed in the data.\n\n- education level,\n- firm's culture,\n- technology,\n- managerial talent,\n- investment opportunities,\n- location (economic development, institutions, etc.),\n- etc.\n\n\n\n\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\n**We say things like (you have to understand that they refer to FE):** \n\n- \"*Time-invariant heterogeneity at the unit-level*\"\n- \"*Unobserved variation that occur at the unit-level that do not vary over time*\"\n\n**Important**: with FE, you are capturing **all** unobserved heterogeneity that do not vary over time.\n\n\n\n\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\nDefinition of *Panel Data*:\n\nYou have multiple observations per unit (individual, firm, etc.)\n\nIn datasets, it is \"one panel below the other\" not \"one panel beside the other\".\n\n. . . \n\n\n**Four main topics in Panel Data:**\n\n1) Pooled cross-sectional\n\n2) Fixed Effect models (including multidimensional FE)\n\n3) Random Effects model\n\n4) First differences\n\n5) Lagged models\n\n\n\n\n\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\nFormal definition\n\n$$y_{i,t} = \\alpha + \\beta_1 x_{i,t} + \\delta FE +  \\epsilon_{i,t}$$\n\n- $E(\\epsilon_{i,t}) = 0$\n\n- $corr(x_{i,t},FE) \\neq 0$\n\n- $corr(FE, \\epsilon_{i,t}) = 0$\n\n- $corr(x_{i,t},epsilon_{i,t}) = 0$, for all t\n\nThe last assumption is called *strict exogeneity assumption* and means that the residual of any t is uncorrelated with x of any t.\n\n*That is, under a strict exogeneity assumption on the explanatory variables, the fixed effects estimator is unbiased: the idiosyncratic error should be uncorrelated with each explanatory variable across all time periods.*\n\n\n. . .\n\n**Remember that if we ignore FE, we have OVB.**\n\n\n\n\n\n## Panel Data {.smaller background=\"#dff5ce\"}\n\n**Before we continue...**\n\n**Comment #1**\n\n*The standard errors in this framework must be “clustered” by panel unit (e.g., individual) to allow for correlation in the residual for the same person over time. This yields valid inference as long as the number of clusters is “large.\"*\n\n. . . \n\n**Comment #2**\n\n*FE cannot solve reverse causality, it might help you with OVB.*\n\n. . . \n\n**Comment #3**\n\n*Three main types of FE:*\n\n- Pooled\n- Within-transformation (when someone says FE, it is usually this one)\n- Random Effects\n\n\n\n\n\n\n# Pooling Cross-sections  {.smaller background=\"#e0cafc\"}\n\n## Pooling Cross-Sections  {.smaller background=\"#e0cafc\"}\n\nWhen you have two periods of the same unit, but the periods are not consecutive, you have a pooled cross-sectional data.\n\nThis is common in survey data.\n\nIf you use only one period, you might find biased results.\n\n. . .\n\nLet's practice with the dataset CRIME2 from Wooldridge. \n\nThis dataset contains data (many cities) on the crime rate, unemployment rate and many other city-related variables.\n\nThere are two years, 82 and 87 (this is pooled cross-section). \n\n\n\n\n\n## Pooling Cross-Sections  {.smaller background=\"#e0cafc\"}\n\nIf we estimate only using the year 87, we would interpret that unemployment leads to lower crime rate.\n\n::: panel-tabset\n\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(haven) \ndata <- read_dta(\"files/CRIME2.dta\")\ndata1 <- subset(data, year == 87)\nmodel <- lm(crmrte ~ unem, data = data1)\nsummary(model)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\n\ndata = pd.read_stata(\"files/CRIME2.dta\")\ndata1 = data[data['year'] == 87]\nmodel = sm.OLS(data1['crmrte'], sm.add_constant(data1['unem'])).fit()\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CRIME2.dta\" , clear\nreg crmrte une if year ==87\n```  \n\n:::\n\n\n\n\n\n\n\n\n## Pooling Cross-Sections  {.smaller background=\"#e0cafc\"}\n\nWhen we consider a panel, we get the expected positive sign. This is evidence that the previous model suffered from OVB. Still, the coefficient of unem is not significant probably because of time-invariant unobserved heterogeneity in the cities.\n\n::: panel-tabset\n\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(haven) \ndata <- read_dta(\"files/CRIME2.dta\")\nmodel <- lm(crmrte ~ d87+ unem, data = data)\nsummary(model)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\n\ndata = pd.read_stata(\"files/CRIME2.dta\")\nmodel = sm.OLS(data['crmrte'], sm.add_constant(data[['d87','unem']])).fit()\nprint(model.summary())\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/CRIME2.dta\" , clear\nreg crmrte  d87 une \n```  \n\n:::\n\n\n\n## Pooling Cross-Sections  {.smaller background=\"#e0cafc\"}\n\nThis shows us that we should also control for the year variable. \n\nWe call this, **Year Fixed Effects.**\n\nWe still most likely have OVB due to the unobserved heterogeneity in cities, that is, we still would need to include **cities FE**. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Demeaned variables  {.smaller background=\"#fccad9\"}\n\n## Demeaned variables  {.smaller background=\"#fccad9\"}\n\n**A first way to eliminate the FE is by demeaning the data.**\n\nConsider the following:\n\n$$\\bar{y_i} = \\alpha +\\beta \\bar{x_i} + \\delta FE + \\bar{\\epsilon_i}$$\n\n$$\\frac{1}{T}\\sum{y_{i,t}} = \\alpha +\\beta \\frac{1}{T}\\sum{x_{i,t}} + \\delta FE + \\frac{1}{T}\\sum{\\epsilon_{i,t}}$$\n\n. . .\n\nIf we subtract the mean of each variable, we have:\n\n$$(y_{i,t} - \\bar{y_i}) = \\beta (x_{i,t} - \\bar{x_i}) + (\\epsilon_{i,t} - \\bar{\\epsilon_i})$$\n\nBecause the FE does not vary over time, each value is equal to the mean.\n\nThus, when you demean, you eliminate the FE from the equation. You also eliminate the intercept $\\alpha$.\n\n. . .\n\n**Takeaway**: OLS will estimate unbiased coefficients if you demean the variables.\n\nThis is called **within-transformation** because you are demeaning \"within\" the group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Demeaned variables  {.smaller background=\"#fccad9\"}\n\nLet's use the dataset WAGEPAN to estimate the following equation.\n\n$$Ln(wage)=\\alpha + \\beta_1 exper^2 + \\beta_2 married + \\beta_3 union + \\epsilon$$\n\n\nSome variables in the dataset do not vary over time. These variables cannot be included in this equation. \n\n\n\n\n\n\n\n## Demeaned variables  {.smaller background=\"#fccad9\"}\n\nSee page 495 Wooldridge.\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign)\nlibrary(stargazer)\nlibrary(sandwich)\n\ndata <- read.dta(\"files/WAGEPAN.dta\")\n# Calculate mean by nr for lwage, expersq, married, and union\ndata <- data[order(data$nr), ]  # Sort data by nr for by-group operations\ndata$lwage_mean <- ave(data$lwage, data$nr, FUN = mean)\ndata$expersq_mean <- ave(data$expersq, data$nr, FUN = mean)\ndata$married_mean <- ave(data$married, data$nr, FUN = mean)\ndata$union_mean <- ave(data$union, data$nr, FUN = mean)\n\ndata$lwage_demean <- data$lwage - data$lwage_mean\ndata$expersq_demean <- data$expersq - data$expersq_mean\ndata$married_demean <- data$married - data$married_mean\ndata$union_demean <- data$union - data$union_mean\n\nmodel1 <- lm(lwage ~ educ + black + hisp + exper + expersq + married + union + d81 + d82 + d83 + d84 + d85 + d86 + d87, data = data)\nmodel2 <- lm(lwage_demean ~ expersq_demean + married_demean + union_demean + d81 + d82 + d83 + d84 + d85 + d86 + d87, data = data)\n\nstargazer(model1, model2 ,title = \"Regression Results\", column.labels=c(\"OLS\",\"Demean\"),  type = \"text\")\n\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.iolib.summary2 import summary_col\n\ndata = pd.read_stata(\"files/WAGEPAN.dta\")\n\ndata = data.sort_values(by='nr')  # Sort data by nr for by-group operations\ndata['lwage_mean'] = data.groupby('nr')['lwage'].transform('mean')\ndata['expersq_mean'] = data.groupby('nr')['expersq'].transform('mean')\ndata['married_mean'] = data.groupby('nr')['married'].transform('mean')\ndata['union_mean'] = data.groupby('nr')['union'].transform('mean')\n\ndata['lwage_demean'] = data['lwage'] - data['lwage_mean']\ndata['expersq_demean'] = data['expersq'] - data['expersq_mean']\ndata['married_demean'] = data['married'] - data['married_mean']\ndata['union_demean'] = data['union'] - data['union_mean']\n\nmodel1 = sm.OLS(data['lwage'], sm.add_constant(data[['educ', 'black', 'hisp', 'exper', 'expersq', 'married', 'union', 'd81', 'd82', 'd83', 'd84', 'd85', 'd86', 'd87']])).fit()\nmodel2 = sm.OLS(data['lwage_demean'], sm.add_constant(data[['expersq_demean', 'married_demean', 'union_demean', 'd81', 'd82', 'd83', 'd84', 'd85', 'd86', 'd87']])).fit()\n\n# Display regression results using stargazer\nsummary = summary_col([model1, model2], stars=True)\nprint(summary)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/WAGEPAN.dta\" , clear\n\nbys nr:  egen lwage_mean = mean(lwage) \nbys nr:  egen expersq_mean = mean(expersq) \nbys nr:  egen married_mean = mean(married) \nbys nr:  egen union_mean = mean(union)\n\ngen lwage_demean = lwage - lwage_mean\ngen expersq_demean = expersq - expersq_mean\ngen married_demean = married - married_mean\ngen union_demean = union - union_mean\n\neststo: qui reg lwage        educ black hisp exper expersq       married        union d81 d82 d83 d84 d85 d86 d87\neststo: qui reg lwage_demean expersq_demean married_demean union_demean d81 d82 d83 d84 d85 d86 d87\nesttab , mtitles(\"OLS\" \"Demean\") compress\n\n```  \n\n:::\n\n\n\n\n\n\n# Practical Tips  {.smaller background=\"#fce0cc\"}\n\n\n## Practical Tips  {.smaller background=\"#fce0cc\"}\n\nYou will not need to demean the variables every time you want to estimate a fixed effect models.\n\nThe statistical softwares have packages that do that.\n\nYou only need to know that **Fixed effects model** is a **demeaned model**, i.e., a **within-transformation model**. \n\nBut notice that you will have many different Fixed Effects together:\n\n- Firm Fixed Effects\n- Year Fixed Effects\n- Individual Fixed Effects (if individuals change between firms)\n\n. . . \n\nI am calling a **multidimensional fixed effects design** if you expand the FE to interactions of FE. Most common:\n\n- Year-Industry Fixed Effects.\n- CEO-Firm Fixed Effects.\n\n\n\n\n\n\n\n## Practical Tips  {.smaller background=\"#fce0cc\"}\n\nNotice the number of dummies in the last two columns.\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(foreign)\nlibrary(stargazer)\nlibrary(sandwich)\nlibrary(plm)\n\ndata <- read.dta(\"files/WAGEPAN.dta\")\n# Calculate mean by nr for lwage, expersq, married, and union\ndata <- data[order(data$nr), ]  # Sort data by nr for by-group operations\ndata$lwage_mean <- ave(data$lwage, data$nr, FUN = mean)\ndata$expersq_mean <- ave(data$expersq, data$nr, FUN = mean)\ndata$married_mean <- ave(data$married, data$nr, FUN = mean)\ndata$union_mean <- ave(data$union, data$nr, FUN = mean)\n\ndata$lwage_demean <- data$lwage - data$lwage_mean\ndata$expersq_demean <- data$expersq - data$expersq_mean\ndata$married_demean <- data$married - data$married_mean\ndata$union_demean <- data$union - data$union_mean\n\n# set panel data\npdata <- pdata.frame(data, index = c(\"nr\", \"year\"))\n\n# Random effects regression using plm\nmodel_de <- lm(lwage_demean ~  expersq_demean + married_demean + union_demean +  d81 +d82+ d83+ d84+ d85 +d86 +d87 , data = data)\nmodel_fe <- plm(lwage ~  expersq + married + union + factor(year)              + educ + black + hisp + exper, data = pdata, model = \"within\")\nmodel_du <- lm( lwage ~  expersq + married + union + factor(year) + factor(nr) + educ + black + hisp + exper, data = data)\n\n# Display regression results using stargazer\n#summary(model_de)\n#summary(model_fe)\n#summary(model_du)\nstargazer(model_de, model_fe, model_du ,title = \"Regression Results\",  type = \"text\")\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/WAGEPAN.dta\" , clear\n\nbys nr:  egen lwage_mean = mean(lwage) \nbys nr:  egen expersq_mean = mean(expersq) \nbys nr:  egen married_mean = mean(married) \nbys nr:  egen union_mean = mean(union)\n\ngen lwage_demean = lwage - lwage_mean\ngen expersq_demean = expersq - expersq_mean\ngen married_demean = married - married_mean\ngen union_demean = union - union_mean\n\nxtset nr year \neststo: qui reg lwage_demean expersq_demean married_demean union_demean i.year\neststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , fe\neststo: qui reg lwage expersq married union i.year i.nr  educ black hisp exper \nesttab , mtitles(\"Demean\" \"FE\" \"LSDV\") compress\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n## Practical Tips  {.smaller background=\"#fce0cc\"}\n\nNotice that the parameter $\\delta$ does not have meaning. \n\n$$y_{i,t} = \\alpha + \\beta_1 x_{i,t} + \\delta FE +  \\epsilon_{i,t}$$\n\nIn fact, the previous slides have shown that you will find the same results of a FE model if you include the dummies for the units in the panel (i.e., dummies for the firms or individuals, etc.).\n\nThis is called **least squares dummy variable (LSDV) model**.\n\n- the SE are also identical to the within-transformation model.\n\n- But the R2 of the LSDV will be very high because you are including a lot of \"explanatory variables\".\n\n::: {.callout-note}\nAt the end of the day, you will use the package for the unit's FE (i.e., the firm), and will include the additional FE as dummies, just like a LSDV model.\n:::\n\n\n\n\n## Practical Tips  {.smaller background=\"#fce0cc\"}\n\nWhen you estimate a LSDV, the software will inform an $\\alpha$. \n\nBut this coefficient **has no interpretation whatsoever.** \n\n- it will be FE for the dropped unit of FE. \n\nYou can simply ignore it, you even don't need to include in your final table. \n\nNo problem if you do, just **don't make inferences from it**.\n\n\n\n\n\n## Practical Tips  {.smaller background=\"#fce0cc\"}\n\nA FE model helps a lot, but it only does what it can do.\n\nThat is, FE models do not capture **time-variant unobserved heterogeneity**.\n\n. . .\n\nAlso, if you have constant Xs in your model, you will have to drop them.\n\n- More technically, if there is no within-variation in a X, you cannot include it (the software will drop them).\n\n- For instance, the software will drop $year_{birth}$ below if you include CEO FE.\n\n$$Y_{i,t} = \\alpha + \\beta_1 year_{birth} + CEO \\;FE + ... + \\epsilon_{i,t}$$\n\nIf you attempt to include the CEO FE manually, the software will drop a random CEO FE or the variable $year_{birth}$. If you get a beta for $year_{birth}$ it has no meaning.\n\n\n\n\n\n\n## Practical Tips  {.smaller background=\"#fce0cc\"}\n\nAdding many FE can demand a lot of computational power.\n\nConsider the multidimensional model as follows:\n\n$$Y_{i,t} = \\alpha + \\beta_1 X_{i,t} + Firm \\;FE + Year\\; FE + Year.Industry \\;FE + CEO \\;FE + ... + \\epsilon_{i,t}$$\n\nIt would take a while to estimate in an average computer.\n\n\n\n\n\n\n\n\n# Random Effects  {.smaller background=\"#c6f7ec\"}\n\n## Random Effects  {.smaller background=\"#c6f7ec\"}\n\nRemember that:\n\n$$\\epsilon_{i,t} = c_i + \\mu_{i,t}$$\n\nThe most important thing here is whether $x_{it}$ and $c_i$ are correlated.\n    \n- If they are, you should estimate Fixed Effects\n\n- If $x_{it}$ and $c_i$ are not correlated, then $c_i$  is referred to as a **random effect**.\n\n  - Endogeneity is not a concern; however, the computation of standard errors is affected.\n\nBut, if the $x_{it}$ and $c_i$ are not correlated, there is **no endogeneity concern**. \n\n$c_i$ can be let as part of the $\\epsilon_{i,t}$ without bias in the estimated betas.\n\n\n\n\n\n\n\n## Random Effects  {.smaller background=\"#c6f7ec\"}\n\nAdditionally, the assumption that $x_{it}$ and $c_i$ are not correlated is rather strong and not practical to most applications of corporate finance, economics or public policy.\n\nRE is a model not used often. Cunningham does not even discuss it.\n\n*If the key explanatory variable is constant over time, we cannot use FE to estimate its effect on y.*\n\n*Of course, we can only use RE because we are willing to assume the unobserved effect is uncorrelated with all explanatory variables.*\n\n*Typically, if one uses RE, and as many time-constant controls as possible are included among the explanatory variables (with an FE analysis, it is not necessary to include such controls) RE is preferred to pooled OLS because RE is generally more efficient.*\n\n(Wooldridge, p.496)\n\n\n\n\n\n\n\n\n## Random Effects  {.smaller background=\"#c6f7ec\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary packages\nlibrary(plm)\nlibrary(jtools)\nlibrary(foreign)\ndata <- read.dta(\"files/WAGEPAN.dta\")\npdata <- pdata.frame(data, index = c(\"nr\", \"year\"))\n\npo_model <- lm(lwage ~ expersq + married + union + factor(year) + educ + black + hisp + exper, data = data)\nfe_model <- plm(lwage ~ expersq + married + union + factor(year) + educ + black + hisp + exper, data = pdata, model = \"within\")\nre_model <- plm(lwage ~ expersq + married + union + factor(year) + educ + black + hisp + exper, data = pdata, model = \"random\")\n\nstargazer(po_model, fe_model , re_model ,title = \"Regression Results\", column.labels=c(\"OLS\",\"FE\",\"RE\"),  type = \"text\")\n\n```\n\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/WAGEPAN.dta\" , clear\n\nxtset nr year \neststo: qui reg   lwage expersq married union i.year  educ black hisp exper \neststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , fe\neststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , re\n\nesttab , mtitles(\"OLS\" \"FE\" \"RE\") compress\n\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n# FE vs. RE    {.smaller background=\"#5c97f7\" }\n\n\n## FE vs. RE    {.smaller background=\"#5c97f7\" }\n\n*The idea is that one uses the random effects estimates unless the Hausman test rejects.* \n\n*In practice, a failure to reject means either that the RE and FE estimates are sufficiently close so that it does not matter which is used, or the sampling variation is so large in the FE estimates that one cannot conclude practically significant differences are statistically significant.* (Wooldridge)\n\n\n**If the p-value of the Hausman test is significant then use FE, if not use RE.**\n\n\n\n\n\n## FE vs. RE   {.smaller background=\"#5c97f7\" }\n\n\n::: panel-tabset\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/WAGEPAN.dta\", clear\nxtset nr year\nqui xtreg lwage expersq married union i.year educ black hisp exper, fe\nestimates store fe_model\nqui xtreg lwage expersq married union i.year educ black hisp exper, re\nestimates store re_model\nhausman fe_model re_model\n```  \n\n:::\n\n\n\n\n\n\n\n# First differences   {.smaller background=\"#e3e2b8\"}\n\n## First differences   {.smaller background=\"#e3e2b8\"}\n\nIn most applications, the main reason for collecting panel data is **to allow for the unobserved effect, $c_i$, to be correlated with the explanatory variables**. \n\nFor example, in the crime equation, we want to allow the unmeasured city factors in $c_i$ that affect the crime rate also to be correlated with the unemployment rate. \n\nIt turns out that this is simple to allow: **because $c_i$ is constant over time, we can difference the data across the two years.** \n\nMore precisely, for a cross-sectional observation $i$, write the two years as:\n\n\n$$y_{i,1} = \\beta_0 + \\beta_1 x_{i,1} + c_i + \\mu_{i,1}, t=1$$ \n\n$$y_{i,2} = (\\beta_0 + \\delta_0) + \\beta_1 x_{i,2} + c_i + \\mu_{i,2}, t=2$$ \n\nIf we subtract the second equation from the first, we obtain\n\n$$(y_{i,2} - y_{i,1}) = \\delta_0 + \\beta_1 (x_{i,2} - x_{i,1}) + (\\mu_{i,2}-\\mu_{i,1})$$ \n\n\n$$\\Delta y_{i} = \\delta_0 + \\beta_1 \\Delta x_{i} + \\Delta \\mu_{i}$$ \n\n\n\n\n\n\n\n\n\n## First differences   {.smaller background=\"#e3e2b8\"}\n\n**So, rather than subtracting the group mean of each variable, you  subtract the lagged observation.**\n\nNot hard to see that, when t=2, FE and FD will give identical solutions\n\n. . .\n\n- FE is more efficient if disturbances $\\mu_{i,t}$ have low serial correlation\n\n- FD is more efficient if disturbance $\\mu_{i,t}$ follow a random walk\n\nAt the end of the day, you can estimate both. \n\nEmpirical research usually estimate FD only in specific circumstances, when they are interested in how changes of X affect changes of Y.\n\nThings like stationarity or trends are often not concerns in panel data\n\n- where N is 10 to 20 \n\n\n\n\n\n\n## First differences   {.smaller background=\"#e3e2b8\"}\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary packages\n# Load necessary libraries\nlibrary(plm)\nlibrary(lmtest)\nlibrary(stargazer)\n\ndata <- read.dta(\"files/WAGEPAN.dta\")\npdata <- pdata.frame(data, index = c(\"nr\", \"year\"))\n\nols_model <- lm(lwage ~ expersq + married + union + factor(year) + educ + black + hisp + exper, data = pdata)\nfe_model <- plm(lwage ~ expersq + married + union + educ + black + hisp + exper, data = pdata, model = \"within\")\nre_model <- plm(lwage ~ expersq + married + union + educ + black + hisp + exper, data = pdata, model = \"random\")\nfd_model <- plm(lwage ~ expersq + married + union + educ + black + hisp + exper, data = pdata, model = \"fd\")\n\nstargazer(ols_model, fe_model ,re_model, fd_model,title = \"Regression Results\",   type = \"text\")\n\n```\n\n\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse \"files/WAGEPAN.dta\" , clear\n\nxtset nr year \neststo: qui reg   lwage expersq married union i.year  educ black hisp exper \neststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , fe\neststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , re\neststo: qui reg D.lwage D.expersq D.married D.union i.year  D.educ D.black D.hisp D.exper \n\nesttab , mtitles(\"OLS\" \"FE\" \"RE\" \"FD\") compress\n\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n# Lagged independent variables   {.smaller background=\"#e3bfc3\"}\n\n## Lagged independent variables   {.smaller background=\"#e3bfc3\"}\n\nWhen you have a panel data and are concerned with simultaneity between Y and X, you can endeavor in lagging the Xs.\n\n\n$$y_{i,t} = \\beta_0 + \\beta_1 x_{i,t-1} + c_i + \\mu_{i,t}$$ \n\nAs a matter of fact, this is often expected in finance research. \n\n. . . \n\nThere is a limitation, however.\n\nThe usual proxy of corporate finance research is highly autocorrelated. \n \n - e.g., total assets do not vary much throughout  time. \n \nThus, lagging the X often does not make much of a difference. \n \n::: {.callout-tip}\nAlways do it. Otherwise, you will have to explain why you didn't do it.\n:::\n\n\n\n\n\n\n\n# Lagged dependent variables   {.smaller background=\"#d6cbf5\"}\n\n## Lagged dependent variables   {.smaller background=\"#d6cbf5\"}\n\nSometimes you may have something like\n\n$$y_{i,t} = \\beta_0 + \\beta_1 y_{i,t-1}+ \\beta_2 x_{i,t} + c_i + \\mu_{i,t}$$ \n\nThis is called a **Dynamic Panel Model**. It includes $y_{i,t-1}$ as X.\n\n. . .\n\nConsider a FE model.\n\n$$y_{i,t} - \\bar{y_i} = \\beta_0 + \\gamma_1 (y_{i,t-1} - \\bar{y}_{i,t-1}) + \\omega_2 (x_{i,t-1} - \\bar{x_i} )   + (FE_i - \\bar{FE}_i)  + (\\mu_{i,t} - \\bar{\\mu}_i )$$ \n\nThe within transformation removes the time-invariant unobserved heterogeneity from the model. \n\nHowever, it introduces a correlation between the transformed lag $(y_{i,t−1}−\\bar{y}_{i,t-1})$ and the transformed error $(\\mu_{i,t−1}−\\bar{\\mu}_{i,t-1})$ because the average error ($\\bar{\\mu} = \\sum_{i=1}^{T} \\mu_{i,t}$) includes $\\mu_{i,t-1}$, which is also \"included\" in $y_{i,t−1}$ \n\n- $y_{i,t-1} = \\beta_0 + \\beta_1 y_{i,t-2}+ \\beta_2 x_{i,t-1} + c_i + \\mu_{i,t-1}$ \n\n\n\n\n\n\n\n\n## Lagged dependent variables   {.smaller background=\"#d6cbf5\"}\n\nThe bias declines with panel length because $\\epsilon_{i,t−1}$ becomes a smaller component of the average error term as T increases. \n\nIn other words, with higher T the correlation between the lagged dependent variable and the regression errors becomes smaller.\n\n**[Flannery and Hankins (2013)](https://doi.org/10.1016/j.jcorpfin.2012.09.004)** have a good review with applications in corporate finance.\n\nThey conclude that FE is biased when estimating these models.\n\nThey suggest to estimate **Sys-GMM** or **Least Squares Dummy Variable Correction**. We do not discuss these models in the course.\n\n\n\n\n\n\n\n\n\n\n# Selection Bias {.smaller background=\"#e3e2b8\"}\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nBack to the selection bias example of before.\n\n-   Imagine that John and Mary are moving to the north of Canada.\n\n-   John has a history of respiratory disease and decide to buy insurance.\n\n-   Mary does not have a history of respiratory disease and decide not to buy insurance.\n\n\n\n| Default                     | John | Mary |\n|-----------------------------|:-----|-----:|\n| State of insurance          | 1    |    0 |\n| Situation without insurance | `3`  |    5 |\n| Situation with insurance    | 4    |  `5` |\n| Observed                    | 4    |    5 |\n| Effect                      | ?    |    ? |\n\n$$(Y_{1,john} - Y_{0,john}) + (Y_{1,Mary}- Y_{0,Mary}) = 4 - 3 + 5 - 5 = 0.5$$\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nRearranging the terms:\n\n\n$$(Y_{1,john} - Y_{0,Mary})   + (Y_{1,Mary}  - Y_{0,john})  = (4 - 5) + (5 - 3)  = 0.5$$\n$$We\\;see   + We\\;do\\;not\\;see  = (4 - 5) + (5 - 3)  = 0.5$$\n\nThe term $(Y_{1,Mary}  - Y_{0,john}) =  (5 - 3) = 2$ is the **selection bias**.\n\nIt exists because we are comparing two people that should not be compared.\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nSome notation:\n\n$d=1$ for the treated units (treatment group)\n\n$d=0$ for the treated units (control group)\n\n\n. . . \n\n\n$Y_{i}$ = Potential outcome of individual *i*.\n\n$Y_{i,1}$ or  $Y(1)$ = Potential outcome of individual *i*, treatement group.\n\n$Y_{i,0}$ or  $Y(0)$ = Potential outcome of individual *i*, control group.\n\n\n\n\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nSome notation:\n\nThese are the representations of the **causal effect** we often want to estimate.\n\n**Average Treatment Effect:**\n\nATE = $\\frac{1}{N} (E[Y_{i,1}] - E[Y_{i,0}])$\n\n. . . \n\n**Average Treatment Effect on the treated:**\n\nATET = $\\frac{1}{N} (E[Y_{i,1}|D_i=1] - E[Y_{i,0}|D_i=1])$\n\n. . . \n\n**Average Treatment Effect on the untreated:**\n\nATEU = $\\frac{1}{N} (E[Y_{i,1}|D_i=0] - E[Y_{i,0}|D_i=0])$\n\n. . . \n\nOf course, again, we cannot observe both potential outcomes of the same unit *i*.\n\n\n\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nWhen dealing with **causal inference**, we have to find ways to approximate what the hidden potential outcome of the treated units is. \n\nThat is, the challenge in identifying causal effects is that the untreated potential outcomes, $Y_{i,0}$, are never\nobserved for the treated group ($D_i= 1$). The \"second\" term in the following equation:\n\nATET = $\\frac{1}{N} (E[Y_{i,1}|D_i=1] - E[Y_{i,0}|D_i=1])$\n\n\nWe need an empirical design to **\"observe\"** what we do not really observe (i.e., the counterfactual). \n\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nMany options:\n\n- Matching/Balancing\n- Difference-in-differences (DiD)\n- Instrumental variables\n- Regression discontinuity design (RDD)\n- Synthetic control (Synth)\n\n\n\n\n\n\n\n\n## Selection Bias {.smaller background=\"#e3e2b8\"}\n\nThe process of finding units that are comparable is called **matching**.\n\n. . .\n\n**Before we continue...**\n\n**We will match on observables. We cannot be on unobservables.**\n\nThus, you may want to write in your article \"selection bias due to observables\".\n\n. . .\n\n**Cunningham:**\n\n*Propensity score matching has not seen as wide adoption among economists as in other nonexperimental methods like regression discontinuity or difference-in-differences. The most common reason given for this is that economists are oftentimes skeptical that CIA can be achieved in any dataset almost as an article of faith. This is because for many applications, economists as a group are usually more concerned about selection on unobservables than they are selection on observables, and as such, they reach for matching methods less often.*\n\nCIA = CMI\n\n\n\n\n\n\n\n\n# Matching  {.smaller background=\"#e0cafc\"}\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\n**Matching** aims to compare the outcomes between observations that have the same values of all control variables, except that one unit is treated and the other is not. \n\n. . .\n\nIn this literature, the control variables used to matched are often called **covariates**.\n\nThat is, for each treated unit, the researcher finds an untreated unit that is similar in all covariates.\n\nThe implication is that the researcher can argue that \"*units are comparable after matching*\". \n\n\n\n\n\n\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\nThe easiest to see is **exact matching**: *it matches observations that have the exact same values*. \n\n- It might be doable if you have only one covariate. \n\n- Naturally, if you have only one covariate, you might still be left with some selection bias.\n\n  - In the previous example, health history is one important covariate that makes John and Mary different. \n  \n  - But what about life style? Nutrition? Etc. \n  \n\nAs the number of covariates grow, you cannot pursue exact matching. That is the job of PSM.\n\n\n\n\n\n\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\n**In exact matching, the causal effect estimator (ATET) is:**\n\n$$ATET = \\frac{1}{N} \\sum (E[Y_{i}] - E[Y_{j(i)}] | D_i=1)$$\n\nWhere $Y_{j(i)}$ is the j-th unit matched to the i-th unit based on the j-th being “closest to” the i-th unit for some  covariate. \n\nFor instance, let’s say that a unit in the treatment group has a covariate with a value of 2 and we find another unit in the control group (exactly one unit) with a covariate value of 2. \n\nThen we will impute the treatment unit’s missing counterfactual with the matched unit’s, and take a difference.\n\n\n\n\n\n\n\n## Matching {.smaller background=\"#e0cafc\"}\n\nConsider the following dataset from Cunningham:\n\n![](figs/scott.png)\n\n\n\n\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\n::: panel-tabset\n### R Averages\n\nAverage ages are very different. The salary of a 24 yrs old person is quite different than the salary of a 32 yrs person.\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(knitr)\nlibrary(kableExtra)\n\nread_data <- function(df)\n{\n  full_path <- paste(\"https://github.com/scunning1975/mixtape/raw/master/\",df, sep = \"\")\n  df <- read_dta(full_path)\n  return(df)\n}\ntraining_example <- read_data(\"training_example.dta\") %>% slice(1:20)\nsummary(training_example$age_treat)\nsummary(training_example$age_control)\n```\n\n\n### R Treated\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(knitr)\nlibrary(kableExtra)\n\nread_data <- function(df)\n{\n  full_path <- paste(\"https://github.com/scunning1975/mixtape/raw/master/\",df, sep = \"\")\n  df <- read_dta(full_path)\n  return(df)\n}\n\ntraining_example <- read_data(\"training_example.dta\") %>% slice(1:20)\n\nggplot(training_example, aes(x=age_treat)) +\n  stat_bin(bins = 10, na.rm = TRUE)\n\n```\n\n### R Control\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(knitr)\nlibrary(kableExtra)\n\nread_data <- function(df)\n{\n  full_path <- paste(\"https://github.com/scunning1975/mixtape/raw/master/\",df, sep = \"\")\n  df <- read_dta(full_path)\n  return(df)\n}\n\ntraining_example <- read_data(\"training_example.dta\") %>% slice(1:20)\n\nggplot(training_example, aes(x=age_control)) +\n  stat_bin(bins = 10, na.rm = TRUE)\n\n```\n\n\n### R Matched\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(knitr)\nlibrary(kableExtra)\n\nread_data <- function(df)\n{\n  full_path <- paste(\"https://github.com/scunning1975/mixtape/raw/master/\",df, sep = \"\")\n  df <- read_dta(full_path)\n  return(df)\n}\n\ntraining_example <- read_data(\"training_example.dta\") %>% slice(1:20)\n\nggplot(training_example, aes(x=age_matched)) +\n  stat_bin(bins = 10, na.rm = TRUE)\n\n```\n\n\n:::\n\n\n\n\n## Matching   {.smaller background=\"#e0cafc\"}\n\nIn this example, you are literally finding the units in the control group that have the same age as the units in the treatment group.\n\nYou are exact matching 1-by-1 in this example.\n\nYou have only one covariate, i.e., age.\n\n\n\n\n\n\n\n\n\n\n\n# Distance Matching  {.smaller background=\"#c6f7ec\"}\n\n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\nThe last example was simple because you could *exact match*.\n\nIf you cannot find one exact match, you need an approximate match. \n\n. . .\n\nIn order to do that, you have to use distance matching.\n\n**Distance matching** minimizes the distance (i.e., how far the covariates are from each other) between the treatment and control groups.\n\n\n\n\n\n\n\n\n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\n**Euclidean distance** = $|X_i-X_j|=\\sqrt{(X_i-X_j)'(X_i-X_j)}=\\sqrt{\\sum_{n=1}^k(X_{n,i}-X_{n,j})^2}$\n\n![](figs/euclidian.png)\n\n\n\n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\n**Normalized Euclidean distance** = $|X_i-X_j|=\\sqrt{(X_i-X_j)'\\hat{V}^{-1}(X_i-X_j)}=\\sqrt{\\sum_{n=1}^k\\frac{(X_{n,i}-X_{n,j})}{\\sigma^2_n}}$\n\nThe problem with this measure of distance is that the distance measure itself depends on the **scale of the variables themselves**. \n\nFor this reason, researchers typically will use some modification of the Euclidean distance, such as the **normalized Euclidean distance**, or they’ll use a wholly different alternative distance. \n\nThe normalized Euclidean distance is a commonly used distance, and what makes it different is that the distance of each variable is scaled by the variable’s variance. \n\n\n \n \n \n \n \n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\n**Mahalanobis  distance** = $|X_i-X_j|=\\sqrt{(X_i-X_j)'\\hat{\\sum_x}^{-1}(X_i-X_j)}$\n\nWhere $\\hat{\\sum_x}$ is the sample covariance matrix of X.\n\n. . . \n\n![](figs/malahanobis_king_nielsen.png)\n\n\n\n\n\n## Distance Matching  {.smaller background=\"#c6f7ec\"}\n\nDistance matching only goes so far...\n\n... **the larger the dimensionality, the harder is to use distance matching**.\n\nAs sample size increases, for a given N of covariates, the matching discrepancies tend to zero.\n\nBut, the more covariates, the longer it takes.\n\n. . . \n\nAt the end of the day, it is preferable to have many covariates, but it is makes distance matching harder.\n\n\n\n\n\n\n\n\n# Coarsened Exact Matching (CER)  {.smaller background=\"#fce0cc\"}\n\n## Coarsened Exact Matching (CER)  {.smaller background=\"#fce0cc\"}\n\nIn coarsened exact matching, something only counts as a match if it exactly matches on each matching variable. \n\n**The “coarsened” part comes in because, if you have any continuous variables to match on, you need to “coarsen” them first by putting them into bins, rather than matching on exact values.**\n\nCoarsening means creating bins. Fewer bins makes exact matches more likely. \n\n. . .\n\nCER is not used much in empirical research in finance. It is used more in the big data realm when you have many variables to match. \n\n\n\n\n\n\n\n# Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n**PSM is one way to matching using many covariates.** \n\n**PSM aggregates all covariates into one score (propensity-score), which is the likelihood of receiving the treatment.**\n\nThe idea is to match units that, based on observables, have the same probability (called propensity-score) of being treated. \n\n. . .\n\nThe idea is to estimate a probit (default in stata) or logit model (fist stage):\n\n$$P(D=1|X)$$\n\n**The propensity-score is the predicted probability of a unit being treated given all covariates X**. The p-score is just a single number.\n\n\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nConsiderations in PSM.\n\n1) How many neighbors to match?\n\n- Nearest neighbor, radius or kernel?\n\n2) With or without replacement?\n\n3) With or without common support?\n\n- *Common support*: imposes a common support by dropping treatment observations whose pscore is higher than the maximum or less than the minimum pscore of the controls.\n\n4) It is expected that, after PSM, you show the overlap of propensity-scores.\n\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**The y-axis is the propensity-score**.\n\n![](figs/ani_katchova1.png)\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**Nearest matching:** Find the observation closest to ($min|p_i-p_j|$)\n\n![](figs/ani_katchova3.png)\n\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**Kernel matching:** Each treated observation i is matched with several control observations, with weights inversely proportional to the distance between treated and control observations.\n\n![](figs/ani_katchova2.png)\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**Radius matching**: Each treated observation i is matched with control observations j that fall within a specified radius.\n\n$$|p_i-p_j| <r$$\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n[Source](https://sites.google.com/site/econometricsacademy/home)\n\n**Common support:** Restrict matching only based on the common range of propensity scores.\n\n![](figs/ani_katchova5.png)\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nSeems good overlap, but \"good\" is arbitrary.\n\n![](figs/psm1.png)\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nSeems bad overlap\n\n![](figs/psm2.png)\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nSeems good overlap, but \"good\" is arbitrary.\n\n![](figs/psm_graph1.png)\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\nSeems bad overlap\n\n![](figs/psm_graph2.png)\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n![](figs/psm_bias.png)\n\n\n\n\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n![](figs/psm_ttest1.png)\n\n\n## Propensity-score matching (PSM)  {.smaller background=\"#e3bfc3\"}\n\n![](figs/psm_ttest2.png)\n\n\n\n\n\n\n\n\n\n\n# Example  {.smaller background=\"#dff5ce\"}\n\n## Example  {.smaller background=\"#dff5ce\"}\n\nLet's practice with an example. 185 treated units vs 15,992 control units. \n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# Load necessary packages\n# Load necessary libraries\nlibrary(haven)\nlibrary(psych)\ndata <- read_dta(\"files/cps1re74.dta\")\nsummary_stats <- by(data, data$treat, FUN = function(group) {\n  c(\n    mean = mean(group$age, na.rm = TRUE),\n    variance = var(group$age, na.rm = TRUE),\n    skewness = skew(group$age, na.rm = TRUE),\n    count = length(group$age)\n  )\n})\nsummary_df <- as.data.frame(do.call(rbind, summary_stats))\ncolnames(summary_df) <- c(\"mean\", \"variance\", \"skewness\", \"count\")\nprint(summary_df)\n```\n\n### Python\n\n```{python}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Python\"\nimport pandas as pd\nfrom scipy.stats import skew\nimport statsmodels.api as sm\ndata = pd.read_stata(\"files/cps1re74.dta\")\ngrouped_data = data.groupby('treat')['age'].agg(['mean', 'var', lambda x: skew(x, nan_policy='omit'), 'count']).reset_index()\ngrouped_data.columns = ['treat', 'mean', 'variance', 'skewness', 'count']\nprint(grouped_data)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/cps1re74.dta, clear\nqui estpost tabstat age black educ , by(treat) c(s) s(me v sk n) nototal\nesttab . \t,varwidth(20) cells(\"mean(fmt(3)) variance(fmt(3)) skewness(fmt(3)) count(fmt(0))\") noobs nonumber compress \n```  \n\n:::\n\n\n\n## Example  {.smaller background=\"#dff5ce\"}\n\nClearly, the treated group is younger, mainly black, and less educated.\n\nAlso note that the **variance and skewness** of the two subsamples are **different**.\n\nIf we were to use these two subsamples in any econometric analysis **without preprocessing to make them comparable**, we would likely have coefficients biased by **selection bias**.\n\nTherefore, it is important to perform some matching method.\n\nLet's start with Propensity Score Matching (PSM). We will use the simplest matching, that is, without using any additional functions.\n\n\n\n\n\n\n\n\n\n\n## Example  {.smaller background=\"#dff5ce\"}\n\n**Nearest with noreplacement.**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# install.packages(\"MatchIt\")\nlibrary(haven)\nlibrary(psych)\nlibrary(MatchIt)\ndata <- read_dta(\"files/cps1re74.dta\")\nmodel <- matchit(treat ~ age + black + educ, data = data, method = \"nearest\")\nsummary(model)\n```\n\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/cps1re74.dta, clear\npsmatch2 treat age black educ , n(1) noreplacement\nsum _weight , d\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n## Example  {.smaller background=\"#dff5ce\"}\n\n**Notice that we are creating weights now**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n# install.packages(\"MatchIt\")\nlibrary(haven)\nlibrary(MatchIt)\ndata <- read_dta(\"files/cps1re74.dta\")\nmodel <- matchit(treat ~ age + black + educ, data = data, method = \"exact\")\nsummary(model$weights)\n\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/cps1re74.dta, clear\nqui psmatch2 treat age black educ , kernel\nsum _weight , d\n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n## Example  {.smaller background=\"#dff5ce\"}\n\n**Now, the descriptive statistics are much closer**\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(haven)\nlibrary(MatchIt)\n#install.packages(\"e1071\")\nlibrary(e1071)\ndata <- read_dta(\"files/cps1re74.dta\")\nmodel <- matchit(treat ~ age + black + educ, data = data, method = \"exact\")\nmatched_data <- match.data(model)\nsummary_stats <- by(matched_data, matched_data$treat, function(x) {\n  c(mean(x$age), var(x$age), skewness(x$age), length(x$age))\n})\n\nresult_df <- data.frame(\n  Treatment = c(\"Control\", \"Treated\"),\n  Mean_Age = sapply(summary_stats, function(x) x[1]),\n  Variance_Age = sapply(summary_stats, function(x) x[2]),\n  Skewness_Age = sapply(summary_stats, function(x) x[3]),\n  Count = sapply(summary_stats, function(x) x[4])\n)\nprint(result_df)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/cps1re74.dta, clear\nqui psmatch2 treat age black educ , kernel\nqui estpost tabstat age black educ [aweight = _weight], by(treat) c(s) s(me v sk n) nototal\nesttab . \t,varwidth(20) cells(\"mean(fmt(3)) variance(fmt(3)) skewness(fmt(3)) count(fmt(0))\") noobs  nonumber compress \n```  \n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Entropy Balancing  {.smaller background=\"#fccad9\"}\n\n## Entropy Balancing  {.smaller background=\"#fccad9\"}\n\n**Here, instead of matching units, we reweight the observations such that the moments of the distributions (mean, variance, skewness) are similar.**\n\n- The ebalance function implements a reweighting scheme. The user starts by choosing the covariates that should be included in the reweighting. \n\n- For each covariate, the user then specifies a set of balance constraints (in Equation 5) to equate the moments of the covariate distribution between the treatment and the reweighted control group. \n\n- The moment constraints may include the mean (first moment), the variance (second moment), and the skewness (third moment).\n\n**The outcome is a vector containing the weights to weight the observations, such that the weighted average, weighted variance, and weighted skewness of the covariates in control group are similar to those in the treatment group**\n\n\n\n\n\n\n\n\n\n\n## Entropy Balancing  {.smaller background=\"#fccad9\"}\n\n\n::: panel-tabset\n### R\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: true\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(haven)\n#install.packages(\"ebal\")\nlibrary(ebal)\ndata <- read_dta(\"files/cps1re74.dta\")\ntreatment <-cbind(data$treat)\nvars <-cbind(data$age, data$educ, data$black)\neb <- ebalance(treatment, vars)\n# means in treatment group data\napply(vars[treatment==1,],2,mean)\n# means in reweighted control group data\napply(vars[treatment==0,],2,weighted.mean,w=eb$w)\n# means in raw data control group data\napply(vars[treatment==0,],2,mean)\n```\n\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/cps1re74.dta, clear\nebalance treat age black educ, targets(3)\n```  \n\n:::\n\n\n\n\n\n\n## Entropy Balancing  {.smaller background=\"#fccad9\"}\n\n\n::: panel-tabset\n### Stata\n\n```{stata}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output: true\n#| output-location: default\n#| code-fold: true\n#| code-line-numbers: true\n#| eval: true\n#| code-summary: \"Stata\"\nuse files/cps1re74.dta, clear\nqui ebalance treat age black educ, targets(3)\nqui estpost tabstat age black educ [aweight = _webal], by(treat) c(s) s(me v sk n) nototal\nesttab . \t,varwidth(20) cells(\"mean(fmt(3)) variance(fmt(3)) skewness(fmt(3)) count(fmt(0))\") noobs  nonumber compress \n```  \n\n:::\n\n\n\n\n\n## **THANK YOU!** {background=\"#b1cafa\"}\n\n::: columns\n::: {.column width=\"60%\"}\n**QUESTIONS?**\n\n![](figs/qa2.png){width=\"150%\" heigth=\"150%\"}\n:::\n\n::: {.column width=\"40%\"}\n**Henrique C. Martins**\n\n-   [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)\n-   [Personal Website](https://henriquemartins.net/)\n-   [LinkedIn](https://www.linkedin.com/in/henriquecastror/)\n-   [Lattes](http://lattes.cnpq.br/6076997472159785)\n-   [Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)\\\n:::\n:::\n\n::: footer\n:::\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","css":["logo.css"],"output-file":"part_5.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.4.549","auto-stretch":true,"html":{"css":"webex.css","include-after-body":"webex.js"},"editor":"visual","title":"Empirical Methods in Finance","subtitle":"Part 5","author":"Henrique C. Martins","title-slide-attributes":{"data-background-color":"#b1cafa"},"include-after":["<script type=\"text/javascript\">\n  Reveal.on('ready', event => {\n    if (event.indexh === 0) {\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n  });\n  Reveal.addEventListener('slidechanged', (event) => {\n    if (event.indexh === 0) {\n      Reveal.configure({ slideNumber: null });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n    if (event.indexh === 1) {\n      Reveal.configure({ slideNumber: 'c' });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n    }\n  });\n</script>\n"],"slideNumber":true,"theme":"simple","chalkboard":true,"previewLinks":"auto","logo":"figs/background8.png","footer":"**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  ","multiplex":true,"scrollable":true}}},"projectFormats":["html"]}