{"title":"Empirical Methods in Finance","markdown":{"yaml":{"title":"Empirical Methods in Finance","subtitle":"Practicing 7","author":"Henrique C. Martins","format":{"revealjs":{"slide-number":true,"theme":"simple","chalkboard":true,"preview-links":"auto","logo":"figs/background8.png","css":"logo.css","footer":"**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  ","multiplex":true,"scrollable":true}},"title-slide-attributes":{"data-background-color":"#b1cafa"},"include-after":"<script type=\"text/javascript\">\n  Reveal.on('ready', event => {\n    if (event.indexh === 0) {\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n  });\n  Reveal.addEventListener('slidechanged', (event) => {\n    if (event.indexh === 0) {\n      Reveal.configure({ slideNumber: null });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n    if (event.indexh === 1) {\n      Reveal.configure({ slideNumber: 'c' });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n    }\n  });\n</script>\n"},"headingText":"library(reticulate)","containsRefs":false,"markdown":"\n\n```{r setup}\n#| include: false\n#| warning: false\n\n# use_python(\"C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe\")\nlibrary(reticulate)\nlibrary(Statamarkdown)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggthemes)\n#reticulate::py_install(\"matplotlib\")\n#reticulate::py_install(\"seaborn\")\n#reticulate::py_install(\"pyfinance\")\n#reticulate::py_install(\"xlrd\")\n#reticulate::py_install(\"quandl\")\n#reticulate::py_install(\"linearmodels\")\n#reticulate::py_install(\"causalml\")\n\n```\n\n\n\n\n\n## Setup {.smaller}\n\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(readxl) \nlibrary(jtools) # for nice tables of models - https://cran.r-project.org/web/packages/jtools/vignettes/summ.html#summ\ndata <- read_excel(\"files/data.xls\")\n\nlibrary(dplyr)\ndata <- data %>% group_by(id )                %>% dplyr::mutate(id_firm = cur_group_id())\ndata <- data %>% group_by(setor_economatica)  %>% dplyr::mutate(id_ind = cur_group_id())\n\nlibrary(plm)\ndata <- pdata.frame(data, index=c(\"id_firm\",\"year\"))\n\nattach(data)\n\n# variables\ndata$lev1 <- Debt / (Debt + Equity.market.value)\ndata$lev2 <- Debt / Total.Assets\ndata$wc_ta <- wc  / Total.Assets \ndata$cash_ta <- cash / Total.Assets\ndata$div_ta <- Dividends / Total.Assets\ndata$fcf_ta <- Free.cash.flow / Total.Assets\ndata$tang_ta <- tangible / Total.Assets\ndata$roa2 <- roa / 100\nlibrary(SciViews)\ndata$size1  <- ln(Total.Assets)\n\n# winsor\nlibrary(DescTools)\ndata$w_lev1     <- Winsorize(data$lev1   , probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_lev2     <- Winsorize(data$lev2   , probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_wc_ta    <- Winsorize(data$wc_ta  , probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_cash_ta  <- Winsorize(data$cash_ta, probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_size1    <- Winsorize(data$size1  , probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_fcf_ta   <- Winsorize(data$fcf_ta , probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_div_ta   <- Winsorize(data$div_ta , probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_roa      <- Winsorize(data$roa    , probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_tang_ta  <- Winsorize(data$tang_ta, probs = c(0.01, 0.99) , na.rm = TRUE) \n\n```\n\n\n\n\n## Reverse causation {.smaller}\n\n- Imagine that you want to investigate the effect of Governance on Q\n\n$𝑸_{i} = α + 𝜷_{i} × Gov + Controls + error$\n\n\nThe ideal is to establish estimates that allow you to infer that __changing Gov will _CAUSE_ a change in Q__. However, without a nice empirical design, we cannot infer causality\n\n\n_One source of bias is: reverse causation_\n\n- Perhaps it is Q that causes Gov\n\n- OLS based methods do not tell the difference between these two betas:\n\n$𝑄_{i} = α + 𝜷_{i} × Gov + Controls + error$\n\n$Gov_{i} = α + 𝜷_{i} × Q + Controls + error$\n\n- If one Beta is significant, the other will most likely be significant too.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Lags to mitigate reverse causation  {.smaller}\n\n In a regression model, there is always the possibility that it is Y that causes X, meaning that your assumptions about the relationship of these variables can always be wrong (i.e., what *causes* what?).\n\nWe are not discussing **causality** yet (which is a whole new chapter). But one possible simple solution is to use lagged values of your X. Something along the following lines:\n\n$$Y_{t,i} = \\alpha + \\beta \\times X_{t-1,i} + \\epsilon_{t,i}$$\n\nNotice the subscript $t-1$ in x now. It means that you are using the previous period's value of X as explanatory variable of the current period's value of Y. \n\nThis type of structure mitigates the concern that variations in Y are the reason of why X varies since it is less likely that the **current** variation of Y **provokes** variations on X in  the previous years (i.e., the idea is that the future does not affect the past).\n\n\n\n\n\n\n\n\n\n## Lags to mitigate reverse causation  {.smaller}\n\nThis is not a perfect solution because Y and X in most accounting and finance research designs are usually auto-correlated, meaning that the previous values are correlated with the present value. \n\nFor instance, the firm's leverage of 2015 is highly correlated with the firm's leverage of 2016. \n\nBut in many cases it is a good idea to use lag values. At least, you should have this solution in your toolbox. \n\n\n\n\n## Lags to mitigate reverse causation  {.smaller}\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nfe <- plm(w_lev1 ~ w_size1 + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"within\")\nlag <- plm(w_lev1 ~ lag(w_size1) + lag(w_fcf_ta) + lag(w_roa) + lag(w_tang_ta) + lag(w_cash_ta) + lag(w_div_ta) + factor(year) , data = data, model=\"within\")\nexport_summs(fe, lag, coefs = c(\"w_size1\",\"w_div_ta\",\"w_fcf_ta\",\"w_roa\",\"w_tang_ta\",\"w_cash_ta\",\n                              \"lag(w_size1)\",\"lag(w_div_ta)\",\"lag(w_fcf_ta)\", \"lag(w_roa)\",\"lag(w_tang_ta)\",\"lag(w_cash_ta)\"), digits = 3 , model.names = c(\"FE\",\"FE with lags\") )\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## First difference models {.smaller}\n\nAlternatively, we can estimate a first difference model as follows.\n\n$$\\Delta Y_{t,i} = \\alpha + \\beta \\times \\Delta X_{t,i} +\\ epsilon_{t,i}$$\n\nWhere, $\\Delta Y_{t,i} =Y_{t,i} - Y_{t-1,i}$   and $\\Delta X_{t,i} =X_{t,i} - X_{t-1,i}$ \n\n$\\beta$ shows the difference in the average change of Y for units that experience a change in x during the same period. \n\nSaying the same thing in a different way, $\\beta$ shows how much y changes, on average, where and when X increases by one unit. \n\nLet's say that x is binary and it changes from 0 to 1 for each firm during several different periods. So, y will change, on average, by $\\beta$ when x changes from 0 to 1. \n\n\n\n\n\n## First difference models {.smaller}\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nfe <- plm(w_lev1 ~ w_size1 + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"within\")\nfd <- plm(w_lev1 ~ w_size1 + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"fd\") \nexport_summs(fe, fd, coefs = c(\"w_size1\",\"w_div_ta\",\"w_fcf_ta\",\"w_roa\",\"w_tang_ta\",\"w_cash_ta\")  , digits = 3, model.names = c(\"FE\",\"FD\"))\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Binary variables {.smaller}\n\nAs mentioned before, a Fixed effect is equivalent to a binary variable marking one group of observations. For instance, all observations of the same year, or from the same firm.\n\nWe can explore many interesting types of binary variables in most cases of corporate finance. For instance, whether the firm is included in \"Novo Mercado\", if the firm has high levels of ESG, etc. \n\nThe implementation of a binary variable is quite simple: it takes the value of  0 for one group, and 1 for the other.\n\nThe interpretation is a bit trickier. \n\n\n## Binary variables {.smaller}\n\n\nLet's think about the example 7.1 of Wooldridge. He estimates the following equation:\n\n$$wage = \\beta_0 + \\delta_1 female + \\beta_1 educ + \\mu$$\n\n*In model (7.1), only two observed factors affect wage: gender and education. Because $female = 1$ when the person is female, and $female = 0 $ when the person is male, the parameter $\\delta_1$ has the following interpretation: *\n\n*- $\\delta_1$  is the difference in hourly wage between females and males, given the same amount of education (and the same error term u). Thus, the coefficient $\\delta_1$  determines whether there is discrimination against women: if $\\delta_1<0$, then, for the same level of other factors, women earn less than men on average.*\n\n\n\n\n\n## Binary variables {.smaller}\n\nIn terms of expectations, if we assume the zero conditional mean assumption E($\\mu$ | female,educ) = 0, then \n\n$\\delta_1 = E(wage | female = 1, educ) - E(wage | female = 0, educ)$ \n\nOr \n\n$\\delta_1 = E(wage | female, educ) - E(wage | male, educ)$ \n\n\n- The key here is that the level of education is the same in both expectations; the difference, $\\delta_1$ , is due to gender only.\n\n\n## Binary variables {.smaller}\n\n\nThe visual interpretation is as follows. The situation can be depicted graphically as an intercept shift between males and females. The interpretation relies on $\\delta_1$. We can observe that $delta_1 < 0$; this is an argument for existence of a gender gap in wage.\n \n\n![](files/wooldridge_7_1.png)\n\n\n\n\n\n\n\n## Binary variables {.smaller}\n\nUsing our own example, we can make the case that it is necessary to separate the firms in two groups: dividend payers and non-payers. \n\nThere is literature suggesting that dividend payers are not financially constrained, while those firms that do not pay dividends are. \n\n**If financial constrain is something important to our model (and assuming that the right way to control for it is by including a dividend payer dummy), we should include such a dummy.**\n\n\n\n\n\n\n## Binary variables {.smaller}\n\n$$Lev_{t,i} = \\alpha + \\beta_1 \\times Size_{t,i} + \\beta_2 \\times Div.\\;payer_{t,i} + controls + \\epsilon_{t,i}$$\n\nWe could estimate this model as follows. \n\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\ndata$w_div_payer <- ifelse(data$w_div_ta <= 0, 0, 1)\ntapply(data$w_div_ta, data$w_div_payer, summary)    # Summary by group using tapply\nfe1 <- plm(w_lev1 ~ w_size1 + w_div_ta + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta  + factor(year) , data = data, model=\"within\")\nfe2 <- plm(w_lev1 ~ w_size1 + w_div_payer + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta  + factor(year) , data = data, model=\"within\")\nexport_summs(fe1, fe2, coefs = c(\"w_size1\",\"w_div_ta\",\"w_div_payer\",\"w_fcf_ta\",\"w_roa\",\"w_tang_ta\",\"w_cash_ta\")  , digits = 3, model.names = c(\"FE 1\",\"FE 2\"))\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Models with squared variables {.smaller}\n\nLet's say you have a variable that should not show a clear linear relationship with another variable. \n\nFor instance, consider ownership concentration and firm value. There is a case to be made the relationship between these variable is not linear. That is, in low levels of ownership concentration (let's say 5% of shares), a small increase in it might lead to an increase in firm value. The argument is that, in such levels, an increase in ownership concentration will lead the shareholder to monitor more the management maximizing the likelihood of value increasing  decisions.\n\nBut consider now the case where the shareholder has 60% or more of the firm's outstanding shares. If you increase further the concentration it might signals the market that this shareholder is too powerful that might start using the firm to personal benefits (which will not be shared with minorities). \n\n\n\n\n\n\n\n## Models with squared variables {.smaller}\n\n\nIf this story is true, the relationship is (inverse) u-shaped. That is, at first the relationship is positive, then becomes negative.\n\nTheoretically, I could make an argument for a non-linear relationship between several variables of interest in finance. Let's say size and leverage. Small firms might not be able to issue too much debt as middle size firms. At the same time, large firms might not need debt. The empirical relationship might be non-linear. \n\nThere is always a potential case to be made regarding the relationship between the variables.\n\n\n\n## Models with squared variables {.smaller}\n\nThe way to empirically test it is as follows:\n\n\n$$Y_{t,i} = \\alpha + \\beta_1 \\times X_{t,i} + \\beta_2 \\times X^2_{t,i} + \\epsilon_{t,i}$$\n\nOr using the size-leverage example:\n\n$$Lev_{t,i} = \\alpha + \\beta_1 \\times Size_{t,i} + \\beta_2 \\times Size^2_{t,i} + controls + \\epsilon_{t,i}$$\nAs Wooldridge says, misspecifying the functional form of a model can certainly have serious consequences. But, in this specific case, the problem seems minor since we have the data to fix it. \n\n\n\n\n\n\n## Models with squared variables {.smaller}\n\nIn this specific case, our theory does not hold, since the squared term is not significant. So we can conclude that in this model, the relationship is linear.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\ndata$w_size1_sq <- data$w_size1 * data$w_size1 \nfe <- plm(w_lev1 ~ w_size1 + w_size1_sq + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"within\")\nexport_summs(fe, coefs = c(\"w_size1\",\"w_size1_sq\",\"w_div_ta\",\"w_fcf_ta\",\"w_roa\", \"w_tang_ta\",\"w_cash_ta\")  , digits = 3)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Models with Interactions {.smaller}\n\nIn some specific cases, you want to interact variables to test if the interacted effect is significant. For instance, you might believe that, using Wooldridge very traditional example 7.4., women that are married are yet more discriminated in the job market. So, you may prefer to estimate the following equation.\n\n\n$$wage = \\beta_0 + \\beta_1 female + \\beta_2 married + \\beta_3 female.married + \\mu$$\n\nWhere $maried$ is a binary variable marking all married people with 1, and 0 otherwise. \n\nIn this setting, the group of single men is the base case and is represented by $\\beta_0$. That is, both female and married are 0.\n\n- The group of single women is represented by $\\beta_0 + \\beta_1$. That is, female is 1 but married is 0.\n\n- The group of married men is represented by $\\beta_0 + \\beta_2$. That is, female is 0 but married is 1.\n\n- Finally, the group of married women is represented by $\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3$. That is, female and married are 1.\n\n\n## Models with Interactions {.smaller}\n\nUsing a random sample taken from the U.S. Current Population Survey for the year 1976, Wooldridge estimates that $female<0$, $married>0$, and $female.married<0$. This result makes sense for the 70s. \n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(wooldridge)\ndata('wage1')\nwage1$fem_mar <- wage1$female * wage1$married\nwage<- lm(lwage ~ female + married + fem_mar + educ + exper + expersq + tenure + tenursq , data = wage1)\nexport_summs(wage, coefs = c(\"(Intercept)\", \"female\",\"married\",\"fem_mar\") , digits = 3)\n```\n\n\n![](files/wooldridge_7_4.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Linear probability model {.smaller}\n\n\nWhen the dependent variable is binary we cannot rely on linear models as those discussed so far. We need a **linear probability model**. In such models, we are interested in how the probability of the occurrence of an event depends on the values of x. That is, we want to know $P[y=1|x]$.\n\nImagine that y is employment status, 0 for unemployed, 1 for employed. This is our Y. Imagine that we are interested in estimating the probability that a person start working after a training program. For these types of problem, we need a linear probability model.\n\n\n$$P[y=1|x] = \\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_kx_k$$\n\nThe mechanics of estimating these model is similar to before, except that Y is binary.\n\nThe interpretation of coefficients change. That is, **a change in x changes the probability of y = 1**. So, let's say that $\\beta_1$ is 0.05. It means that changing $x_1$ by one unit will change the probability of y = 1 (i.e., getting a job) in 5%, ceteris paribus. \n\n\n\n\n\n\n\n\n## Linear probability model {.smaller}\n\n\nUsing Wooldridge's example 7.29:\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(wooldridge)\ndata('mroz')\nlpm <- lm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = mroz)\nexport_summs(lpm, coefs = c(\"(Intercept)\", \"nwifeinc\" , \"educ\" , \"exper\" , \"expersq\" , \"age\"  ,\"kidslt6\" , \"kidsge6\"), digits = 3 , model.names = c(\"LPM\"))\n```\n\n\n\n\n## Linear probability model {.smaller}\n\n![](files/wooldridge_7_29.png)\n\n\nThe relationship between the probability of labor force participation and educ is plotted in the figure below. Fixing the other independent variables at 50, 5, 30, 1 and 6, respectively, the predicted probability is negative until education equals 3.84 years. This is odd, since the model is predicting negative probability of employment given a set of specific values. \n\n\n\n![](files/wooldridge_7_3.png)\n\n\n## Linear probability model {.smaller}\n\n\nAnother example. The model is predicting that *going from 0 to 4 kids less than 6 years old* reduces the probability of working by $4\\times 0.262 = 1.048$, which is impossible since it is higher than 1.\n\n\nThat is, **one important caveat of a linear probability model is that probabilities might falls off of expected empirical values**. If this is troublesome to us, we might need a different solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Logit and Probit {.smaller}\n\nAlthough the linear probability model is simple to estimate and use, it has some limitations as discussed. If that problem is important to us, we need a solution that addresses the problem of  negative or higher than 1 probability. That is, **we need a binary response model**.\n\nIn a binary response model, interest relies on the response probability.\n\n$$P(y =1 | x) = P(y=1| x_1,x_2,x_3,...)$$\n\nThat is, we have a group of X variables explaining Y, which is binary. In a LPM, we assume that the response probability is linear in the parameters $\\beta$. This is the assumption that created the problem discussed above.\n\n\n\n\n\n## Logit and Probit {.smaller}\n\nWe can change that assumption to a different function. \n\n- A **logit** model assumes a logistic function ($G(Z)=\\frac{exp(z)}{[1+exp(z)]}$)\n- A **probit** model assumes a standard normal cumulative distribution function ($\\int_{-inf}^{+z}\\phi(v)dv$)\n\nThe adjustment is something as follows.\n\n$$P(y =1 | x) = G(\\beta_0 + \\beta_1 x_1+ \\beta_2 x_2 + \\beta_3 x_3)$$\nWhere G is either the logistic (logit) or the normal (probit) function. \n\n\n**We don't need to memorize these functions**, but we need to understand the adjustment that assuming a different function makes. Basically, **we will not predict negative or above 1 values anymore because the function adjusts at very low and very high values*. \n\n\n## Logit and Probit {.smaller}\n\n\n![](files/wooldridge_17_1.png)\n\n\n\n## Logit and Probit {.smaller}\n\nLet's estimate a logit and probit to compare with the LPM.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlpm <- lm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = mroz)\nlogit <- glm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = mroz,family = binomial)\nprobit <- glm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = mroz, family = binomial(link = probit))\nexport_summs(lpm , logit, probit, coefs = c( \"nwifeinc\" , \"educ\" , \"exper\" , \"expersq\" , \"age\"  ,\"kidslt6\" , \"kidsge6\", \"(Intercept)\"), digits = 4 , model.names = c(\"LPM\",\"Logit\",\"Probit\"))\n```\n\n\n\n## Logit and Probit {.smaller}\n\n\nImportantly, in a LPM model, the coefficients have similar interpretations as usual. But logit and probit models lead to harder to interpret coefficients. \n\nIn fact, often we do not make any interpretation of these coefficients. Instead, we usually transform them to arrive at an interpretation that is similar to what we have in LPM. \n\nTo make the magnitudes of probit and logit roughly comparable, we can multiply the probit coefficients by 1.6, or we can multiply the logit estimates by .625. \n\nAlso, the probit slope estimates can be divided by 2.5 to make them comparable to the LPM estimates.\n\nAt the end of the day, the interpretation of the logit and probit outputs are similar to LPM's.  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Tobit {.smaller}\n\nAnother problem in the dependent variable occurs when we have a **limited dependent variable** with a corner solution. \n\n- That is, a variable that ranges from zero to all positive values. \n\n- For instance, hours working. Nobody works less than zero hours, but individuals in the population can work many number of positive hours. \n\n- When we have such type of dependent variable, we need to estimate a **tobit** model.\n\n\n\n\n\n\n## Tobit {.smaller}\n\nUsing Wooldridge's example 17.2.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(AER)\nlpm <- lm(hours ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = mroz)\ntobit <- tobit(hours ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = mroz)\nsummary(tobit)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# OLS concerns {.smaller}\n\n\n## Heteroscedasticity {.smaller}\n\nOne of the key assumptions in OLS estimators is that $var(\\mu|x_1,x_2,x_3,...) = \\sigma^2$. That is, the assumption is that the variance of the errors is homoskedastic (present constant variance). It means that throughout all observations, the error term shows the same variance $\\sigma^2$. If errors are not homoskedastic, we have the **Heteroscedasticity problem**.\n\nHeteroskedasticity does not cause bias or inconsistency in the OLS estimators of the $\\beta$ like the OVB would. It also does not affect the $R^2$. What **Heteroscedasticity does is to bias the standard errors of the estimates**.\n\nRemember again that $t_{\\beta} = \\frac{\\hat{\\beta}}{se(\\hat{\\beta})}$. So, if you have biased standard errors, you will not assess correctly the significance of your coefficients. It also affects the F statistics.\n\nGraphically, we can think as follows.\n\n\n\n\n\n## Heteroscedasticity {.smaller}\n\nExample of homoscedasticity: \n\n\n\n\n![](files/homoscedasticity.png)\n\n## Heteroscedasticity {.smaller}\n\nExample of heteroscedasticity: \n\n![](files/heteroscedasticity.png)\n\n\n\n\n## Heteroscedasticity {.smaller}\n\nTo give you more context, think in terms of the relationship that we've discussing $leverage=f(size)$. \n\nIt is quite possible that small firms will have less options of leverage than large companies. \n\nThis means that a subsample of large companies will have higher variance in the leverage decisions (and thus the error terms) than the subsample of small firms. \n\nSo, we need to correct somehow the heteroskedasticity problem to find unbiased standard errors for the independent variable size in this model. \n\n\n\n\n## Heteroscedasticity {.smaller}\n\nThe solution to this problem is to estimate **Robust standard errors**. Basically, we will need to change the estimator of the standard error to an unbiased version.\n\nWe used to estimate (theoretical, populational):\n\n$$var(\\hat\\beta_1) = \\frac{\\sum_{i=1}^n(x_1-\\bar{x}^2) \\sigma^2_i}{SST^2_x}$$\n\nBut now we would estimate (sample).  \n\n$$var(\\hat\\beta_1) = \\frac{\\sum_{i=1}^n(x_1-\\bar{x}^2) \\mu^2_i}{SST^2_x}$$\n\nThis is called *White-Robust standard error* or the *Heteroscedasticity-Robust standard error* and was first showed by White (1980).\n\n\n\n\n\n## Heteroscedasticity {.smaller}\n\nBefore we estimate a model with robust standard errors, let's visually check if there is heteroskedasticity in the errors of the model. I am using Wooldridge's example 8.1.\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: slide\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(dplyr)\nlibrary(sandwich)\nlibrary(lmtest) \nwage1<-wage1  %>%  mutate(marmale = case_when(female == 0 & married == 1 ~ 1,\n                                              female == 0 & married == 0 ~ 0,\n                                              female == 1 & married == 1 ~ 0,\n                                              female == 1 & married == 0 ~ 0) )\nwage1<-wage1  %>%  mutate(marrfem = case_when(female == 0 & married == 1 ~ 0,\n                                              female == 0 & married == 0 ~ 0,\n                                              female == 1 & married == 1 ~ 1,\n                                              female == 1 & married == 0 ~ 0) )\nwage1<-wage1  %>%  mutate(singfem = case_when(female == 0 & married == 1 ~ 0,\n                                              female == 0 & married == 0 ~ 0,\n                                              female == 1 & married == 1 ~ 0,\n                                              female == 1 & married == 0 ~ 1) )\nwage_t <- lm(lwage ~ marmale + marrfem + singfem + educ + exper + expersq + tenure + tenursq , data = wage1)\n\nlibrary(tidyverse)\nlibrary(broom)\nfitted_data <- augment(wage_t, data = wage1)\nggplot(fitted_data, aes(x = .fitted, y = .resid)) +  geom_point() + geom_smooth(method = \"lm\") + theme_solarized()\n```\n\n\n\n\n## Heteroscedasticity {.smaller}\n\n\nVisually, we do not see much variation in the error term throughout the x axis. Not much evidence of  heteroskedasticity. \n\nBut let's formally test it using the Breusch-Pagan test.\n\nThe H_0 of this test is for homoskedasticity. Thus, **if we reject the test, heteroskedasticity is present.** \n\nThe estimated p-value is 10.55%, above the usual levels. We can conclude that heteroskedasticity is not present is such model. \n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nbptest(wage_t)\n```\n\n\n\n## Heteroscedasticity {.smaller}\n\n\nLet's estimate both standard errors to see their difference. Because we do not see much heteroskedasticity we should not see much difference in the estimated standard errors. \n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nwage_r <- coeftest(wage_t, vcov = vcovHC)\nexport_summs(wage_t, wage_r, coefs = c(\"(Intercept)\", \"marmale\",\"marrfem\",\"singfem\",  \"educ\" , \"exper\" ,\"expersq\" , \"tenure\" , \"tenursq\" ) , digits = 4 ,model.names = c(\"Traditional S.E.\", \"Robust S.E.\"))\n```\n\n\n## Heteroscedasticity {.smaller}\n\n\nNotice that the standard errors have changed a bit, but not too much in this example. \n\nUsually, the robust standard errors are larger than the traditional ones in empirical works, but they could be smaller. \n\nAlso notice that most of the independent variables have similar significance levels, but some have less significance. \n\nThis is expected, since the robust standard errors are expected to be larger (but again not always). \n\nA final note, the standard errors estimates here are a bit different than those in Wooldridge I am assuming this is due to the package that I am using.\n\n**Finally, it is often common and most widely accepted to estimate the robust standard errors instead of the traditional ones.** \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Multicollinearity {.smaller}\n\nMulticollinearity is the term used when many of the independent variables in a model are correlated. \n\nThis problem is not clearly formulated in econometrics book due to its nature. \n\nBut one thing is evident: **multicollinearity increases the standard errors of the coefficients**\n\nIn a sense, this problem is similar to having a small sample, from which is hard (in a statistical sense, meaning high standard deviation) to estimate the coefficient.\n\nAs Wooldridge says: *everything else being equal, for estimating $\\beta_j$, it is better to have less correlation between $x_j$ and the other independent variables.* \n\n\n## Multicollinearity {.smaller}\n\n\nWe can use the following test to verify if multicollinearity exists: **variance inflation factor (VIF)**. \n\nThere is no formal threshold to interpret the test, but 10 is usually accepted as appropriate. \n\nThat is, if there is one or more variables showing a VIF of 10 or higher, the interpretation is that there is evidence of multicollinearity.\n\n\n\n\n\n\n\n## Multicollinearity {.smaller}\n\nWe can observe below that the experience variables are multicolinear. This is expected since one is the squared root of the other.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nvif(wage_t)\n```\nAssuming that expersq is important to be included in the model, there is not much we can do in this example. \n\nSome scholars argue that we could ignore the multicollinearity altogether, others would argue to exclude expersq if possible.\n\nThe general most accepted solution is to **keep multicolinear variables if they are control variables**. \n\nThat is, if your focus is on any other variable, you can keep the multicolinear ones and move on. If the multicolinear variable is one of the interest variables, you might want to discuss dropping the other one. \n\n\n\n\n\n## Multicollinearity {.smaller}\n\nLet's investigate our previous OLS model. We see no evidence of multicollinearity.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nols <- lm(w_lev1 ~ w_size1 + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data)\nvif(ols)\n```\n\n\n\n\n\n\n\n##  Measurement error {.smaller}\n\n**The measurement error problem has a similar statistical structure to the omitted variable bias (OVB).**\n\n- \"Classical\" random measurement error for the outcome will inflate standard errors but will not lead to biased coefficients. \n\n    - $y^{*} = y + \\sigma_{1}$\n    - If you estimante $y^{*} = f(x)$, you have $y + \\sigma_{1} = x + \\epsilon$ \n    - $y = x + u$ \n        - where $u = \\epsilon + \\sigma_{1}$ \n\n##  Measurement error {.smaller}\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\ndata$w_lev1_noise <- data$w_lev1 + runif(length((data$w_lev1)), min=-0.2, max= 0.2)\n\nfe <- plm(w_lev1 ~ w_size1 + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"within\")\nfe_noise <- plm(w_lev1_noise  ~ w_size1 + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"within\")\n\nexport_summs(fe, fe_noise, coefs = c(\"w_size1\",\"w_div_ta\",\"w_fcf_ta\",\"w_roa\",\"w_tang_ta\",\"w_cash_ta\"), digits = 3 , model.names = c(\"Size\",\"Lev with noise\") )\n```\n\n##  Measurement error {.smaller}\n\n- \"Classical\" random measurement error in x’s will bias coefficient estimates toward zero\n\n    - $x^{*} = x + \\sigma_{2}$\n    - Imagine that $x^{*}$ is a bunch of noise\n    - It would not explain anything\n    - Thus, your results are biased toward zero\n\n\n\n##  Measurement error {.smaller}\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\ndata$w_size1_noise <- data$w_size1 + runif(length((data$w_size1)), min=-2, max= 2)\n\nfe <- plm(w_lev1 ~ w_size1 + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"within\")\nfe_noise <- plm(w_lev1 ~ w_size1_noise + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"within\")\n\nexport_summs(fe, fe_noise, coefs = c(\"w_size1\",\"w_size1_noise\",\"w_div_ta\",\"w_fcf_ta\",\"w_roa\",\"w_tang_ta\",\"w_cash_ta\"), digits = 3 , model.names = c(\"Size\",\"Size with noise\") )\n```\n\n\n\n\n\n\n\n##  Omitted  variable bias (OVB) {.smaller}\n\nLet's walk through an empirical example of the OVB problem. \n\n- Imagine that you do not include an important “true” predictor of Q:\n\n- Let's say, you have a long equation as:  $𝑸_{i} = 𝜶_{long} + 𝜷_{long}* gov_{i} + δ * omitted + error$\n\n- But you estimate a short version of the same equation missing one variable:  $𝑸_{i} = 𝜶_{short} + 𝜷_{short}* gov_{i} + error$\n\n- $𝜷_{short}$ will be: \n\n    - $𝜷_{short} = 𝜷_{long}$ +  bias\n\n    - $𝜷_{short} = 𝜷_{long}$ +  relationship between omitted (omitted) and included (Gov) * effect of omitted in long (δ)\n\n        - Where: relationship between omitted (omitted) and included (Gov) is: $Omitted = 𝜶 + ϕ *gov_{i} + u$\n\n- Thus, OVB is: $𝜷_{short} – 𝜷_{long} = ϕ * δ$\n\n\n##  Omitted  variable bias (OVB) {.smaller}\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n\nlibrary(readxl)\novb <- read_excel(\"files/ovb.xlsx\")\nshort <- lm(performance ~ bad_decision  , data = ovb)\nlong <- lm(performance ~ bad_decision  + risky_firm, data = ovb)\nexport_summs(short, long, coefs = c(\"bad_decision\",\"risky_firm\"), digits = 3 , model.names = c(\"Short\",\"Long\") )\n```\n\n\n\n##  Omitted  variable bias (OVB) {.smaller}\n\n\nIn this example, we can say that the OVB is $short = long + bias$.\n\nThat is, $0.44535  = -0.38389 + bias$, or $0.44535  = -0.38389 + 0.82924$. \n\nWhich is the same as: $0.44535  = -0.38389 + phi (omitted = f(non-omitted)) * omega (beta\\; \\; omitted \\;in\\; long)$\n\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\novbmodel <- lm(risky_firm ~bad_decision , data = ovb )\n# The OVB is 0.44535  = -0.38389 + 1.25146 * 0.66262\nmatrix1<- summary(long)$coefficients\nmatrix2<- summary(ovbmodel)$coefficients\n# Calculating OVB\nsum(matrix1[3,1] * matrix2[2,1]) \n```\n\n\n##  Omitted  variable bias (OVB) {.smaller}\n\n\nWe can see that omitting the variable \"risky_firm\" is problematic since it seems to explain the outcome of this regression.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\ntapply(ovb$performance, ovb$risky_firm, summary)\ntapply(ovb$bad_decision, ovb$risky_firm, summary)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n## 🙋‍♂️ **Any Questions?**{ .smaller  background=\"#fdf6e3\"}\n\n::: columns\n::: {.column width=\"50%\"}\n### Thank You!\n\n![](figs/qa2.png){width=\"110%\" style=\"box-shadow: none;\"}\n\n:::\n\n::: {.column width=\"50%\"}\n<div style=\"text-align:right;\">\n  <img src=\"figs/avatar.jpg\" width=\"120px\" style=\"border-radius:50%; box-shadow:0 4px 12px rgba(0,0,0,.25);\" />\n</div>\n\n### **Henrique C. Martins**\n\n- 🌐 [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)  \n- 💼 [LinkedIn](https://www.linkedin.com/in/henriquecastror/)  \n- 🧠 [Google Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)  \n- 📄 [Lattes CV](http://lattes.cnpq.br/6076997472159785)  \n- 🏠 [Personal Website](https://henriquemartins.net/)  \n:::\n:::\n\n","srcMarkdownNoYaml":"\n\n```{r setup}\n#| include: false\n#| warning: false\n\n# library(reticulate)\n# use_python(\"C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe\")\nlibrary(reticulate)\nlibrary(Statamarkdown)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggthemes)\n#reticulate::py_install(\"matplotlib\")\n#reticulate::py_install(\"seaborn\")\n#reticulate::py_install(\"pyfinance\")\n#reticulate::py_install(\"xlrd\")\n#reticulate::py_install(\"quandl\")\n#reticulate::py_install(\"linearmodels\")\n#reticulate::py_install(\"causalml\")\n\n```\n\n\n\n\n\n## Setup {.smaller}\n\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(readxl) \nlibrary(jtools) # for nice tables of models - https://cran.r-project.org/web/packages/jtools/vignettes/summ.html#summ\ndata <- read_excel(\"files/data.xls\")\n\nlibrary(dplyr)\ndata <- data %>% group_by(id )                %>% dplyr::mutate(id_firm = cur_group_id())\ndata <- data %>% group_by(setor_economatica)  %>% dplyr::mutate(id_ind = cur_group_id())\n\nlibrary(plm)\ndata <- pdata.frame(data, index=c(\"id_firm\",\"year\"))\n\nattach(data)\n\n# variables\ndata$lev1 <- Debt / (Debt + Equity.market.value)\ndata$lev2 <- Debt / Total.Assets\ndata$wc_ta <- wc  / Total.Assets \ndata$cash_ta <- cash / Total.Assets\ndata$div_ta <- Dividends / Total.Assets\ndata$fcf_ta <- Free.cash.flow / Total.Assets\ndata$tang_ta <- tangible / Total.Assets\ndata$roa2 <- roa / 100\nlibrary(SciViews)\ndata$size1  <- ln(Total.Assets)\n\n# winsor\nlibrary(DescTools)\ndata$w_lev1     <- Winsorize(data$lev1   , probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_lev2     <- Winsorize(data$lev2   , probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_wc_ta    <- Winsorize(data$wc_ta  , probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_cash_ta  <- Winsorize(data$cash_ta, probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_size1    <- Winsorize(data$size1  , probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_fcf_ta   <- Winsorize(data$fcf_ta , probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_div_ta   <- Winsorize(data$div_ta , probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_roa      <- Winsorize(data$roa    , probs = c(0.01, 0.99) , na.rm = TRUE) \ndata$w_tang_ta  <- Winsorize(data$tang_ta, probs = c(0.01, 0.99) , na.rm = TRUE) \n\n```\n\n\n\n\n## Reverse causation {.smaller}\n\n- Imagine that you want to investigate the effect of Governance on Q\n\n$𝑸_{i} = α + 𝜷_{i} × Gov + Controls + error$\n\n\nThe ideal is to establish estimates that allow you to infer that __changing Gov will _CAUSE_ a change in Q__. However, without a nice empirical design, we cannot infer causality\n\n\n_One source of bias is: reverse causation_\n\n- Perhaps it is Q that causes Gov\n\n- OLS based methods do not tell the difference between these two betas:\n\n$𝑄_{i} = α + 𝜷_{i} × Gov + Controls + error$\n\n$Gov_{i} = α + 𝜷_{i} × Q + Controls + error$\n\n- If one Beta is significant, the other will most likely be significant too.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Lags to mitigate reverse causation  {.smaller}\n\n In a regression model, there is always the possibility that it is Y that causes X, meaning that your assumptions about the relationship of these variables can always be wrong (i.e., what *causes* what?).\n\nWe are not discussing **causality** yet (which is a whole new chapter). But one possible simple solution is to use lagged values of your X. Something along the following lines:\n\n$$Y_{t,i} = \\alpha + \\beta \\times X_{t-1,i} + \\epsilon_{t,i}$$\n\nNotice the subscript $t-1$ in x now. It means that you are using the previous period's value of X as explanatory variable of the current period's value of Y. \n\nThis type of structure mitigates the concern that variations in Y are the reason of why X varies since it is less likely that the **current** variation of Y **provokes** variations on X in  the previous years (i.e., the idea is that the future does not affect the past).\n\n\n\n\n\n\n\n\n\n## Lags to mitigate reverse causation  {.smaller}\n\nThis is not a perfect solution because Y and X in most accounting and finance research designs are usually auto-correlated, meaning that the previous values are correlated with the present value. \n\nFor instance, the firm's leverage of 2015 is highly correlated with the firm's leverage of 2016. \n\nBut in many cases it is a good idea to use lag values. At least, you should have this solution in your toolbox. \n\n\n\n\n## Lags to mitigate reverse causation  {.smaller}\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nfe <- plm(w_lev1 ~ w_size1 + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"within\")\nlag <- plm(w_lev1 ~ lag(w_size1) + lag(w_fcf_ta) + lag(w_roa) + lag(w_tang_ta) + lag(w_cash_ta) + lag(w_div_ta) + factor(year) , data = data, model=\"within\")\nexport_summs(fe, lag, coefs = c(\"w_size1\",\"w_div_ta\",\"w_fcf_ta\",\"w_roa\",\"w_tang_ta\",\"w_cash_ta\",\n                              \"lag(w_size1)\",\"lag(w_div_ta)\",\"lag(w_fcf_ta)\", \"lag(w_roa)\",\"lag(w_tang_ta)\",\"lag(w_cash_ta)\"), digits = 3 , model.names = c(\"FE\",\"FE with lags\") )\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## First difference models {.smaller}\n\nAlternatively, we can estimate a first difference model as follows.\n\n$$\\Delta Y_{t,i} = \\alpha + \\beta \\times \\Delta X_{t,i} +\\ epsilon_{t,i}$$\n\nWhere, $\\Delta Y_{t,i} =Y_{t,i} - Y_{t-1,i}$   and $\\Delta X_{t,i} =X_{t,i} - X_{t-1,i}$ \n\n$\\beta$ shows the difference in the average change of Y for units that experience a change in x during the same period. \n\nSaying the same thing in a different way, $\\beta$ shows how much y changes, on average, where and when X increases by one unit. \n\nLet's say that x is binary and it changes from 0 to 1 for each firm during several different periods. So, y will change, on average, by $\\beta$ when x changes from 0 to 1. \n\n\n\n\n\n## First difference models {.smaller}\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nfe <- plm(w_lev1 ~ w_size1 + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"within\")\nfd <- plm(w_lev1 ~ w_size1 + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"fd\") \nexport_summs(fe, fd, coefs = c(\"w_size1\",\"w_div_ta\",\"w_fcf_ta\",\"w_roa\",\"w_tang_ta\",\"w_cash_ta\")  , digits = 3, model.names = c(\"FE\",\"FD\"))\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Binary variables {.smaller}\n\nAs mentioned before, a Fixed effect is equivalent to a binary variable marking one group of observations. For instance, all observations of the same year, or from the same firm.\n\nWe can explore many interesting types of binary variables in most cases of corporate finance. For instance, whether the firm is included in \"Novo Mercado\", if the firm has high levels of ESG, etc. \n\nThe implementation of a binary variable is quite simple: it takes the value of  0 for one group, and 1 for the other.\n\nThe interpretation is a bit trickier. \n\n\n## Binary variables {.smaller}\n\n\nLet's think about the example 7.1 of Wooldridge. He estimates the following equation:\n\n$$wage = \\beta_0 + \\delta_1 female + \\beta_1 educ + \\mu$$\n\n*In model (7.1), only two observed factors affect wage: gender and education. Because $female = 1$ when the person is female, and $female = 0 $ when the person is male, the parameter $\\delta_1$ has the following interpretation: *\n\n*- $\\delta_1$  is the difference in hourly wage between females and males, given the same amount of education (and the same error term u). Thus, the coefficient $\\delta_1$  determines whether there is discrimination against women: if $\\delta_1<0$, then, for the same level of other factors, women earn less than men on average.*\n\n\n\n\n\n## Binary variables {.smaller}\n\nIn terms of expectations, if we assume the zero conditional mean assumption E($\\mu$ | female,educ) = 0, then \n\n$\\delta_1 = E(wage | female = 1, educ) - E(wage | female = 0, educ)$ \n\nOr \n\n$\\delta_1 = E(wage | female, educ) - E(wage | male, educ)$ \n\n\n- The key here is that the level of education is the same in both expectations; the difference, $\\delta_1$ , is due to gender only.\n\n\n## Binary variables {.smaller}\n\n\nThe visual interpretation is as follows. The situation can be depicted graphically as an intercept shift between males and females. The interpretation relies on $\\delta_1$. We can observe that $delta_1 < 0$; this is an argument for existence of a gender gap in wage.\n \n\n![](files/wooldridge_7_1.png)\n\n\n\n\n\n\n\n## Binary variables {.smaller}\n\nUsing our own example, we can make the case that it is necessary to separate the firms in two groups: dividend payers and non-payers. \n\nThere is literature suggesting that dividend payers are not financially constrained, while those firms that do not pay dividends are. \n\n**If financial constrain is something important to our model (and assuming that the right way to control for it is by including a dividend payer dummy), we should include such a dummy.**\n\n\n\n\n\n\n## Binary variables {.smaller}\n\n$$Lev_{t,i} = \\alpha + \\beta_1 \\times Size_{t,i} + \\beta_2 \\times Div.\\;payer_{t,i} + controls + \\epsilon_{t,i}$$\n\nWe could estimate this model as follows. \n\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\ndata$w_div_payer <- ifelse(data$w_div_ta <= 0, 0, 1)\ntapply(data$w_div_ta, data$w_div_payer, summary)    # Summary by group using tapply\nfe1 <- plm(w_lev1 ~ w_size1 + w_div_ta + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta  + factor(year) , data = data, model=\"within\")\nfe2 <- plm(w_lev1 ~ w_size1 + w_div_payer + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta  + factor(year) , data = data, model=\"within\")\nexport_summs(fe1, fe2, coefs = c(\"w_size1\",\"w_div_ta\",\"w_div_payer\",\"w_fcf_ta\",\"w_roa\",\"w_tang_ta\",\"w_cash_ta\")  , digits = 3, model.names = c(\"FE 1\",\"FE 2\"))\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Models with squared variables {.smaller}\n\nLet's say you have a variable that should not show a clear linear relationship with another variable. \n\nFor instance, consider ownership concentration and firm value. There is a case to be made the relationship between these variable is not linear. That is, in low levels of ownership concentration (let's say 5% of shares), a small increase in it might lead to an increase in firm value. The argument is that, in such levels, an increase in ownership concentration will lead the shareholder to monitor more the management maximizing the likelihood of value increasing  decisions.\n\nBut consider now the case where the shareholder has 60% or more of the firm's outstanding shares. If you increase further the concentration it might signals the market that this shareholder is too powerful that might start using the firm to personal benefits (which will not be shared with minorities). \n\n\n\n\n\n\n\n## Models with squared variables {.smaller}\n\n\nIf this story is true, the relationship is (inverse) u-shaped. That is, at first the relationship is positive, then becomes negative.\n\nTheoretically, I could make an argument for a non-linear relationship between several variables of interest in finance. Let's say size and leverage. Small firms might not be able to issue too much debt as middle size firms. At the same time, large firms might not need debt. The empirical relationship might be non-linear. \n\nThere is always a potential case to be made regarding the relationship between the variables.\n\n\n\n## Models with squared variables {.smaller}\n\nThe way to empirically test it is as follows:\n\n\n$$Y_{t,i} = \\alpha + \\beta_1 \\times X_{t,i} + \\beta_2 \\times X^2_{t,i} + \\epsilon_{t,i}$$\n\nOr using the size-leverage example:\n\n$$Lev_{t,i} = \\alpha + \\beta_1 \\times Size_{t,i} + \\beta_2 \\times Size^2_{t,i} + controls + \\epsilon_{t,i}$$\nAs Wooldridge says, misspecifying the functional form of a model can certainly have serious consequences. But, in this specific case, the problem seems minor since we have the data to fix it. \n\n\n\n\n\n\n## Models with squared variables {.smaller}\n\nIn this specific case, our theory does not hold, since the squared term is not significant. So we can conclude that in this model, the relationship is linear.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\ndata$w_size1_sq <- data$w_size1 * data$w_size1 \nfe <- plm(w_lev1 ~ w_size1 + w_size1_sq + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"within\")\nexport_summs(fe, coefs = c(\"w_size1\",\"w_size1_sq\",\"w_div_ta\",\"w_fcf_ta\",\"w_roa\", \"w_tang_ta\",\"w_cash_ta\")  , digits = 3)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Models with Interactions {.smaller}\n\nIn some specific cases, you want to interact variables to test if the interacted effect is significant. For instance, you might believe that, using Wooldridge very traditional example 7.4., women that are married are yet more discriminated in the job market. So, you may prefer to estimate the following equation.\n\n\n$$wage = \\beta_0 + \\beta_1 female + \\beta_2 married + \\beta_3 female.married + \\mu$$\n\nWhere $maried$ is a binary variable marking all married people with 1, and 0 otherwise. \n\nIn this setting, the group of single men is the base case and is represented by $\\beta_0$. That is, both female and married are 0.\n\n- The group of single women is represented by $\\beta_0 + \\beta_1$. That is, female is 1 but married is 0.\n\n- The group of married men is represented by $\\beta_0 + \\beta_2$. That is, female is 0 but married is 1.\n\n- Finally, the group of married women is represented by $\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3$. That is, female and married are 1.\n\n\n## Models with Interactions {.smaller}\n\nUsing a random sample taken from the U.S. Current Population Survey for the year 1976, Wooldridge estimates that $female<0$, $married>0$, and $female.married<0$. This result makes sense for the 70s. \n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(wooldridge)\ndata('wage1')\nwage1$fem_mar <- wage1$female * wage1$married\nwage<- lm(lwage ~ female + married + fem_mar + educ + exper + expersq + tenure + tenursq , data = wage1)\nexport_summs(wage, coefs = c(\"(Intercept)\", \"female\",\"married\",\"fem_mar\") , digits = 3)\n```\n\n\n![](files/wooldridge_7_4.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Linear probability model {.smaller}\n\n\nWhen the dependent variable is binary we cannot rely on linear models as those discussed so far. We need a **linear probability model**. In such models, we are interested in how the probability of the occurrence of an event depends on the values of x. That is, we want to know $P[y=1|x]$.\n\nImagine that y is employment status, 0 for unemployed, 1 for employed. This is our Y. Imagine that we are interested in estimating the probability that a person start working after a training program. For these types of problem, we need a linear probability model.\n\n\n$$P[y=1|x] = \\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_kx_k$$\n\nThe mechanics of estimating these model is similar to before, except that Y is binary.\n\nThe interpretation of coefficients change. That is, **a change in x changes the probability of y = 1**. So, let's say that $\\beta_1$ is 0.05. It means that changing $x_1$ by one unit will change the probability of y = 1 (i.e., getting a job) in 5%, ceteris paribus. \n\n\n\n\n\n\n\n\n## Linear probability model {.smaller}\n\n\nUsing Wooldridge's example 7.29:\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(wooldridge)\ndata('mroz')\nlpm <- lm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = mroz)\nexport_summs(lpm, coefs = c(\"(Intercept)\", \"nwifeinc\" , \"educ\" , \"exper\" , \"expersq\" , \"age\"  ,\"kidslt6\" , \"kidsge6\"), digits = 3 , model.names = c(\"LPM\"))\n```\n\n\n\n\n## Linear probability model {.smaller}\n\n![](files/wooldridge_7_29.png)\n\n\nThe relationship between the probability of labor force participation and educ is plotted in the figure below. Fixing the other independent variables at 50, 5, 30, 1 and 6, respectively, the predicted probability is negative until education equals 3.84 years. This is odd, since the model is predicting negative probability of employment given a set of specific values. \n\n\n\n![](files/wooldridge_7_3.png)\n\n\n## Linear probability model {.smaller}\n\n\nAnother example. The model is predicting that *going from 0 to 4 kids less than 6 years old* reduces the probability of working by $4\\times 0.262 = 1.048$, which is impossible since it is higher than 1.\n\n\nThat is, **one important caveat of a linear probability model is that probabilities might falls off of expected empirical values**. If this is troublesome to us, we might need a different solution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Logit and Probit {.smaller}\n\nAlthough the linear probability model is simple to estimate and use, it has some limitations as discussed. If that problem is important to us, we need a solution that addresses the problem of  negative or higher than 1 probability. That is, **we need a binary response model**.\n\nIn a binary response model, interest relies on the response probability.\n\n$$P(y =1 | x) = P(y=1| x_1,x_2,x_3,...)$$\n\nThat is, we have a group of X variables explaining Y, which is binary. In a LPM, we assume that the response probability is linear in the parameters $\\beta$. This is the assumption that created the problem discussed above.\n\n\n\n\n\n## Logit and Probit {.smaller}\n\nWe can change that assumption to a different function. \n\n- A **logit** model assumes a logistic function ($G(Z)=\\frac{exp(z)}{[1+exp(z)]}$)\n- A **probit** model assumes a standard normal cumulative distribution function ($\\int_{-inf}^{+z}\\phi(v)dv$)\n\nThe adjustment is something as follows.\n\n$$P(y =1 | x) = G(\\beta_0 + \\beta_1 x_1+ \\beta_2 x_2 + \\beta_3 x_3)$$\nWhere G is either the logistic (logit) or the normal (probit) function. \n\n\n**We don't need to memorize these functions**, but we need to understand the adjustment that assuming a different function makes. Basically, **we will not predict negative or above 1 values anymore because the function adjusts at very low and very high values*. \n\n\n## Logit and Probit {.smaller}\n\n\n![](files/wooldridge_17_1.png)\n\n\n\n## Logit and Probit {.smaller}\n\nLet's estimate a logit and probit to compare with the LPM.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlpm <- lm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = mroz)\nlogit <- glm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = mroz,family = binomial)\nprobit <- glm(inlf ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = mroz, family = binomial(link = probit))\nexport_summs(lpm , logit, probit, coefs = c( \"nwifeinc\" , \"educ\" , \"exper\" , \"expersq\" , \"age\"  ,\"kidslt6\" , \"kidsge6\", \"(Intercept)\"), digits = 4 , model.names = c(\"LPM\",\"Logit\",\"Probit\"))\n```\n\n\n\n## Logit and Probit {.smaller}\n\n\nImportantly, in a LPM model, the coefficients have similar interpretations as usual. But logit and probit models lead to harder to interpret coefficients. \n\nIn fact, often we do not make any interpretation of these coefficients. Instead, we usually transform them to arrive at an interpretation that is similar to what we have in LPM. \n\nTo make the magnitudes of probit and logit roughly comparable, we can multiply the probit coefficients by 1.6, or we can multiply the logit estimates by .625. \n\nAlso, the probit slope estimates can be divided by 2.5 to make them comparable to the LPM estimates.\n\nAt the end of the day, the interpretation of the logit and probit outputs are similar to LPM's.  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Tobit {.smaller}\n\nAnother problem in the dependent variable occurs when we have a **limited dependent variable** with a corner solution. \n\n- That is, a variable that ranges from zero to all positive values. \n\n- For instance, hours working. Nobody works less than zero hours, but individuals in the population can work many number of positive hours. \n\n- When we have such type of dependent variable, we need to estimate a **tobit** model.\n\n\n\n\n\n\n## Tobit {.smaller}\n\nUsing Wooldridge's example 17.2.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(AER)\nlpm <- lm(hours ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = mroz)\ntobit <- tobit(hours ~ nwifeinc + educ + exper + expersq + age + kidslt6 + kidsge6  , data = mroz)\nsummary(tobit)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# OLS concerns {.smaller}\n\n\n## Heteroscedasticity {.smaller}\n\nOne of the key assumptions in OLS estimators is that $var(\\mu|x_1,x_2,x_3,...) = \\sigma^2$. That is, the assumption is that the variance of the errors is homoskedastic (present constant variance). It means that throughout all observations, the error term shows the same variance $\\sigma^2$. If errors are not homoskedastic, we have the **Heteroscedasticity problem**.\n\nHeteroskedasticity does not cause bias or inconsistency in the OLS estimators of the $\\beta$ like the OVB would. It also does not affect the $R^2$. What **Heteroscedasticity does is to bias the standard errors of the estimates**.\n\nRemember again that $t_{\\beta} = \\frac{\\hat{\\beta}}{se(\\hat{\\beta})}$. So, if you have biased standard errors, you will not assess correctly the significance of your coefficients. It also affects the F statistics.\n\nGraphically, we can think as follows.\n\n\n\n\n\n## Heteroscedasticity {.smaller}\n\nExample of homoscedasticity: \n\n\n\n\n![](files/homoscedasticity.png)\n\n## Heteroscedasticity {.smaller}\n\nExample of heteroscedasticity: \n\n![](files/heteroscedasticity.png)\n\n\n\n\n## Heteroscedasticity {.smaller}\n\nTo give you more context, think in terms of the relationship that we've discussing $leverage=f(size)$. \n\nIt is quite possible that small firms will have less options of leverage than large companies. \n\nThis means that a subsample of large companies will have higher variance in the leverage decisions (and thus the error terms) than the subsample of small firms. \n\nSo, we need to correct somehow the heteroskedasticity problem to find unbiased standard errors for the independent variable size in this model. \n\n\n\n\n## Heteroscedasticity {.smaller}\n\nThe solution to this problem is to estimate **Robust standard errors**. Basically, we will need to change the estimator of the standard error to an unbiased version.\n\nWe used to estimate (theoretical, populational):\n\n$$var(\\hat\\beta_1) = \\frac{\\sum_{i=1}^n(x_1-\\bar{x}^2) \\sigma^2_i}{SST^2_x}$$\n\nBut now we would estimate (sample).  \n\n$$var(\\hat\\beta_1) = \\frac{\\sum_{i=1}^n(x_1-\\bar{x}^2) \\mu^2_i}{SST^2_x}$$\n\nThis is called *White-Robust standard error* or the *Heteroscedasticity-Robust standard error* and was first showed by White (1980).\n\n\n\n\n\n## Heteroscedasticity {.smaller}\n\nBefore we estimate a model with robust standard errors, let's visually check if there is heteroskedasticity in the errors of the model. I am using Wooldridge's example 8.1.\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: slide\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nlibrary(dplyr)\nlibrary(sandwich)\nlibrary(lmtest) \nwage1<-wage1  %>%  mutate(marmale = case_when(female == 0 & married == 1 ~ 1,\n                                              female == 0 & married == 0 ~ 0,\n                                              female == 1 & married == 1 ~ 0,\n                                              female == 1 & married == 0 ~ 0) )\nwage1<-wage1  %>%  mutate(marrfem = case_when(female == 0 & married == 1 ~ 0,\n                                              female == 0 & married == 0 ~ 0,\n                                              female == 1 & married == 1 ~ 1,\n                                              female == 1 & married == 0 ~ 0) )\nwage1<-wage1  %>%  mutate(singfem = case_when(female == 0 & married == 1 ~ 0,\n                                              female == 0 & married == 0 ~ 0,\n                                              female == 1 & married == 1 ~ 0,\n                                              female == 1 & married == 0 ~ 1) )\nwage_t <- lm(lwage ~ marmale + marrfem + singfem + educ + exper + expersq + tenure + tenursq , data = wage1)\n\nlibrary(tidyverse)\nlibrary(broom)\nfitted_data <- augment(wage_t, data = wage1)\nggplot(fitted_data, aes(x = .fitted, y = .resid)) +  geom_point() + geom_smooth(method = \"lm\") + theme_solarized()\n```\n\n\n\n\n## Heteroscedasticity {.smaller}\n\n\nVisually, we do not see much variation in the error term throughout the x axis. Not much evidence of  heteroskedasticity. \n\nBut let's formally test it using the Breusch-Pagan test.\n\nThe H_0 of this test is for homoskedasticity. Thus, **if we reject the test, heteroskedasticity is present.** \n\nThe estimated p-value is 10.55%, above the usual levels. We can conclude that heteroskedasticity is not present is such model. \n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nbptest(wage_t)\n```\n\n\n\n## Heteroscedasticity {.smaller}\n\n\nLet's estimate both standard errors to see their difference. Because we do not see much heteroskedasticity we should not see much difference in the estimated standard errors. \n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nwage_r <- coeftest(wage_t, vcov = vcovHC)\nexport_summs(wage_t, wage_r, coefs = c(\"(Intercept)\", \"marmale\",\"marrfem\",\"singfem\",  \"educ\" , \"exper\" ,\"expersq\" , \"tenure\" , \"tenursq\" ) , digits = 4 ,model.names = c(\"Traditional S.E.\", \"Robust S.E.\"))\n```\n\n\n## Heteroscedasticity {.smaller}\n\n\nNotice that the standard errors have changed a bit, but not too much in this example. \n\nUsually, the robust standard errors are larger than the traditional ones in empirical works, but they could be smaller. \n\nAlso notice that most of the independent variables have similar significance levels, but some have less significance. \n\nThis is expected, since the robust standard errors are expected to be larger (but again not always). \n\nA final note, the standard errors estimates here are a bit different than those in Wooldridge I am assuming this is due to the package that I am using.\n\n**Finally, it is often common and most widely accepted to estimate the robust standard errors instead of the traditional ones.** \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Multicollinearity {.smaller}\n\nMulticollinearity is the term used when many of the independent variables in a model are correlated. \n\nThis problem is not clearly formulated in econometrics book due to its nature. \n\nBut one thing is evident: **multicollinearity increases the standard errors of the coefficients**\n\nIn a sense, this problem is similar to having a small sample, from which is hard (in a statistical sense, meaning high standard deviation) to estimate the coefficient.\n\nAs Wooldridge says: *everything else being equal, for estimating $\\beta_j$, it is better to have less correlation between $x_j$ and the other independent variables.* \n\n\n## Multicollinearity {.smaller}\n\n\nWe can use the following test to verify if multicollinearity exists: **variance inflation factor (VIF)**. \n\nThere is no formal threshold to interpret the test, but 10 is usually accepted as appropriate. \n\nThat is, if there is one or more variables showing a VIF of 10 or higher, the interpretation is that there is evidence of multicollinearity.\n\n\n\n\n\n\n\n## Multicollinearity {.smaller}\n\nWe can observe below that the experience variables are multicolinear. This is expected since one is the squared root of the other.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nvif(wage_t)\n```\nAssuming that expersq is important to be included in the model, there is not much we can do in this example. \n\nSome scholars argue that we could ignore the multicollinearity altogether, others would argue to exclude expersq if possible.\n\nThe general most accepted solution is to **keep multicolinear variables if they are control variables**. \n\nThat is, if your focus is on any other variable, you can keep the multicolinear ones and move on. If the multicolinear variable is one of the interest variables, you might want to discuss dropping the other one. \n\n\n\n\n\n## Multicollinearity {.smaller}\n\nLet's investigate our previous OLS model. We see no evidence of multicollinearity.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\nols <- lm(w_lev1 ~ w_size1 + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data)\nvif(ols)\n```\n\n\n\n\n\n\n\n##  Measurement error {.smaller}\n\n**The measurement error problem has a similar statistical structure to the omitted variable bias (OVB).**\n\n- \"Classical\" random measurement error for the outcome will inflate standard errors but will not lead to biased coefficients. \n\n    - $y^{*} = y + \\sigma_{1}$\n    - If you estimante $y^{*} = f(x)$, you have $y + \\sigma_{1} = x + \\epsilon$ \n    - $y = x + u$ \n        - where $u = \\epsilon + \\sigma_{1}$ \n\n##  Measurement error {.smaller}\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\ndata$w_lev1_noise <- data$w_lev1 + runif(length((data$w_lev1)), min=-0.2, max= 0.2)\n\nfe <- plm(w_lev1 ~ w_size1 + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"within\")\nfe_noise <- plm(w_lev1_noise  ~ w_size1 + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"within\")\n\nexport_summs(fe, fe_noise, coefs = c(\"w_size1\",\"w_div_ta\",\"w_fcf_ta\",\"w_roa\",\"w_tang_ta\",\"w_cash_ta\"), digits = 3 , model.names = c(\"Size\",\"Lev with noise\") )\n```\n\n##  Measurement error {.smaller}\n\n- \"Classical\" random measurement error in x’s will bias coefficient estimates toward zero\n\n    - $x^{*} = x + \\sigma_{2}$\n    - Imagine that $x^{*}$ is a bunch of noise\n    - It would not explain anything\n    - Thus, your results are biased toward zero\n\n\n\n##  Measurement error {.smaller}\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\ndata$w_size1_noise <- data$w_size1 + runif(length((data$w_size1)), min=-2, max= 2)\n\nfe <- plm(w_lev1 ~ w_size1 + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"within\")\nfe_noise <- plm(w_lev1 ~ w_size1_noise + w_fcf_ta + w_roa + w_tang_ta + w_cash_ta + w_div_ta + factor(year) , data = data, model=\"within\")\n\nexport_summs(fe, fe_noise, coefs = c(\"w_size1\",\"w_size1_noise\",\"w_div_ta\",\"w_fcf_ta\",\"w_roa\",\"w_tang_ta\",\"w_cash_ta\"), digits = 3 , model.names = c(\"Size\",\"Size with noise\") )\n```\n\n\n\n\n\n\n\n##  Omitted  variable bias (OVB) {.smaller}\n\nLet's walk through an empirical example of the OVB problem. \n\n- Imagine that you do not include an important “true” predictor of Q:\n\n- Let's say, you have a long equation as:  $𝑸_{i} = 𝜶_{long} + 𝜷_{long}* gov_{i} + δ * omitted + error$\n\n- But you estimate a short version of the same equation missing one variable:  $𝑸_{i} = 𝜶_{short} + 𝜷_{short}* gov_{i} + error$\n\n- $𝜷_{short}$ will be: \n\n    - $𝜷_{short} = 𝜷_{long}$ +  bias\n\n    - $𝜷_{short} = 𝜷_{long}$ +  relationship between omitted (omitted) and included (Gov) * effect of omitted in long (δ)\n\n        - Where: relationship between omitted (omitted) and included (Gov) is: $Omitted = 𝜶 + ϕ *gov_{i} + u$\n\n- Thus, OVB is: $𝜷_{short} – 𝜷_{long} = ϕ * δ$\n\n\n##  Omitted  variable bias (OVB) {.smaller}\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\n\nlibrary(readxl)\novb <- read_excel(\"files/ovb.xlsx\")\nshort <- lm(performance ~ bad_decision  , data = ovb)\nlong <- lm(performance ~ bad_decision  + risky_firm, data = ovb)\nexport_summs(short, long, coefs = c(\"bad_decision\",\"risky_firm\"), digits = 3 , model.names = c(\"Short\",\"Long\") )\n```\n\n\n\n##  Omitted  variable bias (OVB) {.smaller}\n\n\nIn this example, we can say that the OVB is $short = long + bias$.\n\nThat is, $0.44535  = -0.38389 + bias$, or $0.44535  = -0.38389 + 0.82924$. \n\nWhich is the same as: $0.44535  = -0.38389 + phi (omitted = f(non-omitted)) * omega (beta\\; \\; omitted \\;in\\; long)$\n\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\novbmodel <- lm(risky_firm ~bad_decision , data = ovb )\n# The OVB is 0.44535  = -0.38389 + 1.25146 * 0.66262\nmatrix1<- summary(long)$coefficients\nmatrix2<- summary(ovbmodel)$coefficients\n# Calculating OVB\nsum(matrix1[3,1] * matrix2[2,1]) \n```\n\n\n##  Omitted  variable bias (OVB) {.smaller}\n\n\nWe can see that omitting the variable \"risky_firm\" is problematic since it seems to explain the outcome of this regression.\n\n\n```{r}\n#| warning: false\n#| message: false\n#| fig-align: center\n#| echo: true\n#| output-location: default\n#| code-fold: false\n#| code-summary: \"R\"\n#| code-line-numbers: true\n#| eval: true\ntapply(ovb$performance, ovb$risky_firm, summary)\ntapply(ovb$bad_decision, ovb$risky_firm, summary)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n## 🙋‍♂️ **Any Questions?**{ .smaller  background=\"#fdf6e3\"}\n\n::: columns\n::: {.column width=\"50%\"}\n### Thank You!\n\n![](figs/qa2.png){width=\"110%\" style=\"box-shadow: none;\"}\n\n:::\n\n::: {.column width=\"50%\"}\n<div style=\"text-align:right;\">\n  <img src=\"figs/avatar.jpg\" width=\"120px\" style=\"border-radius:50%; box-shadow:0 4px 12px rgba(0,0,0,.25);\" />\n</div>\n\n### **Henrique C. Martins**\n\n- 🌐 [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)  \n- 💼 [LinkedIn](https://www.linkedin.com/in/henriquecastror/)  \n- 🧠 [Google Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)  \n- 📄 [Lattes CV](http://lattes.cnpq.br/6076997472159785)  \n- 🏠 [Personal Website](https://henriquemartins.net/)  \n:::\n:::\n\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","css":["logo.css"],"output-file":"part_O7.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.4.549","auto-stretch":true,"html":{"css":"webex.css","include-after-body":"webex.js"},"editor":"visual","title":"Empirical Methods in Finance","subtitle":"Practicing 7","author":"Henrique C. Martins","title-slide-attributes":{"data-background-color":"#b1cafa"},"include-after":["<script type=\"text/javascript\">\n  Reveal.on('ready', event => {\n    if (event.indexh === 0) {\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n  });\n  Reveal.addEventListener('slidechanged', (event) => {\n    if (event.indexh === 0) {\n      Reveal.configure({ slideNumber: null });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n    if (event.indexh === 1) {\n      Reveal.configure({ slideNumber: 'c' });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n    }\n  });\n</script>\n"],"slideNumber":true,"theme":"simple","chalkboard":true,"previewLinks":"auto","logo":"figs/background8.png","footer":"**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  ","multiplex":true,"scrollable":true}}},"projectFormats":["html"]}