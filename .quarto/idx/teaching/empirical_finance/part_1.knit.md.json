{"title":"Empirical Methods in Finance","markdown":{"yaml":{"title":"Empirical Methods in Finance","subtitle":"Part 1","author":"Henrique C. Martins","format":{"revealjs":{"slide-number":true,"theme":"simple","chalkboard":true,"preview-links":"auto","logo":"figs/background8.png","css":"logo.css","footer":"**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  ","multiplex":true,"scrollable":true}},"title-slide-attributes":{"data-background-color":"#b1cafa"},"include-after":"<script type=\"text/javascript\">\n  Reveal.on('ready', event => {\n    if (event.indexh === 0) {\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n  });\n  Reveal.addEventListener('slidechanged', (event) => {\n    if (event.indexh === 0) {\n      Reveal.configure({ slideNumber: null });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n    if (event.indexh === 1) {\n      Reveal.configure({ slideNumber: 'c' });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n    }\n  });\n</script>\n"},"headingText":"Agenda","containsRefs":false,"markdown":"\n\n\n\n\n\n\n## Agenda {.smaller}\n\n- Apresenta√ß√£o do syllabus do curso\n  - Apresenta√ß√£o dos crit√©rios de avalia√ß√£o\n  \n. . .\n\n- Breve apresenta√ß√£o dos temas de pesquisa e discuss√£o inicial sobre a entrega final\n\n. . .\n\n- In√≠cio do conte√∫do\n  - Introdu√ß√£o a causalidade\n\n\n\n\n\n\n\n\n\n## Sobre a letter  {.smaller visibility=\"hidden\"}\n\n- Formato letter\n  - Entre 2k e 2.5k palavras a depender do journal.\n  \n*The objective of a letter is to facilitate the rapid dissemination of important research that contains an insight, new data, or discuss current important topic.*\n  \n- Ir√° requerer todas as etapas da pesquisa (com √™nfase na an√°lise dos dados, i.e., regress√µes).\n\n- Idealmente, ser√° submetida com o/a orientador/a. Leia-se, sua miss√£o √© \"convencer\" de que o trabalho final √© submet√≠vel a uma revista. \n\n\n\n\n\n\n\n\n\n## Sobre a letter  {.smaller visibility=\"hidden\"}\n\n- Op√ß√µes de revistas que aceitam letter (checar se refer√™ncias e tabelas fazem parte do word count):\n\n  - [Economic Letters](https://www.sciencedirect.com/journal/economics-letters) (ABS3): 2k palavras\n  - [Journal of Accounting and Public Policy](https://www.sciencedirect.com/journal/journal-of-accounting-and-public-policy) (ABS3): 3k palavras\n  - [Finance Research Letters](https://www.sciencedirect.com/journal/finance-research-letters) (ABS2): 2.5k palavras\n  - [Applied Economic Letters](https://www.tandfonline.com/journals/rael20) (ABS1): 2k palavras\n  - [Brazilian Review of Finance](https://periodicos.fgv.br/rbfin) (A4): [4k palavras](https://periodicos.fgv.br/rbfin/libraryFiles/downloadPublic/140) \n  \n* Voc√™ √© bem-vindo/a para propor outro journal que aceite letter, sob condi√ß√£o de valida√ß√£o junto ao instrutor. \n  \n\n\n\n\n\n## Stata {.smaller}\n\n**Providenciar instala√ß√£o para pr√≥ximo encontro**.\n\nPara instala√ß√£o do Stata, seguir instru√ß√µes da TI. \n\n\n\n\n\n\n\n## R {.smaller}\n\n**Providenciar instala√ß√£o para pr√≥ximo encontro**.\n\n\nInstall R [here Win](https://cran.r-project.org/bin/windows/base/)\n\nInstall R [here Mac](https://cran.r-project.org/bin/macosx/)\n\nInstall R Studio [here](https://posit.co/download/rstudio-desktop/)\n\n. . .\n\nPara instalar e carregar os pacotes voc√™ precisa rodar as duas linhas abaixo.\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"R\" code-line-numbers=\"true\"}\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n```\n:::\n\n\n\n\n## Python {.smaller}\n\n**I might show some code in python, but I cannot offer you support on it.**\n\n\n\n\n\n\n# Selection bias  {.smaller background=\"#fadea7\"} \n\n##  {.smaller background=\"#fadea7\"} \n\n\n![](figs/slides4-airplane.png)\n\n\n\n\n\n\n##  {.smaller background=\"#fadea7\"} \n\n![](figs/slides4-path1.jpg)\n\n\n**Voc√™ nunca sabe o resultado do caminho que n√£o toma.**\n\n\n\n\n\n\n\n\n\n\n\n\n## Quais as aplica√ß√µes do que vamos discutir? {.smaller background=\"#fadea7\"} \n\nH√° uma s√©rie de **quest√µes de pesquisa** que poderiam ser investigadas com as ferramentas que vamos discutir hoje.\n\n::: incremental\n\n1) Vale mais a pena estudar em escola particular ou p√∫blica?\n\n2) Qual o efeito de investimentos de marketing t√™m na lucratividade?\n\n3) Qual o efeito que jornadas de 4 dias semanais t√™m na produtividade?\n\n4) Qual efeito que educa√ß√£o tem na remunera√ß√£o futura?\n\n5) E diversas outras semelhantes...\n\n:::\n\n\n\n\n\n\n## Antes de come√ßar: Nossa agenda {.smaller background=\"#fadea7\"} \n\n\n::: incremental \n\n1) Introdu√ß√£o a **pesquisa quantitativa**\n\n2) Validade **Externa** vs. Validade **Interna**\n\n3) **Problemas** em pesquisa quantitativa inferencial\n\n4) **Rem√©dios**\n\n:::\n\n\n\n\n\n\n        \n\n\n## Introdu√ß√£o {.smaller background=\"#fadea7\"} \n\n**O que fazemos em pesquisa quantitiva?** Seguimos o m√©todo de pesquisa tradicional (com ajustes):\n\n::: incremental\n\n- Observa√ß√£o \n\n- Quest√£o de pesquisa \n\n- Modelo te√≥rico (abstrato)\n\n- Hip√≥teses\n\n- Modelo emp√≠rico\n\n- Coleta de dados \n\n- An√°lise do resultado do modelo (diferente de an√°lise de dados \"pura\")\n\n- Conclus√£o/desdobramentos/aprendizados\n  \n:::\n\n\n\n\n\n\n\n\n\n## Introdu√ß√£o {.smaller background=\"#fadea7\"} \n\n**O que fazemos em pesquisa quantitiva?** Seguimos o m√©todo de pesquisa tradicional (com ajustes):\n\n\n- Observa√ß√£o \n\n- Quest√£o de pesquisa \n\n- Modelo te√≥rico (abstrato): **Aqui √© onde a matem√°tica √© necess√°ria**\n\n- Hip√≥teses\n\n- Modelo emp√≠rico: **Estat√≠stica e econometria necess√°rias**\n\n- Coleta de dados: **Geralmente secund√°rios**\n\n- An√°lise do resultado do modelo (diferente de an√°lise de dados \"pura\")\n\n- Conclus√£o/desdobramentos/aprendizados\n\n\n\n\n\n\n\n. . .\n\n## Defini√ß√£o {.smaller background=\"#fadea7\"} \n\n**_Pesquisa quantitativa busca testar hip√≥teses..._**\n\n. . .\n\n**_...a partir da defini√ß√£o de modelos formais (abstratos)..._**\n\n. . .\n\n**_...de onde se estimam modelos emp√≠ricos utilizando a estat√≠stica e a econometria como mecanismos/instrumentos._**\n\n. . .\n\n\nNo fim do dia, buscamos **entender as rela√ß√µes** (que tenham **validade interna** e que ofere√ßam **validade externa**) entre diferentes **vari√°veis de interesse.**\n\n\n\n\n\n\n\n\n\n\n\n## Quais as vantagens? {.smaller background=\"#fadea7\"} \n\n1) **Validade externa:** \n\n. . .\n\n- Conceito de que, se a pesquisa tem validade externa, os seus **achados s√£o representativos**.\n\n. . .\n\n- I.e., s√£o **v√°lidos al√©m do seu modelo**. Resultados \"valem externamente\".\n\n. . .\n\n- Idealmente, buscamos resultados que valem externamente para **acumular conhecimento**...\n\n. . .\n\n- ...naturalmente, nem toda pesquisa quantitativa oferece validade externa. A pesquisa √≥tima sim. **A pesquisa excelente tem validade externa para al√©m do seu tempo**.\n\n. . .\n\n- Pesquisa qualitativa dificilmente oferece **validade externa**.\n\n\n\n\n\n\n\n\n\n\n## Quais as armadilhas? {.smaller background=\"#fadea7\"} \n\n\n2) **Validade interna:** \n\n. . .\n\n- Conceito de que a pesquisa precisa de validade interna para que seus **resultados sejam cr√≠veis**.\n\n. . .\n\n- I.e., os **resultados n√£o podem conter erros**, vieses, problemas de estima√ß√£o, problemas nos dados, etc..\n\n. . .\n\n- √â aqui que a gente separa a pesquisa ruim da pesquisa boa. Para ser levada a s√©rio, a pesquisa **PRECISA** ter validade interna.\n\n. . .\n\n- Mas isso, nem sempre √© trivial. Muitas pesquisas que vemos publicadas, mesmo em top journals, **n√£o t√™m validade interna** (seja por erro do pesquisador, por m√©todo incorreto, por falta de dados...)\n\n. . .\n\n- Mas cada vez mais, **avaliadores est√£o de olho** em problemas e em modelos  **Trash-in-Trash-out**\n\n\n\n\n\n\n\n\n\n\n\n## Como fazemos na pr√°tica? {.smaller background=\"#fadea7\"} \n\nExemplo de modelo emp√≠rico:\n\n$Y_{i} = Œ± + ùú∑_{1} √ó X_i + Controls + error$\n\n. . .\n\n<img src=\"figs/slides4-ols.jpg\" width=\"30%\" align=\"right\" />\n\n. . .\n\nUma vez que estimemos esse modelo, temos o **valor**, o **sinal** e a **signific√¢ncia** do $ùú∑$.\n\n. . .\n\nSe o Beta for **significativamente diferente de zero** e **positivo** --> X e Y est√£o positivamente correlacionados.\n\n. . .\n\n**O problema?** Os pacotes estat√≠sticos que utilizamos **sempre \"cospem\" um beta**. Seja ele com ou sem vi√©s.\n\n. . .\n\nCabe ao pesquisador ter um **design emp√≠rico** que garanta que o beta estimado tenha validade interna.\n\n\n\n\n\n## Como fazemos na pr√°tica? {.smaller background=\"#fadea7\"} \n\n\n<img src=\"figs/slides4-table.png\" width=\"110%\" align=\"center\" />\n\nA decis√£o final √© baseada na signific√¢ncia do Beta estimado. Se **significativo**, as vari√°veis s√£o relacionadas e fazemos infer√™ncias em cima disso.\n\nContudo, **sem um design emp√≠rico inteligente**, o beta encontrado pode ter literalmente qualquer sinal e signific√¢ncia.\n\n\n\n\n\n\n\n\n\n\n\n\n## Exemplo desses problemas {.smaller background=\"#fadea7\"} \n\nVeja esse [site](http://www.tylervigen.com/spurious-correlations).\n\n<img src=\"figs/slides4-spurius1.png\" width=\"100%\" align=\"center\" />\n\n\n\n\n\n## Exemplo desses problemas {.smaller background=\"#fadea7\"} \n\nVeja esse [site](http://www.tylervigen.com/spurious-correlations).\n\n<img src=\"figs/slides4-spurius2.png\" width=\"110%\" align=\"center\" />\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Selection bias - We see I {.smaller background=\"#fadea7\"} \n\n\n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(data.table)\nlibrary(ggplot2)\n# Generate Data\nn = 10000\nset.seed(100)\nx <- rnorm(n)\ny <- rnorm(n)\ndata1 <- 1/(1+exp( 2 - x  -  y))\ngroup  <- rbinom(n, 1, data1)\n\n# Data Together\ndata_we_see     <- subset(data.table(x, y, group), group==1)\ndata_all        <- data.table(x, y, group)\n\n# Graphs\nggplot(data_we_see, aes(x = x, y = y)) + \n      geom_point(aes(colour = factor(-group)), size = 1) +\n      geom_smooth(method=lm, se=FALSE, fullrange=FALSE)+\n      labs( y = \"\", x=\"\", title = \"The observations we see\")+\n      xlim(-3,4)+ ylim(-3,4)+ \n      theme(plot.title = element_text(color=\"black\", size=30, face=\"bold\"),\n            panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n            axis.text.y = element_text(face=\"bold\", color=\"black\", size = 18),\n            axis.text.x = element_text(face=\"bold\", color=\"black\", size = 18),\n            legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nn = 10000\nnp.random.seed(100)\nx = np.random.normal(size=n)\ny = np.random.normal(size=n)\ndata1 = 1 / (1 + np.exp(2 - x - y))\ngroup = np.random.binomial(1, data1, n)\n\ndata_we_see = pd.DataFrame({'x': x[group == 1], 'y': y[group == 1], 'group': group[group == 1]})\ndata_all = pd.DataFrame({'x': x, 'y': y, 'group': group})\n\nsns.set(style='whitegrid')\nplt.figure(figsize=(7, 5))\nplt.scatter(data_we_see['x'], data_we_see['y'], c=-data_we_see['group'], cmap='viridis', s=20)\nsns.regplot(x='x', y='y', data=data_we_see, scatter=False, ci=None, line_kws={'color': 'blue'})\nplt.title(\"The observations we see\", fontsize=18)\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nclear all\nset seed 100\nset obs 10000\ngen x = rnormal(0,1)\ngen y = rnormal(0,1)\ngen data1 = 1 / (1 + exp(2 - x - y))\ngen group = rbinomial(1, data1)\ntwoway (scatter x y if group == 1, mcolor(black) msize(small))    (lfit y x if group == 1, color(blue)),title(\"The observations we see\", size(large) ) xtitle(\"\") ytitle(\"\")\nquietly graph export figs/graph1.svg, replace\n```\n:::\n\n\n![](figs/graph1.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n## Selection bias - We see II  {.smaller background=\"#fadea7\"} \n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# Fit a linear regression model\nmodel <- lm(y ~ x, data = data_we_see)\n# Print the summary of the regression model\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, data = data_we_see)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.05878 -0.63754 -0.00276  0.62056  3.11374 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.72820    0.02660   27.37  < 2e-16 ***\nx           -0.14773    0.02327   -6.35 2.75e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9113 on 1747 degrees of freedom\nMultiple R-squared:  0.02256,\tAdjusted R-squared:  0.022 \nF-statistic: 40.32 on 1 and 1747 DF,  p-value: 2.746e-10\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport statsmodels.api as sm\nimport pandas as pd\nn = 10000\nnp.random.seed(100)\nx = np.random.normal(size=n)\ny = np.random.normal(size=n)\ndata1 = 1 / (1 + np.exp(2 - x - y))\ngroup = np.random.binomial(1, data1, n)\n\ndata_we_see = pd.DataFrame({'x': x[group == 1], 'y': y[group == 1], 'group': group[group == 1]})\ndata_all = pd.DataFrame({'x': x, 'y': y, 'group': group})\n\nX = data_we_see['x']  \nX = sm.add_constant(X)\ny = data_we_see['y']  \nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.018\nModel:                            OLS   Adj. R-squared:                  0.018\nMethod:                 Least Squares   F-statistic:                     33.84\nDate:                qua, 28 ago 2024   Prob (F-statistic):           7.06e-09\nTime:                        20:44:23   Log-Likelihood:                -2411.1\nNo. Observations:                1809   AIC:                             4826.\nDf Residuals:                    1807   BIC:                             4837.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.7037      0.026     26.826      0.000       0.652       0.755\nx             -0.1339      0.023     -5.817      0.000      -0.179      -0.089\n==============================================================================\nOmnibus:                        4.656   Durbin-Watson:                   1.973\nProb(Omnibus):                  0.097   Jarque-Bera (JB):                5.264\nSkew:                          -0.038   Prob(JB):                       0.0720\nKurtosis:                       3.253   Cond. No.                         1.93\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nclear all\nset seed 100\nset obs 10000\ngen x = rnormal(0,1)\ngen y = rnormal(0,1)\ngen data1 = 1 / (1 + exp(2 - x - y))\ngen group = rbinomial(1, data1)\nreg y x if group ==1\n\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of observations (_N) was 0, now 10,000.\n\n      Source |       SS           df       MS      Number of obs   =     1,872\n-------------+----------------------------------   F(1, 1870)      =     48.62\n       Model |  40.9398907         1  40.9398907   Prob > F        =    0.0000\n    Residual |  1574.57172     1,870  .842016963   R-squared       =    0.0253\n-------------+----------------------------------   Adj R-squared   =    0.0248\n       Total |  1615.51161     1,871  .863448215   Root MSE        =    .91761\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |  -.1579538   .0226526    -6.97   0.000    -.2023808   -.1135269\n       _cons |   .7202285   .0257215    28.00   0.000     .6697827    .7706744\n------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Selection bias - All I  {.smaller background=\"#fadea7\"} \n\n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nggplot(data_all, aes(x = x, y = y,  colour=group)) + \n  geom_point(aes(colour = factor(-group)), size = 1) +\n  geom_smooth(method=lm, se=FALSE, fullrange=FALSE)+\n  labs( y = \"\", x=\"\", title = \"All observations\")+\n  xlim(-3,4)+ ylim(-3,4)+ \n  theme(plot.title = element_text(color=\"black\", size=30, face=\"bold\"),\n      panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n      axis.text.y = element_text(face=\"bold\", color=\"black\", size = 18),\n      axis.text.x = element_text(face=\"bold\", color=\"black\", size = 18),\n      legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style='whitegrid')\nplt.figure(figsize=(6, 4))\nsns.scatterplot(data=data_all, x='x', y='y', hue='group', palette=['blue', 'red'], s=20)\nsns.regplot(data=data_all, x='x', y='y', scatter=False, ci=None, line_kws={'color': 'blue'})\nplt.title(\"All observations\", fontsize=18)\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.legend(title=\"Group\", labels=[\"0\", \"1\"], loc=\"upper left\")\n\nplt.gca().get_legend().remove()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of observations (_N) was 0, now 10,000.\n```\n\n\n:::\n:::\n\n\n![](figs/graph2.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n\n## Selection bias - All I {.smaller background=\"#fadea7\"} \n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nmodel2 <- lm(y ~ x, data = data_all)\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, data = data_all)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9515 -0.6716  0.0087  0.6698  3.9878 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept) -0.011825   0.009994  -1.183    0.237\nx           -0.003681   0.010048  -0.366    0.714\n\nResidual standard error: 0.9994 on 9998 degrees of freedom\nMultiple R-squared:  1.342e-05,\tAdjusted R-squared:  -8.66e-05 \nF-statistic: 0.1342 on 1 and 9998 DF,  p-value: 0.7141\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport statsmodels.api as sm\nimport pandas as pd\nn = 10000\nnp.random.seed(100)\nx = np.random.normal(size=n)\ny = np.random.normal(size=n)\ndata1 = 1 / (1 + np.exp(2 - x - y))\ngroup = np.random.binomial(1, data1, n)\n\ndata_we_see = pd.DataFrame({'x': x[group == 1], 'y': y[group == 1], 'group': group[group == 1]})\ndata_all = pd.DataFrame({'x': x, 'y': y, 'group': group})\n\nX = data_all['x']  \nX = sm.add_constant(X)\ny = data_all['y']  \nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     1.281\nDate:                qua, 28 ago 2024   Prob (F-statistic):              0.258\nTime:                        20:44:32   Log-Likelihood:                -14157.\nNo. Observations:               10000   AIC:                         2.832e+04\nDf Residuals:                    9998   BIC:                         2.833e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0003      0.010     -0.034      0.973      -0.020       0.019\nx             -0.0112      0.010     -1.132      0.258      -0.031       0.008\n==============================================================================\nOmnibus:                        0.267   Durbin-Watson:                   2.009\nProb(Omnibus):                  0.875   Jarque-Bera (JB):                0.242\nSkew:                           0.009   Prob(JB):                        0.886\nKurtosis:                       3.017   Cond. No.                         1.01\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nclear all\nset seed 100\nset obs 10000\ngen x = rnormal(0,1)\ngen y = rnormal(0,1)\ngen data1 = 1 / (1 + exp(2 - x - y))\ngen group = rbinomial(1, data1)\nreg y x \n\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of observations (_N) was 0, now 10,000.\n\n      Source |       SS           df       MS      Number of obs   =    10,000\n-------------+----------------------------------   F(1, 9998)      =      0.28\n       Model |  .284496142         1  .284496142   Prob > F        =    0.5938\n    Residual |  9999.04347     9,998  1.00010437   R-squared       =    0.0000\n-------------+----------------------------------   Adj R-squared   =   -0.0001\n       Total |  9999.32797     9,999   1.0000328   Root MSE        =    1.0001\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |  -.0053101    .009956    -0.53   0.594    -.0248259    .0142057\n       _cons |   .0006182   .0100006     0.06   0.951    -.0189849    .0202213\n------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n## Selection bias {.smaller background=\"#fadea7\"} \n\nSelection bias n√£o √© o √∫nico dos nossos problemas, mas √© um **importante**.\n\nVeja que suas conclus√µes mudaram significativamente.\n\nN√£o seria dif√≠cil criar um exemplo em que o **coeficiente verdadeiro** fosse positivo.\n\n\n\n\n\n\n\n\n\n\n\n## Exemplo desses problemas {.smaller background=\"#fadea7\"} \n\n![](figs/slides4-path2b.png) \n\n\nSource: [Angrist](https://www.youtube.com/watch?v=iPBV3BlV7jk)\n\n**N√£o podemos pegar dois caminhos.**\n\n\n\n\n\n\n\n\n\n\n## Exemplo desses problemas {.smaller background=\"#fadea7\"} \n\n![](figs/slides4-matching.png) \n\n\nSource: [Angrist](https://www.youtube.com/watch?v=6YrIDhaUQOE)\n\n**N√£o podemos comparar pessoas que n√£o s√£o compar√°veis.**\n\n\n\n\n\n\n\n\n## O que precisamos fazer? {.smaller background=\"#fadea7\"} \n\n. . .\n\nDefinir um bom **_Design emp√≠rico_**\n\n. . .\n\nNo mundo ideal: ter√≠amos **universos paralelos.** Ter√≠amos **dois clones**, em que cada um escolhe um caminho. Todo o resto √© igual.\n\n- Obviamente, isso n√£o existe.\n\n. . .\n\nSegunda melhor solu√ß√£o: **experimentos**\n\n. . .\n\n**Mas o que √© um experimento?**\n\n- Grupo de tratamento vs. Grupo de controle\n\n- Igualdade entre os grupos (i.e., aleatoriedade no sampling)\n\n    - Nada diferencia os grupos a n√£o ser o fato de que um indiv√≠duo recebe tratamento e o outro n√£o\n    - Estamos comparando ma√ßas com ma√ßas e laranjas com laranjas\n      \n- Testes placebo/falsifica√ß√£o.\n\n\n\n\n\n\n\n\n\n\n\n\n\n# The challenge {.smaller background=\"#b0aeae\"}\n\n## Correlation & Causality {.smaller background=\"#b0aeae\"}\n\n\nIt is very common these days to hear someone say ‚Äú*correlation does not mean causality*.‚Äù \n\nIn essence, that is true.\n\n- *The killer struck during daylight. Had the sun not been out that day, the victim would have been safe.*\n\n. . .\n\n- There is a correlation, but it is clear there is no causation.\n\n\n\n\n\n\n\n## Correlation & Causality  {.smaller background=\"#b0aeae\"}\n\nSometimes, there is causality even when we do not observe correlation.\n\n*The sailor is adjusting the rudder on a windy day to align the boat with the wind, but the boat is not changing direction.* ([Source: The Mixtape](https://mixtape.scunning.com/01-introduction#do-not-confuse-correlation-with-causality))\n\n\n![](figs/scottboat.jpg)\n\n\n\n\n\n::: {.callout-note}\n\nIn this example, the sailor is *endogenously* adjusting the course to balance the unobserved wind.\n\n:::\n\n\n\n\n\n\n\n## The challenge  {.smaller background=\"#b0aeae\"}\n\n- I will discuss some issues in using plain OLS models in Finance Research (mainly with panel data).\n\n. . .\n\n- I will avoid the word ‚Äúendogeneity‚Äù as much as I can\n\n. . .\n\n- I will also avoid the word ‚Äúidentification‚Äù because identification does not guarantee causality and vice-versa (Kahn and Whited 2017)\n\n. . .\n\n- The discussion is based on [Atanasov and Black (2016)](https://www.nowpublishers.com/article/Details/CFR-0036)\n\n![](figs/slides1-empiricalissues-paper.png)\n\n\n\n\n\n\n\n\n## The challenge  {.smaller background=\"#b0aeae\"}\n\n- Imagine that you want to investigate the effect of Governance on Q\n\n    - You may have more covariates explaining Q (omitted  from slides)\n  \n $ùë∏_{i} = Œ± + ùú∑_{i} √ó Gov + Controls + error$\n\n. . . \n\n All the issues in the next slides will make it not possible to infer that __changing Gov will _CAUSE_ a change in Q__ \n \n That is, cannot infer causality\n \n![](figs/slides1-empiricalissues-wrong.jpg)\n\n\n\n\n\n\n\n\n## 1) Reverse causation   {.smaller background=\"#b0aeae\"}\n\n_One source of bias is: reverse causation_\n\n- Perhaps it is Q that causes Gov\n\n- OLS based methods do not tell the difference between these two betas:\n\n$ùëÑ_{i} = Œ± + ùú∑_{i} √ó Gov + Controls + error$\n\n$Gov_{i} = Œ± + ùú∑_{i} √ó Q + Controls + error$\n\n- If one Beta is significant, the other will most likely be significant too\n\n- You need a sound theory!\n\n\n\n\n\n\n\n\n\n\n\n\n## 2) Omitted variable bias (OVB)  {.smaller background=\"#b0aeae\"}\n\n_The second source of bias is: OVB_\n\n- Imagine that you do not include an important ‚Äútrue‚Äù predictor of Q\n\n- Let's say, long is:  $ùë∏_{i} = ùú∂_{long} + ùú∑_{long}* gov_{i} + Œ¥ * omitted + error$\n\n- But you estimate short:  $ùë∏_{i} = ùú∂_{short} + ùú∑_{short}* gov_{i} + error$\n\n- $ùú∑_{short}$ will be: \n\n    - $ùú∑_{short} = ùú∑_{long}$ +  bias\n\n    - $ùú∑_{short} = ùú∑_{long}$ +  relationship between omitted (omitted) and included (Gov) * effect of omitted in long (Œ¥)\n\n        - Where: relationship between omitted (omitted) and included (Gov) is: $Omitted = ùú∂ + œï *gov_{i} + u$\n\n- Thus, OVB is: $ùú∑_{short} ‚Äì ùú∑_{long} = œï * Œ¥$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## 3) Specification error  {.smaller background=\"#b0aeae\"}\n\n_The third source of bias is: Specification error_\n\n- Even if we could perfectly measure gov and all relevant covariates, we would not know for sure the functional form through which each influences q\n\n    - Functional form: linear? Quadratic? Log-log? Semi-log?\n\n- Misspecification of x‚Äôs is similar to OVB\n\n\n\n\n\n\n\n\n## 4) Signaling   {.smaller background=\"#b0aeae\"}\n\n_The fourth source of bias is: Signaling_\n\n- Perhaps, some individuals are signaling the existence of an X without truly having it:\n\n    - For instance: firms signaling they have good governance without having it\n\n- This is similar to the OVB because you cannot observe the full story\n\n\n\n\n\n\n\n\n\n## 5) Simultaneity  {.smaller background=\"#b0aeae\"}\n\n_The fifth source of bias is: Simultaneity_\n\n- Perhaps gov and some other variable x are determined simultaneously\n\n- Perhaps there is bidirectional causation, with q causing gov and gov also causing q \n\n- In both cases, OLS regression will provide a biased estimate of the effect\n\n- Also, the sign might be wrong\n\n\n\n\n\n\n\n\n\n\n## 6) Heterogeneous effects   {.smaller background=\"#b0aeae\"}\n\n_The sixth source of bias is: Heterogeneous effects_\n\n- Maybe the causal effect of gov on q depends on observed and unobserved firm characteristics:\n\n    - Let's assume that firms seek to maximize q\n    - Different firms have different optimal gov\n    - Firms know their optimal gov\n    - If we observed all factors that affect q, each firm would be at its own optimum and OLS regression would give a non-significant coefficient\n\n- In such case, we may find a positive or negative relationship.\n\n- Neither is the true causal relationship\n\n\n\n\n\n## 7) Construct validity  {.smaller background=\"#b0aeae\"}\n\n_The seventh source of bias is: Construct validity_\n\n- Some constructs (e.g. Corporate governance) are complex, and sometimes have conflicting mechanisms\n\n- We usually don‚Äôt know for sure what ‚Äúgood‚Äù governance is, for instance\n\n- It is common that we use imperfect proxies\n\n- They may poorly fit the underlying concept\n\n\n\n\n\n\n\n## 8) Measurement error   {.smaller background=\"#b0aeae\"}\n\n_The eighth source of bias is: Measurement error_\n\n- \"Classical\" random measurement error for the outcome will inflate standard errors but will not lead to biased coefficients. \n\n    - $y^{*} = y + \\sigma_{1}$\n    - If you estimante $y^{*} = f(x)$, you have $y + \\sigma_{1} = x + \\epsilon$ \n    - $y = x + u$ \n        - where $u = \\epsilon + \\sigma_{1}$ \n\n- \"Classical\" random measurement error in x‚Äôs will bias coefficient estimates toward zero\n\n    - $x^{*} = x + \\sigma_{2}$\n    - Imagine that $x^{*}$ is a bunch of noise\n    - It would not explain anything\n    - Thus, your results are biased toward zero\n\n\n<!-- https://web.stanford.edu/class/polisci100a/regress5.pdf  --> \n\n\n\n\n\n\n\n\n\n## 9) Observation bias   {.smaller background=\"#b0aeae\"}\n\n_The ninth source of bias is: Observation bias_\n\n- This is analogous to the Hawthorne effect, in which observed subjects behave differently because they are observed\n\n- Firms which change gov may behave differently because their managers or employees think the change in gov matters, when in fact it has no direct effect\n\n\n\n\n\n\n\n\n\n\n## 10) Interdependent effects   {.smaller background=\"#b0aeae\"}\n\n_The tenth source of bias is: Interdependent effects_\n\n- Imagine that a governance reform that will not affect share prices for a single firm might be effective if several firms adopt\n\n- Conversely, a reform that improves efficiency for a single firm might not improve profitability if adopted widely because the gains will be competed away\n\n- \"One swallow doesn't make a summer\" \n\n\n\n\n\n\n## 11) Selection bias   {.smaller background=\"#b0aeae\"}\n\n_The eleventh source of bias is: Selection bias_\n\n- If you run a regression with two types of companies\n\n    - High gov (let's say they are the treated group)\n    - Low gov (let's say they are the control group)\n\n    \n- Without any matching method, these companies are likely not comparable\n\n- Thus, the estimated beta will contain selection bias\n\n- The bias can be either be positive or negative\n\n- It is similar to OVB\n\n\n  \n\n## 12) Self-Selection  {.smaller background=\"#b0aeae\"}\n\n_The twelfth source of bias is: Self-Selection_\n\n- Self-selection is a type of selection bias\n\n- Usually, firms decide which level of governance they adopt\n\n- There are reasons why firms adopt high governance\n\n    - If observable, you need to control for\n    - If unobservable, you have a problem\n\n- It is like they \"self-select\" into the treatment.\n\n    - Units decide whether they receive the treatment of not\n\n- Your coefficients will be biased.\n\n\n\n\n\n\n## 13) Collider Bias (endogenous selection bias) {.smaller background=\"#b0aeae\"}\n\n\n**Let's assume an arbitrary population. Two variables describe the population: IQ and luck. **\n\n**These variables are random.**\n\n**Let's say that after you reach a certain level of IQ and Luck, you become successful (i.e., upper-right quadrant).**\n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(data.table)\nlibrary(ggplot2)\nset.seed(100)\nluck <- rnorm(1000, 100, 15)\niq   <- rnorm(1000, 100, 15)\npop <- data.frame(luck, iq)\n\nggplot(pop, aes(x = iq, y = luck)) + \n      geom_point() +\n      labs(title = \"The general population\")  + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\n\n```\n:::\n\n\n\n:::\n\n\n\n\n\n## 13) Collider Bias (endogenous selection bias) {.smaller background=\"#b0aeae\"}\n\n**Zooming in: the graph suggests a negative correlation between luck and IQ. ** \n\n::: {.callout-important}\n**Thus, if the analysis includes only successful people, the researcher will find a negative correlation between luck and IQ**\n::: \n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(data.table)\nlibrary(ggplot2)\nset.seed(100)\nluck <- rnorm(1000, 100, 15)\niq   <- rnorm(1000, 100, 15)\npop <- data.frame(luck, iq)\n\npop$comb <- pop$luck + pop$iq \nsuccessfull <- pop[pop$comb > 240, ] \n\nggplot(successfull, aes(x = iq, y = luck)) + \n      geom_point() +  geom_smooth(method = \"lm\", color = \"blue\", se = FALSE) +  # Regression line\n      labs(title = \"The successful subpopulation\")  + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\n\n```\n:::\n\n\n\n:::\n\n\n\n\n\n\n## 13) Collider Bias (endogenous selection bias) {.smaller background=\"#b0aeae\"}\n\n\n**Successful is a collider. It \"collides\" with luck and IQ (the variables by which we sample our population).**\n\n**If you only observe the collider, you may find unreasonable correlations between variables.**.\n\n**The bias should be easy to observe if you look at the total population (sorted by luck and IQ).** \n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(data.table)\nlibrary(ggplot2)\nset.seed(100)\nluck <- rnorm(1000, 100, 15)\niq   <- rnorm(1000, 100, 15)\npop <- data.frame(luck, iq)\n\npop$comb <- pop$luck + pop$iq \nsuccessfull <- pop[pop$comb > 240, ] \n\nggplot() + \n  geom_point(data = pop, aes(x = iq, y = luck)) +  \n  geom_point(data = successfull, aes(x = iq, y = luck), color = \"red\") +  \n  geom_abline(intercept = 235, slope = -1, color = \"blue\") +\n  labs(title = \"The general population & successful subpopulation\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\n\n```\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Conclus√£o  {.smaller background=\"#b0aeae\"}\n\n**Pesquisa quantitativa tem a parte _quanti (m√©todos, modelos, etc.)_...**\n\n**... Mas talvez a parte mais importante seja o desenho da pesquisa (design emp√≠rico)!**\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Preocupa√ß√µes recentes em pesquisa   {.smaller background=\"#b0aeae\"}\n\n**P-Hacking**\n\n![](figs/slides4-phacking.png) \n\nArtigo original [aqui](https://doi.org/10.1111/jofi.12530).\n\n\n\n\n\n\n\n\n\n\n## Preocupa√ß√µes recentes em pesquisa  {.smaller background=\"#b0aeae\"}\n\n**Publication bias**\n\n![](figs/slides4-Harvey-2017.png) \n\n\nArtigo original [aqui](https://doi.org/10.1111/jofi.12530).\n\n\n\n\n\n\n\n \n\n## Preocupa√ß√µes recentes em pesquisa   {.smaller background=\"#b0aeae\"}\n\n**Crise de replica√ß√£o**\n\n\n![](figs/slides4-aguinis.png) \n\n\nArtigo original [aqui](https://link.springer.com/article/10.1057/s41267-017-0081-0).\n\n\n\n\n\n## Some fun stuff  {.smaller background=\"#b0aeae\"}\n\n![](figs/selection bias.png) \n\n\n## Some fun stuff  {.smaller background=\"#b0aeae\"}\n\n![](figs/fig1.jpg) \n\n\n\n\n\n## Some fun stuff  {.smaller background=\"#b0aeae\"}\n\n\n![](figs/hypothesis2.png) \n\n\n\n\n\n\n## Some fun stuff {.smaller background=\"#b0aeae\"}\n\n\n![](figs/confounding variables.png) \n\n\n\n\n\n\n\n\n\n## Some fun stuff {.smaller background=\"#b0aeae\"}\n\n![](figs/proxy variable.png) \n\n\n\n\n\n\n\n\n\n\n\n\n\n## **THANK YOU!** {background=\"#b1cafa\"}\n\n::: columns\n::: {.column width=\"60%\"}\n**QUESTIONS?**\n\n![](figs/qa2.png){width=\"150%\" heigth=\"150%\"}\n:::\n\n::: {.column width=\"40%\"}\n**Henrique C. Martins**\n\n-   [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)\n-   [Personal Website](https://henriquemartins.net/)\n-   [LinkedIn](https://www.linkedin.com/in/henriquecastror/)\n-   [Lattes](http://lattes.cnpq.br/6076997472159785)\n-   [Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)\\\n:::\n:::\n\n::: footer\n:::\n\n","srcMarkdownNoYaml":"\n\n\n\n\n\n# Agenda\n\n## Agenda {.smaller}\n\n- Apresenta√ß√£o do syllabus do curso\n  - Apresenta√ß√£o dos crit√©rios de avalia√ß√£o\n  \n. . .\n\n- Breve apresenta√ß√£o dos temas de pesquisa e discuss√£o inicial sobre a entrega final\n\n. . .\n\n- In√≠cio do conte√∫do\n  - Introdu√ß√£o a causalidade\n\n\n\n\n\n\n\n\n\n## Sobre a letter  {.smaller visibility=\"hidden\"}\n\n- Formato letter\n  - Entre 2k e 2.5k palavras a depender do journal.\n  \n*The objective of a letter is to facilitate the rapid dissemination of important research that contains an insight, new data, or discuss current important topic.*\n  \n- Ir√° requerer todas as etapas da pesquisa (com √™nfase na an√°lise dos dados, i.e., regress√µes).\n\n- Idealmente, ser√° submetida com o/a orientador/a. Leia-se, sua miss√£o √© \"convencer\" de que o trabalho final √© submet√≠vel a uma revista. \n\n\n\n\n\n\n\n\n\n## Sobre a letter  {.smaller visibility=\"hidden\"}\n\n- Op√ß√µes de revistas que aceitam letter (checar se refer√™ncias e tabelas fazem parte do word count):\n\n  - [Economic Letters](https://www.sciencedirect.com/journal/economics-letters) (ABS3): 2k palavras\n  - [Journal of Accounting and Public Policy](https://www.sciencedirect.com/journal/journal-of-accounting-and-public-policy) (ABS3): 3k palavras\n  - [Finance Research Letters](https://www.sciencedirect.com/journal/finance-research-letters) (ABS2): 2.5k palavras\n  - [Applied Economic Letters](https://www.tandfonline.com/journals/rael20) (ABS1): 2k palavras\n  - [Brazilian Review of Finance](https://periodicos.fgv.br/rbfin) (A4): [4k palavras](https://periodicos.fgv.br/rbfin/libraryFiles/downloadPublic/140) \n  \n* Voc√™ √© bem-vindo/a para propor outro journal que aceite letter, sob condi√ß√£o de valida√ß√£o junto ao instrutor. \n  \n\n\n\n\n\n## Stata {.smaller}\n\n**Providenciar instala√ß√£o para pr√≥ximo encontro**.\n\nPara instala√ß√£o do Stata, seguir instru√ß√µes da TI. \n\n\n\n\n\n\n\n## R {.smaller}\n\n**Providenciar instala√ß√£o para pr√≥ximo encontro**.\n\n\nInstall R [here Win](https://cran.r-project.org/bin/windows/base/)\n\nInstall R [here Mac](https://cran.r-project.org/bin/macosx/)\n\nInstall R Studio [here](https://posit.co/download/rstudio-desktop/)\n\n. . .\n\nPara instalar e carregar os pacotes voc√™ precisa rodar as duas linhas abaixo.\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"R\" code-line-numbers=\"true\"}\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n```\n:::\n\n\n\n\n## Python {.smaller}\n\n**I might show some code in python, but I cannot offer you support on it.**\n\n\n\n\n\n\n# Selection bias  {.smaller background=\"#fadea7\"} \n\n##  {.smaller background=\"#fadea7\"} \n\n\n![](figs/slides4-airplane.png)\n\n\n\n\n\n\n##  {.smaller background=\"#fadea7\"} \n\n![](figs/slides4-path1.jpg)\n\n\n**Voc√™ nunca sabe o resultado do caminho que n√£o toma.**\n\n\n\n\n\n\n\n\n\n\n\n\n## Quais as aplica√ß√µes do que vamos discutir? {.smaller background=\"#fadea7\"} \n\nH√° uma s√©rie de **quest√µes de pesquisa** que poderiam ser investigadas com as ferramentas que vamos discutir hoje.\n\n::: incremental\n\n1) Vale mais a pena estudar em escola particular ou p√∫blica?\n\n2) Qual o efeito de investimentos de marketing t√™m na lucratividade?\n\n3) Qual o efeito que jornadas de 4 dias semanais t√™m na produtividade?\n\n4) Qual efeito que educa√ß√£o tem na remunera√ß√£o futura?\n\n5) E diversas outras semelhantes...\n\n:::\n\n\n\n\n\n\n## Antes de come√ßar: Nossa agenda {.smaller background=\"#fadea7\"} \n\n\n::: incremental \n\n1) Introdu√ß√£o a **pesquisa quantitativa**\n\n2) Validade **Externa** vs. Validade **Interna**\n\n3) **Problemas** em pesquisa quantitativa inferencial\n\n4) **Rem√©dios**\n\n:::\n\n\n\n\n\n\n        \n\n\n## Introdu√ß√£o {.smaller background=\"#fadea7\"} \n\n**O que fazemos em pesquisa quantitiva?** Seguimos o m√©todo de pesquisa tradicional (com ajustes):\n\n::: incremental\n\n- Observa√ß√£o \n\n- Quest√£o de pesquisa \n\n- Modelo te√≥rico (abstrato)\n\n- Hip√≥teses\n\n- Modelo emp√≠rico\n\n- Coleta de dados \n\n- An√°lise do resultado do modelo (diferente de an√°lise de dados \"pura\")\n\n- Conclus√£o/desdobramentos/aprendizados\n  \n:::\n\n\n\n\n\n\n\n\n\n## Introdu√ß√£o {.smaller background=\"#fadea7\"} \n\n**O que fazemos em pesquisa quantitiva?** Seguimos o m√©todo de pesquisa tradicional (com ajustes):\n\n\n- Observa√ß√£o \n\n- Quest√£o de pesquisa \n\n- Modelo te√≥rico (abstrato): **Aqui √© onde a matem√°tica √© necess√°ria**\n\n- Hip√≥teses\n\n- Modelo emp√≠rico: **Estat√≠stica e econometria necess√°rias**\n\n- Coleta de dados: **Geralmente secund√°rios**\n\n- An√°lise do resultado do modelo (diferente de an√°lise de dados \"pura\")\n\n- Conclus√£o/desdobramentos/aprendizados\n\n\n\n\n\n\n\n. . .\n\n## Defini√ß√£o {.smaller background=\"#fadea7\"} \n\n**_Pesquisa quantitativa busca testar hip√≥teses..._**\n\n. . .\n\n**_...a partir da defini√ß√£o de modelos formais (abstratos)..._**\n\n. . .\n\n**_...de onde se estimam modelos emp√≠ricos utilizando a estat√≠stica e a econometria como mecanismos/instrumentos._**\n\n. . .\n\n\nNo fim do dia, buscamos **entender as rela√ß√µes** (que tenham **validade interna** e que ofere√ßam **validade externa**) entre diferentes **vari√°veis de interesse.**\n\n\n\n\n\n\n\n\n\n\n\n## Quais as vantagens? {.smaller background=\"#fadea7\"} \n\n1) **Validade externa:** \n\n. . .\n\n- Conceito de que, se a pesquisa tem validade externa, os seus **achados s√£o representativos**.\n\n. . .\n\n- I.e., s√£o **v√°lidos al√©m do seu modelo**. Resultados \"valem externamente\".\n\n. . .\n\n- Idealmente, buscamos resultados que valem externamente para **acumular conhecimento**...\n\n. . .\n\n- ...naturalmente, nem toda pesquisa quantitativa oferece validade externa. A pesquisa √≥tima sim. **A pesquisa excelente tem validade externa para al√©m do seu tempo**.\n\n. . .\n\n- Pesquisa qualitativa dificilmente oferece **validade externa**.\n\n\n\n\n\n\n\n\n\n\n## Quais as armadilhas? {.smaller background=\"#fadea7\"} \n\n\n2) **Validade interna:** \n\n. . .\n\n- Conceito de que a pesquisa precisa de validade interna para que seus **resultados sejam cr√≠veis**.\n\n. . .\n\n- I.e., os **resultados n√£o podem conter erros**, vieses, problemas de estima√ß√£o, problemas nos dados, etc..\n\n. . .\n\n- √â aqui que a gente separa a pesquisa ruim da pesquisa boa. Para ser levada a s√©rio, a pesquisa **PRECISA** ter validade interna.\n\n. . .\n\n- Mas isso, nem sempre √© trivial. Muitas pesquisas que vemos publicadas, mesmo em top journals, **n√£o t√™m validade interna** (seja por erro do pesquisador, por m√©todo incorreto, por falta de dados...)\n\n. . .\n\n- Mas cada vez mais, **avaliadores est√£o de olho** em problemas e em modelos  **Trash-in-Trash-out**\n\n\n\n\n\n\n\n\n\n\n\n## Como fazemos na pr√°tica? {.smaller background=\"#fadea7\"} \n\nExemplo de modelo emp√≠rico:\n\n$Y_{i} = Œ± + ùú∑_{1} √ó X_i + Controls + error$\n\n. . .\n\n<img src=\"figs/slides4-ols.jpg\" width=\"30%\" align=\"right\" />\n\n. . .\n\nUma vez que estimemos esse modelo, temos o **valor**, o **sinal** e a **signific√¢ncia** do $ùú∑$.\n\n. . .\n\nSe o Beta for **significativamente diferente de zero** e **positivo** --> X e Y est√£o positivamente correlacionados.\n\n. . .\n\n**O problema?** Os pacotes estat√≠sticos que utilizamos **sempre \"cospem\" um beta**. Seja ele com ou sem vi√©s.\n\n. . .\n\nCabe ao pesquisador ter um **design emp√≠rico** que garanta que o beta estimado tenha validade interna.\n\n\n\n\n\n## Como fazemos na pr√°tica? {.smaller background=\"#fadea7\"} \n\n\n<img src=\"figs/slides4-table.png\" width=\"110%\" align=\"center\" />\n\nA decis√£o final √© baseada na signific√¢ncia do Beta estimado. Se **significativo**, as vari√°veis s√£o relacionadas e fazemos infer√™ncias em cima disso.\n\nContudo, **sem um design emp√≠rico inteligente**, o beta encontrado pode ter literalmente qualquer sinal e signific√¢ncia.\n\n\n\n\n\n\n\n\n\n\n\n\n## Exemplo desses problemas {.smaller background=\"#fadea7\"} \n\nVeja esse [site](http://www.tylervigen.com/spurious-correlations).\n\n<img src=\"figs/slides4-spurius1.png\" width=\"100%\" align=\"center\" />\n\n\n\n\n\n## Exemplo desses problemas {.smaller background=\"#fadea7\"} \n\nVeja esse [site](http://www.tylervigen.com/spurious-correlations).\n\n<img src=\"figs/slides4-spurius2.png\" width=\"110%\" align=\"center\" />\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Selection bias - We see I {.smaller background=\"#fadea7\"} \n\n\n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(data.table)\nlibrary(ggplot2)\n# Generate Data\nn = 10000\nset.seed(100)\nx <- rnorm(n)\ny <- rnorm(n)\ndata1 <- 1/(1+exp( 2 - x  -  y))\ngroup  <- rbinom(n, 1, data1)\n\n# Data Together\ndata_we_see     <- subset(data.table(x, y, group), group==1)\ndata_all        <- data.table(x, y, group)\n\n# Graphs\nggplot(data_we_see, aes(x = x, y = y)) + \n      geom_point(aes(colour = factor(-group)), size = 1) +\n      geom_smooth(method=lm, se=FALSE, fullrange=FALSE)+\n      labs( y = \"\", x=\"\", title = \"The observations we see\")+\n      xlim(-3,4)+ ylim(-3,4)+ \n      theme(plot.title = element_text(color=\"black\", size=30, face=\"bold\"),\n            panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n            axis.text.y = element_text(face=\"bold\", color=\"black\", size = 18),\n            axis.text.x = element_text(face=\"bold\", color=\"black\", size = 18),\n            legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nn = 10000\nnp.random.seed(100)\nx = np.random.normal(size=n)\ny = np.random.normal(size=n)\ndata1 = 1 / (1 + np.exp(2 - x - y))\ngroup = np.random.binomial(1, data1, n)\n\ndata_we_see = pd.DataFrame({'x': x[group == 1], 'y': y[group == 1], 'group': group[group == 1]})\ndata_all = pd.DataFrame({'x': x, 'y': y, 'group': group})\n\nsns.set(style='whitegrid')\nplt.figure(figsize=(7, 5))\nplt.scatter(data_we_see['x'], data_we_see['y'], c=-data_we_see['group'], cmap='viridis', s=20)\nsns.regplot(x='x', y='y', data=data_we_see, scatter=False, ci=None, line_kws={'color': 'blue'})\nplt.title(\"The observations we see\", fontsize=18)\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nclear all\nset seed 100\nset obs 10000\ngen x = rnormal(0,1)\ngen y = rnormal(0,1)\ngen data1 = 1 / (1 + exp(2 - x - y))\ngen group = rbinomial(1, data1)\ntwoway (scatter x y if group == 1, mcolor(black) msize(small))    (lfit y x if group == 1, color(blue)),title(\"The observations we see\", size(large) ) xtitle(\"\") ytitle(\"\")\nquietly graph export figs/graph1.svg, replace\n```\n:::\n\n\n![](figs/graph1.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n## Selection bias - We see II  {.smaller background=\"#fadea7\"} \n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\n# Fit a linear regression model\nmodel <- lm(y ~ x, data = data_we_see)\n# Print the summary of the regression model\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, data = data_we_see)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.05878 -0.63754 -0.00276  0.62056  3.11374 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.72820    0.02660   27.37  < 2e-16 ***\nx           -0.14773    0.02327   -6.35 2.75e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9113 on 1747 degrees of freedom\nMultiple R-squared:  0.02256,\tAdjusted R-squared:  0.022 \nF-statistic: 40.32 on 1 and 1747 DF,  p-value: 2.746e-10\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport statsmodels.api as sm\nimport pandas as pd\nn = 10000\nnp.random.seed(100)\nx = np.random.normal(size=n)\ny = np.random.normal(size=n)\ndata1 = 1 / (1 + np.exp(2 - x - y))\ngroup = np.random.binomial(1, data1, n)\n\ndata_we_see = pd.DataFrame({'x': x[group == 1], 'y': y[group == 1], 'group': group[group == 1]})\ndata_all = pd.DataFrame({'x': x, 'y': y, 'group': group})\n\nX = data_we_see['x']  \nX = sm.add_constant(X)\ny = data_we_see['y']  \nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.018\nModel:                            OLS   Adj. R-squared:                  0.018\nMethod:                 Least Squares   F-statistic:                     33.84\nDate:                qua, 28 ago 2024   Prob (F-statistic):           7.06e-09\nTime:                        20:44:23   Log-Likelihood:                -2411.1\nNo. Observations:                1809   AIC:                             4826.\nDf Residuals:                    1807   BIC:                             4837.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.7037      0.026     26.826      0.000       0.652       0.755\nx             -0.1339      0.023     -5.817      0.000      -0.179      -0.089\n==============================================================================\nOmnibus:                        4.656   Durbin-Watson:                   1.973\nProb(Omnibus):                  0.097   Jarque-Bera (JB):                5.264\nSkew:                          -0.038   Prob(JB):                       0.0720\nKurtosis:                       3.253   Cond. No.                         1.93\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nclear all\nset seed 100\nset obs 10000\ngen x = rnormal(0,1)\ngen y = rnormal(0,1)\ngen data1 = 1 / (1 + exp(2 - x - y))\ngen group = rbinomial(1, data1)\nreg y x if group ==1\n\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of observations (_N) was 0, now 10,000.\n\n      Source |       SS           df       MS      Number of obs   =     1,872\n-------------+----------------------------------   F(1, 1870)      =     48.62\n       Model |  40.9398907         1  40.9398907   Prob > F        =    0.0000\n    Residual |  1574.57172     1,870  .842016963   R-squared       =    0.0253\n-------------+----------------------------------   Adj R-squared   =    0.0248\n       Total |  1615.51161     1,871  .863448215   Root MSE        =    .91761\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |  -.1579538   .0226526    -6.97   0.000    -.2023808   -.1135269\n       _cons |   .7202285   .0257215    28.00   0.000     .6697827    .7706744\n------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Selection bias - All I  {.smaller background=\"#fadea7\"} \n\n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nggplot(data_all, aes(x = x, y = y,  colour=group)) + \n  geom_point(aes(colour = factor(-group)), size = 1) +\n  geom_smooth(method=lm, se=FALSE, fullrange=FALSE)+\n  labs( y = \"\", x=\"\", title = \"All observations\")+\n  xlim(-3,4)+ ylim(-3,4)+ \n  theme(plot.title = element_text(color=\"black\", size=30, face=\"bold\"),\n      panel.background = element_rect(fill = \"grey95\", colour = \"grey95\"),\n      axis.text.y = element_text(face=\"bold\", color=\"black\", size = 18),\n      axis.text.x = element_text(face=\"bold\", color=\"black\", size = 18),\n      legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style='whitegrid')\nplt.figure(figsize=(6, 4))\nsns.scatterplot(data=data_all, x='x', y='y', hue='group', palette=['blue', 'red'], s=20)\nsns.regplot(data=data_all, x='x', y='y', scatter=False, ci=None, line_kws={'color': 'blue'})\nplt.title(\"All observations\", fontsize=18)\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.legend(title=\"Group\", labels=[\"0\", \"1\"], loc=\"upper left\")\n\nplt.gca().get_legend().remove()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of observations (_N) was 0, now 10,000.\n```\n\n\n:::\n:::\n\n\n![](figs/graph2.svg)\n\n:::\n\n\n\n\n\n\n\n\n\n\n## Selection bias - All I {.smaller background=\"#fadea7\"} \n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nmodel2 <- lm(y ~ x, data = data_all)\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, data = data_all)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9515 -0.6716  0.0087  0.6698  3.9878 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept) -0.011825   0.009994  -1.183    0.237\nx           -0.003681   0.010048  -0.366    0.714\n\nResidual standard error: 0.9994 on 9998 degrees of freedom\nMultiple R-squared:  1.342e-05,\tAdjusted R-squared:  -8.66e-05 \nF-statistic: 0.1342 on 1 and 9998 DF,  p-value: 0.7141\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Python\" code-line-numbers=\"true\"}\nimport statsmodels.api as sm\nimport pandas as pd\nn = 10000\nnp.random.seed(100)\nx = np.random.normal(size=n)\ny = np.random.normal(size=n)\ndata1 = 1 / (1 + np.exp(2 - x - y))\ngroup = np.random.binomial(1, data1, n)\n\ndata_we_see = pd.DataFrame({'x': x[group == 1], 'y': y[group == 1], 'group': group[group == 1]})\ndata_all = pd.DataFrame({'x': x, 'y': y, 'group': group})\n\nX = data_all['x']  \nX = sm.add_constant(X)\ny = data_all['y']  \nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     1.281\nDate:                qua, 28 ago 2024   Prob (F-statistic):              0.258\nTime:                        20:44:32   Log-Likelihood:                -14157.\nNo. Observations:               10000   AIC:                         2.832e+04\nDf Residuals:                    9998   BIC:                         2.833e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.0003      0.010     -0.034      0.973      -0.020       0.019\nx             -0.0112      0.010     -1.132      0.258      -0.031       0.008\n==============================================================================\nOmnibus:                        0.267   Durbin-Watson:                   2.009\nProb(Omnibus):                  0.875   Jarque-Bera (JB):                0.242\nSkew:                           0.009   Prob(JB):                        0.886\nKurtosis:                       3.017   Cond. No.                         1.01\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n\n\n:::\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\nclear all\nset seed 100\nset obs 10000\ngen x = rnormal(0,1)\ngen y = rnormal(0,1)\ngen data1 = 1 / (1 + exp(2 - x - y))\ngen group = rbinomial(1, data1)\nreg y x \n\n```\n\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of observations (_N) was 0, now 10,000.\n\n      Source |       SS           df       MS      Number of obs   =    10,000\n-------------+----------------------------------   F(1, 9998)      =      0.28\n       Model |  .284496142         1  .284496142   Prob > F        =    0.5938\n    Residual |  9999.04347     9,998  1.00010437   R-squared       =    0.0000\n-------------+----------------------------------   Adj R-squared   =   -0.0001\n       Total |  9999.32797     9,999   1.0000328   Root MSE        =    1.0001\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |  -.0053101    .009956    -0.53   0.594    -.0248259    .0142057\n       _cons |   .0006182   .0100006     0.06   0.951    -.0189849    .0202213\n------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n## Selection bias {.smaller background=\"#fadea7\"} \n\nSelection bias n√£o √© o √∫nico dos nossos problemas, mas √© um **importante**.\n\nVeja que suas conclus√µes mudaram significativamente.\n\nN√£o seria dif√≠cil criar um exemplo em que o **coeficiente verdadeiro** fosse positivo.\n\n\n\n\n\n\n\n\n\n\n\n## Exemplo desses problemas {.smaller background=\"#fadea7\"} \n\n![](figs/slides4-path2b.png) \n\n\nSource: [Angrist](https://www.youtube.com/watch?v=iPBV3BlV7jk)\n\n**N√£o podemos pegar dois caminhos.**\n\n\n\n\n\n\n\n\n\n\n## Exemplo desses problemas {.smaller background=\"#fadea7\"} \n\n![](figs/slides4-matching.png) \n\n\nSource: [Angrist](https://www.youtube.com/watch?v=6YrIDhaUQOE)\n\n**N√£o podemos comparar pessoas que n√£o s√£o compar√°veis.**\n\n\n\n\n\n\n\n\n## O que precisamos fazer? {.smaller background=\"#fadea7\"} \n\n. . .\n\nDefinir um bom **_Design emp√≠rico_**\n\n. . .\n\nNo mundo ideal: ter√≠amos **universos paralelos.** Ter√≠amos **dois clones**, em que cada um escolhe um caminho. Todo o resto √© igual.\n\n- Obviamente, isso n√£o existe.\n\n. . .\n\nSegunda melhor solu√ß√£o: **experimentos**\n\n. . .\n\n**Mas o que √© um experimento?**\n\n- Grupo de tratamento vs. Grupo de controle\n\n- Igualdade entre os grupos (i.e., aleatoriedade no sampling)\n\n    - Nada diferencia os grupos a n√£o ser o fato de que um indiv√≠duo recebe tratamento e o outro n√£o\n    - Estamos comparando ma√ßas com ma√ßas e laranjas com laranjas\n      \n- Testes placebo/falsifica√ß√£o.\n\n\n\n\n\n\n\n\n\n\n\n\n\n# The challenge {.smaller background=\"#b0aeae\"}\n\n## Correlation & Causality {.smaller background=\"#b0aeae\"}\n\n\nIt is very common these days to hear someone say ‚Äú*correlation does not mean causality*.‚Äù \n\nIn essence, that is true.\n\n- *The killer struck during daylight. Had the sun not been out that day, the victim would have been safe.*\n\n. . .\n\n- There is a correlation, but it is clear there is no causation.\n\n\n\n\n\n\n\n## Correlation & Causality  {.smaller background=\"#b0aeae\"}\n\nSometimes, there is causality even when we do not observe correlation.\n\n*The sailor is adjusting the rudder on a windy day to align the boat with the wind, but the boat is not changing direction.* ([Source: The Mixtape](https://mixtape.scunning.com/01-introduction#do-not-confuse-correlation-with-causality))\n\n\n![](figs/scottboat.jpg)\n\n\n\n\n\n::: {.callout-note}\n\nIn this example, the sailor is *endogenously* adjusting the course to balance the unobserved wind.\n\n:::\n\n\n\n\n\n\n\n## The challenge  {.smaller background=\"#b0aeae\"}\n\n- I will discuss some issues in using plain OLS models in Finance Research (mainly with panel data).\n\n. . .\n\n- I will avoid the word ‚Äúendogeneity‚Äù as much as I can\n\n. . .\n\n- I will also avoid the word ‚Äúidentification‚Äù because identification does not guarantee causality and vice-versa (Kahn and Whited 2017)\n\n. . .\n\n- The discussion is based on [Atanasov and Black (2016)](https://www.nowpublishers.com/article/Details/CFR-0036)\n\n![](figs/slides1-empiricalissues-paper.png)\n\n\n\n\n\n\n\n\n## The challenge  {.smaller background=\"#b0aeae\"}\n\n- Imagine that you want to investigate the effect of Governance on Q\n\n    - You may have more covariates explaining Q (omitted  from slides)\n  \n $ùë∏_{i} = Œ± + ùú∑_{i} √ó Gov + Controls + error$\n\n. . . \n\n All the issues in the next slides will make it not possible to infer that __changing Gov will _CAUSE_ a change in Q__ \n \n That is, cannot infer causality\n \n![](figs/slides1-empiricalissues-wrong.jpg)\n\n\n\n\n\n\n\n\n## 1) Reverse causation   {.smaller background=\"#b0aeae\"}\n\n_One source of bias is: reverse causation_\n\n- Perhaps it is Q that causes Gov\n\n- OLS based methods do not tell the difference between these two betas:\n\n$ùëÑ_{i} = Œ± + ùú∑_{i} √ó Gov + Controls + error$\n\n$Gov_{i} = Œ± + ùú∑_{i} √ó Q + Controls + error$\n\n- If one Beta is significant, the other will most likely be significant too\n\n- You need a sound theory!\n\n\n\n\n\n\n\n\n\n\n\n\n## 2) Omitted variable bias (OVB)  {.smaller background=\"#b0aeae\"}\n\n_The second source of bias is: OVB_\n\n- Imagine that you do not include an important ‚Äútrue‚Äù predictor of Q\n\n- Let's say, long is:  $ùë∏_{i} = ùú∂_{long} + ùú∑_{long}* gov_{i} + Œ¥ * omitted + error$\n\n- But you estimate short:  $ùë∏_{i} = ùú∂_{short} + ùú∑_{short}* gov_{i} + error$\n\n- $ùú∑_{short}$ will be: \n\n    - $ùú∑_{short} = ùú∑_{long}$ +  bias\n\n    - $ùú∑_{short} = ùú∑_{long}$ +  relationship between omitted (omitted) and included (Gov) * effect of omitted in long (Œ¥)\n\n        - Where: relationship between omitted (omitted) and included (Gov) is: $Omitted = ùú∂ + œï *gov_{i} + u$\n\n- Thus, OVB is: $ùú∑_{short} ‚Äì ùú∑_{long} = œï * Œ¥$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## 3) Specification error  {.smaller background=\"#b0aeae\"}\n\n_The third source of bias is: Specification error_\n\n- Even if we could perfectly measure gov and all relevant covariates, we would not know for sure the functional form through which each influences q\n\n    - Functional form: linear? Quadratic? Log-log? Semi-log?\n\n- Misspecification of x‚Äôs is similar to OVB\n\n\n\n\n\n\n\n\n## 4) Signaling   {.smaller background=\"#b0aeae\"}\n\n_The fourth source of bias is: Signaling_\n\n- Perhaps, some individuals are signaling the existence of an X without truly having it:\n\n    - For instance: firms signaling they have good governance without having it\n\n- This is similar to the OVB because you cannot observe the full story\n\n\n\n\n\n\n\n\n\n## 5) Simultaneity  {.smaller background=\"#b0aeae\"}\n\n_The fifth source of bias is: Simultaneity_\n\n- Perhaps gov and some other variable x are determined simultaneously\n\n- Perhaps there is bidirectional causation, with q causing gov and gov also causing q \n\n- In both cases, OLS regression will provide a biased estimate of the effect\n\n- Also, the sign might be wrong\n\n\n\n\n\n\n\n\n\n\n## 6) Heterogeneous effects   {.smaller background=\"#b0aeae\"}\n\n_The sixth source of bias is: Heterogeneous effects_\n\n- Maybe the causal effect of gov on q depends on observed and unobserved firm characteristics:\n\n    - Let's assume that firms seek to maximize q\n    - Different firms have different optimal gov\n    - Firms know their optimal gov\n    - If we observed all factors that affect q, each firm would be at its own optimum and OLS regression would give a non-significant coefficient\n\n- In such case, we may find a positive or negative relationship.\n\n- Neither is the true causal relationship\n\n\n\n\n\n## 7) Construct validity  {.smaller background=\"#b0aeae\"}\n\n_The seventh source of bias is: Construct validity_\n\n- Some constructs (e.g. Corporate governance) are complex, and sometimes have conflicting mechanisms\n\n- We usually don‚Äôt know for sure what ‚Äúgood‚Äù governance is, for instance\n\n- It is common that we use imperfect proxies\n\n- They may poorly fit the underlying concept\n\n\n\n\n\n\n\n## 8) Measurement error   {.smaller background=\"#b0aeae\"}\n\n_The eighth source of bias is: Measurement error_\n\n- \"Classical\" random measurement error for the outcome will inflate standard errors but will not lead to biased coefficients. \n\n    - $y^{*} = y + \\sigma_{1}$\n    - If you estimante $y^{*} = f(x)$, you have $y + \\sigma_{1} = x + \\epsilon$ \n    - $y = x + u$ \n        - where $u = \\epsilon + \\sigma_{1}$ \n\n- \"Classical\" random measurement error in x‚Äôs will bias coefficient estimates toward zero\n\n    - $x^{*} = x + \\sigma_{2}$\n    - Imagine that $x^{*}$ is a bunch of noise\n    - It would not explain anything\n    - Thus, your results are biased toward zero\n\n\n<!-- https://web.stanford.edu/class/polisci100a/regress5.pdf  --> \n\n\n\n\n\n\n\n\n\n## 9) Observation bias   {.smaller background=\"#b0aeae\"}\n\n_The ninth source of bias is: Observation bias_\n\n- This is analogous to the Hawthorne effect, in which observed subjects behave differently because they are observed\n\n- Firms which change gov may behave differently because their managers or employees think the change in gov matters, when in fact it has no direct effect\n\n\n\n\n\n\n\n\n\n\n## 10) Interdependent effects   {.smaller background=\"#b0aeae\"}\n\n_The tenth source of bias is: Interdependent effects_\n\n- Imagine that a governance reform that will not affect share prices for a single firm might be effective if several firms adopt\n\n- Conversely, a reform that improves efficiency for a single firm might not improve profitability if adopted widely because the gains will be competed away\n\n- \"One swallow doesn't make a summer\" \n\n\n\n\n\n\n## 11) Selection bias   {.smaller background=\"#b0aeae\"}\n\n_The eleventh source of bias is: Selection bias_\n\n- If you run a regression with two types of companies\n\n    - High gov (let's say they are the treated group)\n    - Low gov (let's say they are the control group)\n\n    \n- Without any matching method, these companies are likely not comparable\n\n- Thus, the estimated beta will contain selection bias\n\n- The bias can be either be positive or negative\n\n- It is similar to OVB\n\n\n  \n\n## 12) Self-Selection  {.smaller background=\"#b0aeae\"}\n\n_The twelfth source of bias is: Self-Selection_\n\n- Self-selection is a type of selection bias\n\n- Usually, firms decide which level of governance they adopt\n\n- There are reasons why firms adopt high governance\n\n    - If observable, you need to control for\n    - If unobservable, you have a problem\n\n- It is like they \"self-select\" into the treatment.\n\n    - Units decide whether they receive the treatment of not\n\n- Your coefficients will be biased.\n\n\n\n\n\n\n## 13) Collider Bias (endogenous selection bias) {.smaller background=\"#b0aeae\"}\n\n\n**Let's assume an arbitrary population. Two variables describe the population: IQ and luck. **\n\n**These variables are random.**\n\n**Let's say that after you reach a certain level of IQ and Luck, you become successful (i.e., upper-right quadrant).**\n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(data.table)\nlibrary(ggplot2)\nset.seed(100)\nluck <- rnorm(1000, 100, 15)\niq   <- rnorm(1000, 100, 15)\npop <- data.frame(luck, iq)\n\nggplot(pop, aes(x = iq, y = luck)) + \n      geom_point() +\n      labs(title = \"The general population\")  + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\n\n```\n:::\n\n\n\n:::\n\n\n\n\n\n## 13) Collider Bias (endogenous selection bias) {.smaller background=\"#b0aeae\"}\n\n**Zooming in: the graph suggests a negative correlation between luck and IQ. ** \n\n::: {.callout-important}\n**Thus, if the analysis includes only successful people, the researcher will find a negative correlation between luck and IQ**\n::: \n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(data.table)\nlibrary(ggplot2)\nset.seed(100)\nluck <- rnorm(1000, 100, 15)\niq   <- rnorm(1000, 100, 15)\npop <- data.frame(luck, iq)\n\npop$comb <- pop$luck + pop$iq \nsuccessfull <- pop[pop$comb > 240, ] \n\nggplot(successfull, aes(x = iq, y = luck)) + \n      geom_point() +  geom_smooth(method = \"lm\", color = \"blue\", se = FALSE) +  # Regression line\n      labs(title = \"The successful subpopulation\")  + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\n\n```\n:::\n\n\n\n:::\n\n\n\n\n\n\n## 13) Collider Bias (endogenous selection bias) {.smaller background=\"#b0aeae\"}\n\n\n**Successful is a collider. It \"collides\" with luck and IQ (the variables by which we sample our population).**\n\n**If you only observe the collider, you may find unreasonable correlations between variables.**.\n\n**The bias should be easy to observe if you look at the total population (sorted by luck and IQ).** \n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R\" code-line-numbers=\"true\"}\nlibrary(data.table)\nlibrary(ggplot2)\nset.seed(100)\nluck <- rnorm(1000, 100, 15)\niq   <- rnorm(1000, 100, 15)\npop <- data.frame(luck, iq)\n\npop$comb <- pop$luck + pop$iq \nsuccessfull <- pop[pop$comb > 240, ] \n\nggplot() + \n  geom_point(data = pop, aes(x = iq, y = luck)) +  \n  geom_point(data = successfull, aes(x = iq, y = luck), color = \"red\") +  \n  geom_abline(intercept = 235, slope = -1, color = \"blue\") +\n  labs(title = \"The general population & successful subpopulation\") + \n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](part_1_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n:::\n\n\n### Stata\n\n\n::: {.cell layout-align=\"center\" output-location='default'}\n\n```{.stata .cell-code  code-fold=\"true\" code-summary=\"Stata\" code-line-numbers=\"true\"}\n\n```\n:::\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Conclus√£o  {.smaller background=\"#b0aeae\"}\n\n**Pesquisa quantitativa tem a parte _quanti (m√©todos, modelos, etc.)_...**\n\n**... Mas talvez a parte mais importante seja o desenho da pesquisa (design emp√≠rico)!**\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Preocupa√ß√µes recentes em pesquisa   {.smaller background=\"#b0aeae\"}\n\n**P-Hacking**\n\n![](figs/slides4-phacking.png) \n\nArtigo original [aqui](https://doi.org/10.1111/jofi.12530).\n\n\n\n\n\n\n\n\n\n\n## Preocupa√ß√µes recentes em pesquisa  {.smaller background=\"#b0aeae\"}\n\n**Publication bias**\n\n![](figs/slides4-Harvey-2017.png) \n\n\nArtigo original [aqui](https://doi.org/10.1111/jofi.12530).\n\n\n\n\n\n\n\n \n\n## Preocupa√ß√µes recentes em pesquisa   {.smaller background=\"#b0aeae\"}\n\n**Crise de replica√ß√£o**\n\n\n![](figs/slides4-aguinis.png) \n\n\nArtigo original [aqui](https://link.springer.com/article/10.1057/s41267-017-0081-0).\n\n\n\n\n\n## Some fun stuff  {.smaller background=\"#b0aeae\"}\n\n![](figs/selection bias.png) \n\n\n## Some fun stuff  {.smaller background=\"#b0aeae\"}\n\n![](figs/fig1.jpg) \n\n\n\n\n\n## Some fun stuff  {.smaller background=\"#b0aeae\"}\n\n\n![](figs/hypothesis2.png) \n\n\n\n\n\n\n## Some fun stuff {.smaller background=\"#b0aeae\"}\n\n\n![](figs/confounding variables.png) \n\n\n\n\n\n\n\n\n\n## Some fun stuff {.smaller background=\"#b0aeae\"}\n\n![](figs/proxy variable.png) \n\n\n\n\n\n\n\n\n\n\n\n\n\n## **THANK YOU!** {background=\"#b1cafa\"}\n\n::: columns\n::: {.column width=\"60%\"}\n**QUESTIONS?**\n\n![](figs/qa2.png){width=\"150%\" heigth=\"150%\"}\n:::\n\n::: {.column width=\"40%\"}\n**Henrique C. Martins**\n\n-   [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)\n-   [Personal Website](https://henriquemartins.net/)\n-   [LinkedIn](https://www.linkedin.com/in/henriquecastror/)\n-   [Lattes](http://lattes.cnpq.br/6076997472159785)\n-   [Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)\\\n:::\n:::\n\n::: footer\n:::\n\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","css":["logo.css"],"output-file":"part_1.knit.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.4.549","auto-stretch":true,"html":{"css":"webex.css","include-after-body":"webex.js"},"editor":"visual","title":"Empirical Methods in Finance","subtitle":"Part 1","author":"Henrique C. Martins","title-slide-attributes":{"data-background-color":"#b1cafa"},"include-after":["<script type=\"text/javascript\">\n  Reveal.on('ready', event => {\n    if (event.indexh === 0) {\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n  });\n  Reveal.addEventListener('slidechanged', (event) => {\n    if (event.indexh === 0) {\n      Reveal.configure({ slideNumber: null });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = \"none\";\n    }\n    if (event.indexh === 1) {\n      Reveal.configure({ slideNumber: 'c' });\n      document.querySelector(\"div.has-logo > img.slide-logo\").style.display = null;\n    }\n  });\n</script>\n"],"slideNumber":true,"theme":"simple","chalkboard":true,"previewLinks":"auto","logo":"figs/background8.png","footer":"**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  ","multiplex":true,"scrollable":true}}},"projectFormats":["html"]}