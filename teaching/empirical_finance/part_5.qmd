---
title: 'Empirical Methods in Finance'
subtitle: 'Part 5'
author: 'Henrique C. Martins'
format:
  revealjs: 
    slide-number: true
    theme: simple
    chalkboard: true
    preview-links: auto
    logo: figs/background8.png
    css: logo.css
    footer: '**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  '
    multiplex: true
    scrollable: true 
title-slide-attributes:
    data-background-color: "#b1cafa"
include-after: |
  <script type="text/javascript">
    Reveal.on('ready', event => {
      if (event.indexh === 0) {
        document.querySelector("div.has-logo > img.slide-logo").style.display = "none";
      }
    });
    Reveal.addEventListener('slidechanged', (event) => {
      if (event.indexh === 0) {
        Reveal.configure({ slideNumber: null });
        document.querySelector("div.has-logo > img.slide-logo").style.display = "none";
      }
      if (event.indexh === 1) { 
        Reveal.configure({ slideNumber: 'c' });
        document.querySelector("div.has-logo > img.slide-logo").style.display = null;
      }
    });
  </script>

---



```{r setup}
#| include: false
#| warning: false


# library(reticulate)
# use_python("C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe")
library(reticulate)
library(Statamarkdown)
#reticulate::py_install("matplotlib")
#reticulate::py_install("seaborn")
#reticulate::py_install("pyfinance")
#reticulate::py_install("xlrd")
#reticulate::py_install("quandl")

```







# Panel Data {.smaller background="#dff5ce"}

## Panel Data {.smaller background="#dff5ce"}

As explained previously, OVB is a significant source of "endogeneity" in empirical research.

OVB is a problem because of the considerable heterogeneity in many empirical settings. 

**Many of the omitted variables are unobservable to the researcher.**

Panel data can sometimes offer a partial.






## Panel Data {.smaller background="#dff5ce"}

We start defining the following:


$$y_{i,t} = \alpha + \beta_1 x_{i,t} + \epsilon_{i,t}$$

Where: 

  - $i = 1, . . . , N$
  - $t = 1, . . . , T$

. . . 


Imagine that the residual can be decomposed in: 

$$\epsilon_{i,t} = c_i + \mu_{i,t}$$

The term $c_i$ is constant.



## Panel Data {.smaller background="#dff5ce"}

The term $c_i$ is constant.

**It captures the aggregate effect of all of the unobservable, time-invariant explanatory variables for $y_{it}$.**

To focus attention on the issues specific to panel data, we assume that $e_{it}$ has a zero mean conditional on $x_{it}$ and $c_i$ for all $t$.

. . .

The most important thing here is whether $x_{it}$ and $c_i$ are correlated.

**Why?**






## Panel Data {.smaller background="#dff5ce"}

The most important thing here is whether $x_{it}$ and $c_i$ are correlated.


- If $x_{it}$ and $c_i$ are correlated, then $c_i$  is referred to as a “fixed effect”.
  
  - It there is correlation, there is violation of the *Conditional Mean Independence* (CMI) assumption.

    
- If $x_{it}$ and $c_i$ are not correlated, then $c_i$  is referred to as a “random effect”.

  - Endogeneity is not a concern; however, the computation of standard errors is affected.








## Panel Data {.smaller background="#dff5ce"}

**Why might fixed effects arise?**

FE are any time-invariant unit characteristic that cannot be observed in the data.

- education level,
- firm's culture,
- technology,
- managerial talent,
- investment opportunities,
- location (economic development, institutions, etc.),
- etc.







## Panel Data {.smaller background="#dff5ce"}

**We say things like (you have to understand that they refer to FE):** 

- "*Time-invariant heterogeneity at the unit-level*"
- "*Unobserved variation that occur at the unit-level that do not vary over time*"

**Important**: with FE, you are capturing **all** unobserved heterogeneity that do not vary over time.







## Panel Data {.smaller background="#dff5ce"}

Definition of *Panel Data*:

You have multiple observations per unit (individual, firm, etc.)

In datasets, it is "one panel below the other" not "one panel beside the other".

. . . 


**Four main topics in Panel Data:**

1) Pooled cross-sectional

2) Fixed Effect models (including multidimensional FE)

3) Random Effects model

4) First differences

5) Lagged models








## Panel Data {.smaller background="#dff5ce"}

Formal definition

$$y_{i,t} = \alpha + \beta_1 x_{i,t} + \delta FE +  \epsilon_{i,t}$$

- $E(\epsilon_{i,t}) = 0$

- $corr(x_{i,t},FE) \neq 0$

- $corr(FE, \epsilon_{i,t}) = 0$

- $corr(x_{i,t},epsilon_{i,t}) = 0$, for all t

The last assumption is called *strict exogeneity assumption* and means that the residual of any t is uncorrelated with x of any t.

*That is, under a strict exogeneity assumption on the explanatory variables, the fixed effects estimator is unbiased: the idiosyncratic error should be uncorrelated with each explanatory variable across all time periods.*


. . .

**Remember that if we ignore FE, we have OVB.**





## Panel Data {.smaller background="#dff5ce"}

**Before we continue...**

**Comment #1**

*The standard errors in this framework must be “clustered” by panel unit (e.g., individual) to allow for correlation in the residual for the same person over time. This yields valid inference as long as the number of clusters is “large."*

. . . 

**Comment #2**

*FE cannot solve reverse causality, it might help you with OVB.*

. . . 

**Comment #3**

*Three main types of FE:*

- Pooled
- Within-transformation (when someone says FE, it is usually this one)
- Random Effects






# Pooling Cross-sections  {.smaller background="#e0cafc"}

## Pooling Cross-Sections  {.smaller background="#e0cafc"}

When you have two periods of the same unit, but the periods are not consecutive, you have a pooled cross-sectional data.

This is common in survey data.

If you use only one period, you might find biased results.

. . .

Let's practice with the dataset CRIME2 from Wooldridge. 

This dataset contains data (many cities) on the crime rate, unemployment rate and many other city-related variables.

There are two years, 82 and 87 (this is pooled cross-section). 





## Pooling Cross-Sections  {.smaller background="#e0cafc"}

If we estimate only using the year 87, we would interpret that unemployment leads to lower crime rate.

::: panel-tabset

### R

```{r}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output-location: default
#| code-fold: true
#| code-summary: "R"
#| code-line-numbers: true
#| eval: true
library(haven) 
data <- read_dta("files/CRIME2.dta")
data1 <- subset(data, year == 87)
model <- lm(crmrte ~ unem, data = data1)
summary(model)
```

### Python

```{python}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Python"
import pandas as pd
import statsmodels.api as sm

data = pd.read_stata("files/CRIME2.dta")
data1 = data[data['year'] == 87]
model = sm.OLS(data1['crmrte'], sm.add_constant(data1['unem'])).fit()
print(model.summary())
```

### Stata

```{stata}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Stata"
use "files/CRIME2.dta" , clear
reg crmrte une if year ==87
```  

:::








## Pooling Cross-Sections  {.smaller background="#e0cafc"}

When we consider a panel, we get the expected positive sign. This is evidence that the previous model suffered from OVB. Still, the coefficient of unem is not significant probably because of time-invariant unobserved heterogeneity in the cities.

::: panel-tabset

### R

```{r}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output-location: default
#| code-fold: true
#| code-summary: "R"
#| code-line-numbers: true
#| eval: true
library(haven) 
data <- read_dta("files/CRIME2.dta")
model <- lm(crmrte ~ d87+ unem, data = data)
summary(model)
```

### Python

```{python}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Python"
import pandas as pd
import statsmodels.api as sm

data = pd.read_stata("files/CRIME2.dta")
model = sm.OLS(data['crmrte'], sm.add_constant(data[['d87','unem']])).fit()
print(model.summary())
```

### Stata

```{stata}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Stata"
use "files/CRIME2.dta" , clear
reg crmrte  d87 une 
```  

:::



## Pooling Cross-Sections  {.smaller background="#e0cafc"}

This shows us that we should also control for the year variable. 

We call this, **Year Fixed Effects.**

We still most likely have OVB due to the unobserved heterogeneity in cities, that is, we still would need to include **cities FE**. 














# Demeaned variables  {.smaller background="#fccad9"}

## Demeaned variables  {.smaller background="#fccad9"}

**A first way to eliminate the FE is by demeaning the data.**

Consider the following:

$$\bar{y_i} = \alpha +\beta \bar{x_i} + \delta FE + \bar{\epsilon_i}$$

$$\frac{1}{T}\sum{y_{i,t}} = \alpha +\beta \frac{1}{T}\sum{x_{i,t}} + \delta FE + \frac{1}{T}\sum{\epsilon_{i,t}}$$

. . .

If we subtract the mean of each variable, we have:

$$(y_{i,t} - \bar{y_i}) = \beta (x_{i,t} - \bar{x_i}) + (\epsilon_{i,t} - \bar{\epsilon_i})$$

Because the FE does not vary over time, each value is equal to the mean.

Thus, when you demean, you eliminate the FE from the equation. You also eliminate the intercept $\alpha$.

. . .

**Takeaway**: OLS will estimate unbiased coefficients if you demean the variables.

This is called **within-transformation** because you are demeaning "within" the group.













## Demeaned variables  {.smaller background="#fccad9"}

Let's use the dataset WAGEPAN to estimate the following equation.

$$Ln(wage)=\alpha + \beta_1 exper^2 + \beta_2 married + \beta_3 union + \epsilon$$


Some variables in the dataset do not vary over time. These variables cannot be included in this equation. 







## Demeaned variables  {.smaller background="#fccad9"}

See page 495 Wooldridge.

::: panel-tabset
### R

```{r}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output-location: default
#| code-fold: true
#| code-summary: "R"
#| code-line-numbers: true
#| eval: true
library(foreign)
library(stargazer)
library(sandwich)

data <- read.dta("files/WAGEPAN.dta")
# Calculate mean by nr for lwage, expersq, married, and union
data <- data[order(data$nr), ]  # Sort data by nr for by-group operations
data$lwage_mean <- ave(data$lwage, data$nr, FUN = mean)
data$expersq_mean <- ave(data$expersq, data$nr, FUN = mean)
data$married_mean <- ave(data$married, data$nr, FUN = mean)
data$union_mean <- ave(data$union, data$nr, FUN = mean)

data$lwage_demean <- data$lwage - data$lwage_mean
data$expersq_demean <- data$expersq - data$expersq_mean
data$married_demean <- data$married - data$married_mean
data$union_demean <- data$union - data$union_mean

model1 <- lm(lwage ~ educ + black + hisp + exper + expersq + married + union + d81 + d82 + d83 + d84 + d85 + d86 + d87, data = data)
model2 <- lm(lwage_demean ~ expersq_demean + married_demean + union_demean + d81 + d82 + d83 + d84 + d85 + d86 + d87, data = data)

stargazer(model1, model2 ,title = "Regression Results", column.labels=c("OLS","Demean"),  type = "text")

```

### Python

```{python}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Python"
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.iolib.summary2 import summary_col

data = pd.read_stata("files/WAGEPAN.dta")

data = data.sort_values(by='nr')  # Sort data by nr for by-group operations
data['lwage_mean'] = data.groupby('nr')['lwage'].transform('mean')
data['expersq_mean'] = data.groupby('nr')['expersq'].transform('mean')
data['married_mean'] = data.groupby('nr')['married'].transform('mean')
data['union_mean'] = data.groupby('nr')['union'].transform('mean')

data['lwage_demean'] = data['lwage'] - data['lwage_mean']
data['expersq_demean'] = data['expersq'] - data['expersq_mean']
data['married_demean'] = data['married'] - data['married_mean']
data['union_demean'] = data['union'] - data['union_mean']

model1 = sm.OLS(data['lwage'], sm.add_constant(data[['educ', 'black', 'hisp', 'exper', 'expersq', 'married', 'union', 'd81', 'd82', 'd83', 'd84', 'd85', 'd86', 'd87']])).fit()
model2 = sm.OLS(data['lwage_demean'], sm.add_constant(data[['expersq_demean', 'married_demean', 'union_demean', 'd81', 'd82', 'd83', 'd84', 'd85', 'd86', 'd87']])).fit()

# Display regression results using stargazer
summary = summary_col([model1, model2], stars=True)
print(summary)
```

### Stata

```{stata}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Stata"
use "files/WAGEPAN.dta" , clear

bys nr:  egen lwage_mean = mean(lwage) 
bys nr:  egen expersq_mean = mean(expersq) 
bys nr:  egen married_mean = mean(married) 
bys nr:  egen union_mean = mean(union)

gen lwage_demean = lwage - lwage_mean
gen expersq_demean = expersq - expersq_mean
gen married_demean = married - married_mean
gen union_demean = union - union_mean

eststo: qui reg lwage        educ black hisp exper expersq       married        union d81 d82 d83 d84 d85 d86 d87
eststo: qui reg lwage_demean expersq_demean married_demean union_demean d81 d82 d83 d84 d85 d86 d87
esttab , mtitles("OLS" "Demean") compress

```  

:::






# Practical Tips  {.smaller background="#fce0cc"}


## Practical Tips  {.smaller background="#fce0cc"}

You will not need to demean the variables every time you want to estimate a fixed effect models.

The statistical softwares have packages that do that.

You only need to know that **Fixed effects model** is a **demeaned model**, i.e., a **within-transformation model**. 

But notice that you will have many different Fixed Effects together:

- Firm Fixed Effects
- Year Fixed Effects
- Individual Fixed Effects (if individuals change between firms)

. . . 

I am calling a **multidimensional fixed effects design** if you expand the FE to interactions of FE. Most common:

- Year-Industry Fixed Effects.
- CEO-Firm Fixed Effects.







## Practical Tips  {.smaller background="#fce0cc"}

Notice the number of dummies in the last two columns.

::: panel-tabset
### R

```{r}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output-location: default
#| code-fold: true
#| code-summary: "R"
#| code-line-numbers: true
#| eval: true
library(foreign)
library(stargazer)
library(sandwich)
library(plm)

data <- read.dta("files/WAGEPAN.dta")
# Calculate mean by nr for lwage, expersq, married, and union
data <- data[order(data$nr), ]  # Sort data by nr for by-group operations
data$lwage_mean <- ave(data$lwage, data$nr, FUN = mean)
data$expersq_mean <- ave(data$expersq, data$nr, FUN = mean)
data$married_mean <- ave(data$married, data$nr, FUN = mean)
data$union_mean <- ave(data$union, data$nr, FUN = mean)

data$lwage_demean <- data$lwage - data$lwage_mean
data$expersq_demean <- data$expersq - data$expersq_mean
data$married_demean <- data$married - data$married_mean
data$union_demean <- data$union - data$union_mean

# set panel data
pdata <- pdata.frame(data, index = c("nr", "year"))

# Random effects regression using plm
model_de <- lm(lwage_demean ~  expersq_demean + married_demean + union_demean +  d81 +d82+ d83+ d84+ d85 +d86 +d87 , data = data)
model_fe <- plm(lwage ~  expersq + married + union + factor(year)              + educ + black + hisp + exper, data = pdata, model = "within")
model_du <- lm( lwage ~  expersq + married + union + factor(year) + factor(nr) + educ + black + hisp + exper, data = data)

# Display regression results using stargazer
#summary(model_de)
#summary(model_fe)
#summary(model_du)
stargazer(model_de, model_fe, model_du ,title = "Regression Results",  type = "text")
```

### Stata

```{stata}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Stata"
use "files/WAGEPAN.dta" , clear

bys nr:  egen lwage_mean = mean(lwage) 
bys nr:  egen expersq_mean = mean(expersq) 
bys nr:  egen married_mean = mean(married) 
bys nr:  egen union_mean = mean(union)

gen lwage_demean = lwage - lwage_mean
gen expersq_demean = expersq - expersq_mean
gen married_demean = married - married_mean
gen union_demean = union - union_mean

xtset nr year 
eststo: qui reg lwage_demean expersq_demean married_demean union_demean i.year
eststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , fe
eststo: qui reg lwage expersq married union i.year i.nr  educ black hisp exper 
esttab , mtitles("Demean" "FE" "LSDV") compress
```  

:::











## Practical Tips  {.smaller background="#fce0cc"}

Notice that the parameter $\delta$ does not have meaning. 

$$y_{i,t} = \alpha + \beta_1 x_{i,t} + \delta FE +  \epsilon_{i,t}$$

In fact, the previous slides have shown that you will find the same results of a FE model if you include the dummies for the units in the panel (i.e., dummies for the firms or individuals, etc.).

This is called **least squares dummy variable (LSDV) model**.

- the SE are also identical to the within-transformation model.

- But the R2 of the LSDV will be very high because you are including a lot of "explanatory variables".

::: {.callout-note}
At the end of the day, you will use the package for the unit's FE (i.e., the firm), and will include the additional FE as dummies, just like a LSDV model.
:::




## Practical Tips  {.smaller background="#fce0cc"}

When you estimate a LSDV, the software will inform an $\alpha$. 

But this coefficient **has no interpretation whatsoever.** 

- it will be FE for the dropped unit of FE. 

You can simply ignore it, you even don't need to include in your final table. 

No problem if you do, just **don't make inferences from it**.





## Practical Tips  {.smaller background="#fce0cc"}

A FE model helps a lot, but it only does what it can do.

That is, FE models do not capture **time-variant unobserved heterogeneity**.

. . .

Also, if you have constant Xs in your model, you will have to drop them.

- More technically, if there is no within-variation in a X, you cannot include it (the software will drop them).

- For instance, the software will drop $year_{birth}$ below if you include CEO FE.

$$Y_{i,t} = \alpha + \beta_1 year_{birth} + CEO \;FE + ... + \epsilon_{i,t}$$

If you attempt to include the CEO FE manually, the software will drop a random CEO FE or the variable $year_{birth}$. If you get a beta for $year_{birth}$ it has no meaning.






## Practical Tips  {.smaller background="#fce0cc"}

Adding many FE can demand a lot of computational power.

Consider the multidimensional model as follows:

$$Y_{i,t} = \alpha + \beta_1 X_{i,t} + Firm \;FE + Year\; FE + Year.Industry \;FE + CEO \;FE + ... + \epsilon_{i,t}$$

It would take a while to estimate in an average computer.








# Random Effects  {.smaller background="#c6f7ec"}

## Random Effects  {.smaller background="#c6f7ec"}

Remember that:

$$\epsilon_{i,t} = c_i + \mu_{i,t}$$

The most important thing here is whether $x_{it}$ and $c_i$ are correlated.
    
- If they are, you should estimate Fixed Effects

- If $x_{it}$ and $c_i$ are not correlated, then $c_i$  is referred to as a **random effect**.

  - Endogeneity is not a concern; however, the computation of standard errors is affected.

But, if the $x_{it}$ and $c_i$ are not correlated, there is **no endogeneity concern**. 

$c_i$ can be let as part of the $\epsilon_{i,t}$ without bias in the estimated betas.







## Random Effects  {.smaller background="#c6f7ec"}

Additionally, the assumption that $x_{it}$ and $c_i$ are not correlated is rather strong and not practical to most applications of corporate finance, economics or public policy.

RE is a model not used often. Cunningham does not even discuss it.

*If the key explanatory variable is constant over time, we cannot use FE to estimate its effect on y.*

*Of course, we can only use RE because we are willing to assume the unobserved effect is uncorrelated with all explanatory variables.*

*Typically, if one uses RE, and as many time-constant controls as possible are included among the explanatory variables (with an FE analysis, it is not necessary to include such controls) RE is preferred to pooled OLS because RE is generally more efficient.*

(Wooldridge, p.496)








## Random Effects  {.smaller background="#c6f7ec"}

::: panel-tabset
### R

```{r}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output-location: default
#| code-fold: true
#| code-summary: "R"
#| code-line-numbers: true
#| eval: true
# Load necessary packages
library(plm)
library(jtools)
library(foreign)
data <- read.dta("files/WAGEPAN.dta")
pdata <- pdata.frame(data, index = c("nr", "year"))

po_model <- lm(lwage ~ expersq + married + union + factor(year) + educ + black + hisp + exper, data = data)
fe_model <- plm(lwage ~ expersq + married + union + factor(year) + educ + black + hisp + exper, data = pdata, model = "within")
re_model <- plm(lwage ~ expersq + married + union + factor(year) + educ + black + hisp + exper, data = pdata, model = "random")

stargazer(po_model, fe_model , re_model ,title = "Regression Results", column.labels=c("OLS","FE","RE"),  type = "text")

```


### Stata

```{stata}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Stata"
use "files/WAGEPAN.dta" , clear

xtset nr year 
eststo: qui reg   lwage expersq married union i.year  educ black hisp exper 
eststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , fe
eststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , re

esttab , mtitles("OLS" "FE" "RE") compress

```  

:::












# FE vs. RE    {.smaller background="#5c97f7" }


## FE vs. RE    {.smaller background="#5c97f7" }

*The idea is that one uses the random effects estimates unless the Hausman test rejects.* 

*In practice, a failure to reject means either that the RE and FE estimates are sufficiently close so that it does not matter which is used, or the sampling variation is so large in the FE estimates that one cannot conclude practically significant differences are statistically significant.* (Wooldridge)


**If the p-value of the Hausman test is significant then use FE, if not use RE.**





## FE vs. RE   {.smaller background="#5c97f7" }


::: panel-tabset

### Stata

```{stata}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Stata"
use "files/WAGEPAN.dta", clear
xtset nr year
qui xtreg lwage expersq married union i.year educ black hisp exper, fe
estimates store fe_model
qui xtreg lwage expersq married union i.year educ black hisp exper, re
estimates store re_model
hausman fe_model re_model
```  

:::







# First differences   {.smaller background="#e3e2b8"}

## First differences   {.smaller background="#e3e2b8"}

In most applications, the main reason for collecting panel data is **to allow for the unobserved effect, $c_i$, to be correlated with the explanatory variables**. 

For example, in the crime equation, we want to allow the unmeasured city factors in $c_i$ that affect the crime rate also to be correlated with the unemployment rate. 

It turns out that this is simple to allow: **because $c_i$ is constant over time, we can difference the data across the two years.** 

More precisely, for a cross-sectional observation $i$, write the two years as:


$$y_{i,1} = \beta_0 + \beta_1 x_{i,1} + c_i + \mu_{i,1}, t=1$$ 

$$y_{i,2} = (\beta_0 + \delta_0) + \beta_1 x_{i,2} + c_i + \mu_{i,2}, t=2$$ 

If we subtract the second equation from the first, we obtain

$$(y_{i,2} - y_{i,1}) = \delta_0 + \beta_1 (x_{i,2} - x_{i,1}) + (\mu_{i,2}-\mu_{i,1})$$ 


$$\Delta y_{i} = \delta_0 + \beta_1 \Delta x_{i} + \Delta \mu_{i}$$ 









## First differences   {.smaller background="#e3e2b8"}

**So, rather than subtracting the group mean of each variable, you  subtract the lagged observation.**

Not hard to see that, when t=2, FE and FD will give identical solutions

. . .

- FE is more efficient if disturbances $\mu_{i,t}$ have low serial correlation

- FD is more efficient if disturbance $\mu_{i,t}$ follow a random walk

At the end of the day, you can estimate both. 

Empirical research usually estimate FD only in specific circumstances, when they are interested in how changes of X affect changes of Y.

Things like stationarity or trends are often not concerns in panel data

- where N is 10 to 20 






## First differences   {.smaller background="#e3e2b8"}

::: panel-tabset
### R

```{r}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output-location: default
#| code-fold: true
#| code-summary: "R"
#| code-line-numbers: true
#| eval: true
# Load necessary packages
# Load necessary libraries
library(plm)
library(lmtest)
library(stargazer)

data <- read.dta("files/WAGEPAN.dta")
pdata <- pdata.frame(data, index = c("nr", "year"))

ols_model <- lm(lwage ~ expersq + married + union + factor(year) + educ + black + hisp + exper, data = pdata)
fe_model <- plm(lwage ~ expersq + married + union + educ + black + hisp + exper, data = pdata, model = "within")
re_model <- plm(lwage ~ expersq + married + union + educ + black + hisp + exper, data = pdata, model = "random")
fd_model <- plm(lwage ~ expersq + married + union + educ + black + hisp + exper, data = pdata, model = "fd")

stargazer(ols_model, fe_model ,re_model, fd_model,title = "Regression Results",   type = "text")

```



### Stata

```{stata}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Stata"
use "files/WAGEPAN.dta" , clear

xtset nr year 
eststo: qui reg   lwage expersq married union i.year  educ black hisp exper 
eststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , fe
eststo: qui xtreg lwage expersq married union i.year  educ black hisp exper , re
eststo: qui reg D.lwage D.expersq D.married D.union i.year  D.educ D.black D.hisp D.exper 

esttab , mtitles("OLS" "FE" "RE" "FD") compress

```  

:::











# Lagged independent variables   {.smaller background="#e3bfc3"}

## Lagged independent variables   {.smaller background="#e3bfc3"}

When you have a panel data and are concerned with simultaneity between Y and X, you can endeavor in lagging the Xs.


$$y_{i,t} = \beta_0 + \beta_1 x_{i,t-1} + c_i + \mu_{i,t}$$ 

As a matter of fact, this is often expected in finance research. 

. . . 

There is a limitation, however.

The usual proxy of corporate finance research is highly autocorrelated. 
 
 - e.g., total assets do not vary much throughout  time. 
 
Thus, lagging the X often does not make much of a difference. 
 
::: {.callout-tip}
Always do it. Otherwise, you will have to explain why you didn't do it.
:::







# Lagged dependent variables   {.smaller background="#d6cbf5"}

## Lagged dependent variables   {.smaller background="#d6cbf5"}

Sometimes you may have something like

$$y_{i,t} = \beta_0 + \beta_1 y_{i,t-1}+ \beta_2 x_{i,t} + c_i + \mu_{i,t}$$ 

This is called a **Dynamic Panel Model**. It includes $y_{i,t-1}$ as X.

. . .

Consider a FE model.

$$y_{i,t} - \bar{y_i} = \beta_0 + \gamma_1 (y_{i,t-1} - \bar{y}_{i,t-1}) + \omega_2 (x_{i,t-1} - \bar{x_i} )   + (FE_i - \bar{FE}_i)  + (\mu_{i,t} - \bar{\mu}_i )$$ 

The within transformation removes the time-invariant unobserved heterogeneity from the model. 

However, it introduces a correlation between the transformed lag $(y_{i,t−1}−\bar{y}_{i,t-1})$ and the transformed error $(\mu_{i,t−1}−\bar{\mu}_{i,t-1})$ because the average error ($\bar{\mu} = \sum_{i=1}^{T} \mu_{i,t}$) includes $\mu_{i,t-1}$, which is also "included" in $y_{i,t−1}$ 

- $y_{i,t-1} = \beta_0 + \beta_1 y_{i,t-2}+ \beta_2 x_{i,t-1} + c_i + \mu_{i,t-1}$ 








## Lagged dependent variables   {.smaller background="#d6cbf5"}

The bias declines with panel length because $\epsilon_{i,t−1}$ becomes a smaller component of the average error term as T increases. 

In other words, with higher T the correlation between the lagged dependent variable and the regression errors becomes smaller.

**[Flannery and Hankins (2013)](https://doi.org/10.1016/j.jcorpfin.2012.09.004)** have a good review with applications in corporate finance.

They conclude that FE is biased when estimating these models.

They suggest to estimate **Sys-GMM** or **Least Squares Dummy Variable Correction**. We do not discuss these models in the course.




















## 🙋‍♂️ **Any Questions?**{ .smaller  background="#fdf6e3"}

::: columns
::: {.column width="50%"}
### Thank You!

![](figs/qa2.png){width="110%" style="box-shadow: none;"}

:::

::: {.column width="50%"}
<div style="text-align:right;">
  <img src="figs/avatar.jpg" width="120px" style="border-radius:50%; box-shadow:0 4px 12px rgba(0,0,0,.25);" />
</div>

### **Henrique C. Martins**

- 🌐 [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)  
- 💼 [LinkedIn](https://www.linkedin.com/in/henriquecastror/)  
- 🧠 [Google Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)  
- 📄 [Lattes CV](http://lattes.cnpq.br/6076997472159785)  
- 🏠 [Personal Website](https://henriquemartins.net/)  
:::
:::

