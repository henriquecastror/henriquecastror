---
title: 'Empirical Methods in Finance'
subtitle: 'Part 1'
author: 'Henrique C. Martins'
format:
  revealjs: 
    slide-number: true
    theme: simple
    chalkboard: true
    preview-links: auto
    logo: figs/background8.png
    css: logo.css
    footer: '**[**Henrique C. Martins**] [[henrique.martins@fgv.br](mailto:henrique.martins@fgv.br)][Do not use without permission]**  '
    multiplex: true
    scrollable: true 
title-slide-attributes:
    data-background-color: "#b1cafa"
include-after: |
  <script type="text/javascript">
    Reveal.on('ready', event => {
      if (event.indexh === 0) {
        document.querySelector("div.has-logo > img.slide-logo").style.display = "none";
      }
    });
    Reveal.addEventListener('slidechanged', (event) => {
      if (event.indexh === 0) {
        Reveal.configure({ slideNumber: null });
        document.querySelector("div.has-logo > img.slide-logo").style.display = "none";
      }
      if (event.indexh === 1) { 
        Reveal.configure({ slideNumber: 'c' });
        document.querySelector("div.has-logo > img.slide-logo").style.display = null;
      }
    });
  </script>

---



```{r setup}
#| include: false
#| warning: false


# library(reticulate)
# use_python("C:/Users/hcmrt/AppData/Local/Programs/Python/Python310/python.exe")
library(reticulate)
library(Statamarkdown)
#reticulate::py_install("matplotlib")
#reticulate::py_install("seaborn")
#reticulate::py_install("pyfinance")
#reticulate::py_install("xlrd")
#reticulate::py_install("quandl")

```


# Agenda

## Agenda {.smaller}

- Apresenta√ß√£o do syllabus do curso
  - Apresenta√ß√£o dos crit√©rios de avalia√ß√£o
  
. . .

- Breve apresenta√ß√£o dos temas de pesquisa e discuss√£o inicial sobre a entrega final

. . .

- In√≠cio do conte√∫do
  - Introdu√ß√£o a causalidade









## Sobre a letter  {.smaller visibility="hidden"}

- Formato letter
  - Entre 2k e 2.5k palavras a depender do journal.
  
*The objective of a letter is to facilitate the rapid dissemination of important research that contains an insight, new data, or discuss current important topic.*
  
- Ir√° requerer todas as etapas da pesquisa (com √™nfase na an√°lise dos dados, i.e., regress√µes).

- Idealmente, ser√° submetida com o/a orientador/a. Leia-se, sua miss√£o √© "convencer" de que o trabalho final √© submet√≠vel a uma revista. 









## Sobre a letter  {.smaller visibility="hidden"}

- Op√ß√µes de revistas que aceitam letter (checar se refer√™ncias e tabelas fazem parte do word count):

  - [Economic Letters](https://www.sciencedirect.com/journal/economics-letters) (ABS3): 2k palavras
  - [Journal of Accounting and Public Policy](https://www.sciencedirect.com/journal/journal-of-accounting-and-public-policy) (ABS3): 3k palavras
  - [Finance Research Letters](https://www.sciencedirect.com/journal/finance-research-letters) (ABS2): 2.5k palavras
  - [Applied Economic Letters](https://www.tandfonline.com/journals/rael20) (ABS1): 2k palavras
  - [Brazilian Review of Finance](https://periodicos.fgv.br/rbfin) (A4): [4k palavras](https://periodicos.fgv.br/rbfin/libraryFiles/downloadPublic/140) 
  
* Voc√™ √© bem-vindo/a para propor outro journal que aceite letter, sob condi√ß√£o de valida√ß√£o junto ao instrutor. 
  





## Stata {.smaller}

**Providenciar instala√ß√£o para pr√≥ximo encontro**.

Para instala√ß√£o do Stata, seguir instru√ß√µes da TI. 







## R {.smaller}

**Providenciar instala√ß√£o para pr√≥ximo encontro**.


Install R [here Win](https://cran.r-project.org/bin/windows/base/)

Install R [here Mac](https://cran.r-project.org/bin/macosx/)

Install R Studio [here](https://posit.co/download/rstudio-desktop/)

. . .

Para instalar e carregar os pacotes voc√™ precisa rodar as duas linhas abaixo.

```{r}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output-location: default
#| code-fold: false
#| code-summary: "R"
#| code-line-numbers: true
#| eval: false

install.packages("ggplot2")
library(ggplot2)
```



## Python {.smaller}

**I might show some code in python, but I cannot offer you support on it.**






# Selection bias  {.smaller background="#fadea7"} 

##  {.smaller background="#fadea7"} 


![](figs/slides4-airplane.png)






##  {.smaller background="#fadea7"} 

![](figs/slides4-path1.jpg)


**Voc√™ nunca sabe o resultado do caminho que n√£o toma.**












## Quais as aplica√ß√µes do que vamos discutir? {.smaller background="#fadea7"} 

H√° uma s√©rie de **quest√µes de pesquisa** que poderiam ser investigadas com as ferramentas que vamos discutir hoje.

::: incremental

1) Vale mais a pena estudar em escola particular ou p√∫blica?

2) Qual o efeito de investimentos de marketing t√™m na lucratividade?

3) Qual o efeito que jornadas de 4 dias semanais t√™m na produtividade?

4) Qual efeito que educa√ß√£o tem na remunera√ß√£o futura?

5) E diversas outras semelhantes...

:::






## Antes de come√ßar: Nossa agenda {.smaller background="#fadea7"} 


::: incremental 

1) Introdu√ß√£o a **pesquisa quantitativa**

2) Validade **Externa** vs. Validade **Interna**

3) **Problemas** em pesquisa quantitativa inferencial

4) **Rem√©dios**

:::






        


## Introdu√ß√£o {.smaller background="#fadea7"} 

**O que fazemos em pesquisa quantitiva?** Seguimos o m√©todo de pesquisa tradicional (com ajustes):

::: incremental

- Observa√ß√£o 

- Quest√£o de pesquisa 

- Modelo te√≥rico (abstrato)

- Hip√≥teses

- Modelo emp√≠rico

- Coleta de dados 

- An√°lise do resultado do modelo (diferente de an√°lise de dados "pura")

- Conclus√£o/desdobramentos/aprendizados
  
:::









## Introdu√ß√£o {.smaller background="#fadea7"} 

**O que fazemos em pesquisa quantitiva?** Seguimos o m√©todo de pesquisa tradicional (com ajustes):


- Observa√ß√£o 

- Quest√£o de pesquisa 

- Modelo te√≥rico (abstrato): **Aqui √© onde a matem√°tica √© necess√°ria**

- Hip√≥teses

- Modelo emp√≠rico: **Estat√≠stica e econometria necess√°rias**

- Coleta de dados: **Geralmente secund√°rios**

- An√°lise do resultado do modelo (diferente de an√°lise de dados "pura")

- Conclus√£o/desdobramentos/aprendizados







. . .

## Defini√ß√£o {.smaller background="#fadea7"} 

**_Pesquisa quantitativa busca testar hip√≥teses..._**

. . .

**_...a partir da defini√ß√£o de modelos formais (abstratos)..._**

. . .

**_...de onde se estimam modelos emp√≠ricos utilizando a estat√≠stica e a econometria como mecanismos/instrumentos._**

. . .


No fim do dia, buscamos **entender as rela√ß√µes** (que tenham **validade interna** e que ofere√ßam **validade externa**) entre diferentes **vari√°veis de interesse.**











## Quais as vantagens? {.smaller background="#fadea7"} 

1) **Validade externa:** 

. . .

- Conceito de que, se a pesquisa tem validade externa, os seus **achados s√£o representativos**.

. . .

- I.e., s√£o **v√°lidos al√©m do seu modelo**. Resultados "valem externamente".

. . .

- Idealmente, buscamos resultados que valem externamente para **acumular conhecimento**...

. . .

- ...naturalmente, nem toda pesquisa quantitativa oferece validade externa. A pesquisa √≥tima sim. **A pesquisa excelente tem validade externa para al√©m do seu tempo**.

. . .

- Pesquisa qualitativa dificilmente oferece **validade externa**.










## Quais as armadilhas? {.smaller background="#fadea7"} 


2) **Validade interna:** 

. . .

- Conceito de que a pesquisa precisa de validade interna para que seus **resultados sejam cr√≠veis**.

. . .

- I.e., os **resultados n√£o podem conter erros**, vieses, problemas de estima√ß√£o, problemas nos dados, etc..

. . .

- √â aqui que a gente separa a pesquisa ruim da pesquisa boa. Para ser levada a s√©rio, a pesquisa **PRECISA** ter validade interna.

. . .

- Mas isso, nem sempre √© trivial. Muitas pesquisas que vemos publicadas, mesmo em top journals, **n√£o t√™m validade interna** (seja por erro do pesquisador, por m√©todo incorreto, por falta de dados...)

. . .

- Mas cada vez mais, **avaliadores est√£o de olho** em problemas e em modelos  **Trash-in-Trash-out**











## Como fazemos na pr√°tica? {.smaller background="#fadea7"} 

Exemplo de modelo emp√≠rico:

$Y_{i} = Œ± + ùú∑_{1} √ó X_i + Controls + error$

. . .

<img src="figs/slides4-ols.jpg" width="30%" align="right" />

. . .

Uma vez que estimemos esse modelo, temos o **valor**, o **sinal** e a **signific√¢ncia** do $ùú∑$.

. . .

Se o Beta for **significativamente diferente de zero** e **positivo** --> X e Y est√£o positivamente correlacionados.

. . .

**O problema?** Os pacotes estat√≠sticos que utilizamos **sempre "cospem" um beta**. Seja ele com ou sem vi√©s.

. . .

Cabe ao pesquisador ter um **design emp√≠rico** que garanta que o beta estimado tenha validade interna.





## Como fazemos na pr√°tica? {.smaller background="#fadea7"} 


<img src="figs/slides4-table.png" width="110%" align="center" />

A decis√£o final √© baseada na signific√¢ncia do Beta estimado. Se **significativo**, as vari√°veis s√£o relacionadas e fazemos infer√™ncias em cima disso.

Contudo, **sem um design emp√≠rico inteligente**, o beta encontrado pode ter literalmente qualquer sinal e signific√¢ncia.












## Exemplo desses problemas {.smaller background="#fadea7"} 

Veja esse [site](http://www.tylervigen.com/spurious-correlations).

<img src="figs/slides4-spurius1.png" width="100%" align="center" />





## Exemplo desses problemas {.smaller background="#fadea7"} 

Veja esse [site](http://www.tylervigen.com/spurious-correlations).

<img src="figs/slides4-spurius2.png" width="110%" align="center" />
















## Selection bias - We see I {.smaller background="#fadea7"} 


::: panel-tabset

### R

```{r}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output-location: default
#| code-fold: true
#| code-summary: "R"
#| code-line-numbers: true
#| eval: true

library(data.table)
library(ggplot2)
# Generate Data
n = 10000
set.seed(100)
x <- rnorm(n)
y <- rnorm(n)
data1 <- 1/(1+exp( 2 - x  -  y))
group  <- rbinom(n, 1, data1)

# Data Together
data_we_see     <- subset(data.table(x, y, group), group==1)
data_all        <- data.table(x, y, group)

# Graphs
ggplot(data_we_see, aes(x = x, y = y)) + 
      geom_point(aes(colour = factor(-group)), size = 1) +
      geom_smooth(method=lm, se=FALSE, fullrange=FALSE)+
      labs( y = "", x="", title = "The observations we see")+
      xlim(-3,4)+ ylim(-3,4)+ 
      theme(plot.title = element_text(color="black", size=30, face="bold"),
            panel.background = element_rect(fill = "grey95", colour = "grey95"),
            axis.text.y = element_text(face="bold", color="black", size = 18),
            axis.text.x = element_text(face="bold", color="black", size = 18),
            legend.position = "none")
```       


### Python

```{python}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| results: false
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Python"
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

n = 10000
np.random.seed(100)
x = np.random.normal(size=n)
y = np.random.normal(size=n)
data1 = 1 / (1 + np.exp(2 - x - y))
group = np.random.binomial(1, data1, n)

data_we_see = pd.DataFrame({'x': x[group == 1], 'y': y[group == 1], 'group': group[group == 1]})
data_all = pd.DataFrame({'x': x, 'y': y, 'group': group})

sns.set(style='whitegrid')
plt.figure(figsize=(7, 5))
plt.scatter(data_we_see['x'], data_we_see['y'], c=-data_we_see['group'], cmap='viridis', s=20)
sns.regplot(x='x', y='y', data=data_we_see, scatter=False, ci=None, line_kws={'color': 'blue'})
plt.title("The observations we see", fontsize=18)
plt.xlabel("")
plt.ylabel("")
plt.show()

```       

### Stata

```{stata}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| results: false
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Stata"
clear all
set seed 100
set obs 10000
gen x = rnormal(0,1)
gen y = rnormal(0,1)
gen data1 = 1 / (1 + exp(2 - x - y))
gen group = rbinomial(1, data1)
twoway (scatter x y if group == 1, mcolor(black) msize(small))    (lfit y x if group == 1, color(blue)),title("The observations we see", size(large) ) xtitle("") ytitle("")
quietly graph export figs/graph1.svg, replace
```       

![](figs/graph1.svg)

:::












## Selection bias - We see II  {.smaller background="#fadea7"} 

::: panel-tabset

### R
```{r}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output-location: default
#| code-fold: true
#| code-summary: "R"
#| code-line-numbers: true
#| eval: true

# Fit a linear regression model
model <- lm(y ~ x, data = data_we_see)
# Print the summary of the regression model
summary(model)
```

### Python

```{python}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Python"

import statsmodels.api as sm
import pandas as pd
n = 10000
np.random.seed(100)
x = np.random.normal(size=n)
y = np.random.normal(size=n)
data1 = 1 / (1 + np.exp(2 - x - y))
group = np.random.binomial(1, data1, n)

data_we_see = pd.DataFrame({'x': x[group == 1], 'y': y[group == 1], 'group': group[group == 1]})
data_all = pd.DataFrame({'x': x, 'y': y, 'group': group})

X = data_we_see['x']  
X = sm.add_constant(X)
y = data_we_see['y']  
model = sm.OLS(y, X).fit()
print(model.summary())
```

### Stata

```{stata}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Stata"
clear all
set seed 100
set obs 10000
gen x = rnormal(0,1)
gen y = rnormal(0,1)
gen data1 = 1 / (1 + exp(2 - x - y))
gen group = rbinomial(1, data1)
reg y x if group ==1

```    

:::



















## Selection bias - All I  {.smaller background="#fadea7"} 

::: panel-tabset

### R

```{r}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output-location: default
#| code-fold: true
#| code-summary: "R"
#| code-line-numbers: true
#| eval: true

ggplot(data_all, aes(x = x, y = y,  colour=group)) + 
  geom_point(aes(colour = factor(-group)), size = 1) +
  geom_smooth(method=lm, se=FALSE, fullrange=FALSE)+
  labs( y = "", x="", title = "All observations")+
  xlim(-3,4)+ ylim(-3,4)+ 
  theme(plot.title = element_text(color="black", size=30, face="bold"),
      panel.background = element_rect(fill = "grey95", colour = "grey95"),
      axis.text.y = element_text(face="bold", color="black", size = 18),
      axis.text.x = element_text(face="bold", color="black", size = 18),
      legend.position = "none")
``` 

### Python

```{python}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| results: false
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Python"
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style='whitegrid')
plt.figure(figsize=(6, 4))
sns.scatterplot(data=data_all, x='x', y='y', hue='group', palette=['blue', 'red'], s=20)
sns.regplot(data=data_all, x='x', y='y', scatter=False, ci=None, line_kws={'color': 'blue'})
plt.title("All observations", fontsize=18)
plt.xlabel("")
plt.ylabel("")
plt.legend(title="Group", labels=["0", "1"], loc="upper left")

plt.gca().get_legend().remove()
plt.show()
```       

### Stata

```{stata}
#| warning: false
#| message: false
#| fig-align: center
#| echo: false
#| output: true
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Stata"
clear all
set seed 100
set obs 10000
gen x = rnormal(0,1)
gen y = rnormal(0,1)
gen data1 = 1 / (1 + exp(2 - x - y))
gen group = rbinomial(1, data1)
twoway (scatter x y if group == 1, mcolor(red) msize(small))   (scatter x y if group == 0, mcolor(blue) msize(small))   (lfit y x , color(blue)),  title("All observations", size(large))    legend(order(1 "Group 0" 2 "Group 1")) 
quietly graph export figs/graph2.svg, replace

```  

![](figs/graph2.svg)

:::










## Selection bias - All I {.smaller background="#fadea7"} 

::: panel-tabset

### R
```{r}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output-location: default
#| code-fold: true
#| code-summary: "R"
#| code-line-numbers: true
#| eval: true

model2 <- lm(y ~ x, data = data_all)
summary(model2)
```

### Python

```{python}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Python"

import statsmodels.api as sm
import pandas as pd
n = 10000
np.random.seed(100)
x = np.random.normal(size=n)
y = np.random.normal(size=n)
data1 = 1 / (1 + np.exp(2 - x - y))
group = np.random.binomial(1, data1, n)

data_we_see = pd.DataFrame({'x': x[group == 1], 'y': y[group == 1], 'group': group[group == 1]})
data_all = pd.DataFrame({'x': x, 'y': y, 'group': group})

X = data_all['x']  
X = sm.add_constant(X)
y = data_all['y']  
model = sm.OLS(y, X).fit()
print(model.summary())
```

### Stata

```{stata}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Stata"
clear all
set seed 100
set obs 10000
gen x = rnormal(0,1)
gen y = rnormal(0,1)
gen data1 = 1 / (1 + exp(2 - x - y))
gen group = rbinomial(1, data1)
reg y x 

```    


:::









## Selection bias {.smaller background="#fadea7"} 

Selection bias n√£o √© o √∫nico dos nossos problemas, mas √© um **importante**.

Veja que suas conclus√µes mudaram significativamente.

N√£o seria dif√≠cil criar um exemplo em que o **coeficiente verdadeiro** fosse positivo.











## Exemplo desses problemas {.smaller background="#fadea7"} 

![](figs/slides4-path2b.png) 


Source: [Angrist](https://www.youtube.com/watch?v=iPBV3BlV7jk)

**N√£o podemos pegar dois caminhos.**










## Exemplo desses problemas {.smaller background="#fadea7"} 

![](figs/slides4-matching.png) 


Source: [Angrist](https://www.youtube.com/watch?v=6YrIDhaUQOE)

**N√£o podemos comparar pessoas que n√£o s√£o compar√°veis.**








## O que precisamos fazer? {.smaller background="#fadea7"} 

. . .

Definir um bom **_Design emp√≠rico_**

. . .

No mundo ideal: ter√≠amos **universos paralelos.** Ter√≠amos **dois clones**, em que cada um escolhe um caminho. Todo o resto √© igual.

- Obviamente, isso n√£o existe.

. . .

Segunda melhor solu√ß√£o: **experimentos**

. . .

**Mas o que √© um experimento?**

- Grupo de tratamento vs. Grupo de controle

- Igualdade entre os grupos (i.e., aleatoriedade no sampling)

    - Nada diferencia os grupos a n√£o ser o fato de que um indiv√≠duo recebe tratamento e o outro n√£o
    - Estamos comparando ma√ßas com ma√ßas e laranjas com laranjas
      
- Testes placebo/falsifica√ß√£o.













# The challenge {.smaller background="#b0aeae"}

## Correlation & Causality {.smaller background="#b0aeae"}


It is very common these days to hear someone say ‚Äú*correlation does not mean causality*.‚Äù 

In essence, that is true.

- *The killer struck during daylight. Had the sun not been out that day, the victim would have been safe.*

. . .

- There is a correlation, but it is clear there is no causation.







## Correlation & Causality  {.smaller background="#b0aeae"}

Sometimes, there is causality even when we do not observe correlation.

*The sailor is adjusting the rudder on a windy day to align the boat with the wind, but the boat is not changing direction.* ([Source: The Mixtape](https://mixtape.scunning.com/01-introduction#do-not-confuse-correlation-with-causality))


![](figs/scottboat.jpg)





::: {.callout-note}

In this example, the sailor is *endogenously* adjusting the course to balance the unobserved wind.

:::







## The challenge  {.smaller background="#b0aeae"}

- I will discuss some issues in using plain OLS models in Finance Research (mainly with panel data).

. . .

- I will avoid the word ‚Äúendogeneity‚Äù as much as I can

. . .

- I will also avoid the word ‚Äúidentification‚Äù because identification does not guarantee causality and vice-versa (Kahn and Whited 2017)

. . .

- The discussion is based on [Atanasov and Black (2016)](https://www.nowpublishers.com/article/Details/CFR-0036)

![](figs/slides1-empiricalissues-paper.png)








## The challenge  {.smaller background="#b0aeae"}

- Imagine that you want to investigate the effect of Governance on Q

    - You may have more covariates explaining Q (omitted  from slides)
  
 $ùë∏_{i} = Œ± + ùú∑_{i} √ó Gov + Controls + error$

. . . 

 All the issues in the next slides will make it not possible to infer that __changing Gov will _CAUSE_ a change in Q__ 
 
 That is, cannot infer causality
 
![](figs/slides1-empiricalissues-wrong.jpg)








## 1) Reverse causation   {.smaller background="#b0aeae"}

_One source of bias is: reverse causation_

- Perhaps it is Q that causes Gov

- OLS based methods do not tell the difference between these two betas:

$ùëÑ_{i} = Œ± + ùú∑_{i} √ó Gov + Controls + error$

$Gov_{i} = Œ± + ùú∑_{i} √ó Q + Controls + error$

- If one Beta is significant, the other will most likely be significant too

- You need a sound theory!












## 2) Omitted variable bias (OVB)  {.smaller background="#b0aeae"}

_The second source of bias is: OVB_

- Imagine that you do not include an important ‚Äútrue‚Äù predictor of Q

- Let's say, long is:  $ùë∏_{i} = ùú∂_{long} + ùú∑_{long}* gov_{i} + Œ¥ * omitted + error$

- But you estimate short:  $ùë∏_{i} = ùú∂_{short} + ùú∑_{short}* gov_{i} + error$

- $ùú∑_{short}$ will be: 

    - $ùú∑_{short} = ùú∑_{long}$ +  bias

    - $ùú∑_{short} = ùú∑_{long}$ +  relationship between omitted (omitted) and included (Gov) * effect of omitted in long (Œ¥)

        - Where: relationship between omitted (omitted) and included (Gov) is: $Omitted = ùú∂ + œï *gov_{i} + u$

- Thus, OVB is: $ùú∑_{short} ‚Äì ùú∑_{long} = œï * Œ¥$














## 3) Specification error  {.smaller background="#b0aeae"}

_The third source of bias is: Specification error_

- Even if we could perfectly measure gov and all relevant covariates, we would not know for sure the functional form through which each influences q

    - Functional form: linear? Quadratic? Log-log? Semi-log?

- Misspecification of x‚Äôs is similar to OVB








## 4) Signaling   {.smaller background="#b0aeae"}

_The fourth source of bias is: Signaling_

- Perhaps, some individuals are signaling the existence of an X without truly having it:

    - For instance: firms signaling they have good governance without having it

- This is similar to the OVB because you cannot observe the full story









## 5) Simultaneity  {.smaller background="#b0aeae"}

_The fifth source of bias is: Simultaneity_

- Perhaps gov and some other variable x are determined simultaneously

- Perhaps there is bidirectional causation, with q causing gov and gov also causing q 

- In both cases, OLS regression will provide a biased estimate of the effect

- Also, the sign might be wrong










## 6) Heterogeneous effects   {.smaller background="#b0aeae"}

_The sixth source of bias is: Heterogeneous effects_

- Maybe the causal effect of gov on q depends on observed and unobserved firm characteristics:

    - Let's assume that firms seek to maximize q
    - Different firms have different optimal gov
    - Firms know their optimal gov
    - If we observed all factors that affect q, each firm would be at its own optimum and OLS regression would give a non-significant coefficient

- In such case, we may find a positive or negative relationship.

- Neither is the true causal relationship





## 7) Construct validity  {.smaller background="#b0aeae"}

_The seventh source of bias is: Construct validity_

- Some constructs (e.g. Corporate governance) are complex, and sometimes have conflicting mechanisms

- We usually don‚Äôt know for sure what ‚Äúgood‚Äù governance is, for instance

- It is common that we use imperfect proxies

- They may poorly fit the underlying concept







## 8) Measurement error   {.smaller background="#b0aeae"}

_The eighth source of bias is: Measurement error_

- "Classical" random measurement error for the outcome will inflate standard errors but will not lead to biased coefficients. 

    - $y^{*} = y + \sigma_{1}$
    - If you estimante $y^{*} = f(x)$, you have $y + \sigma_{1} = x + \epsilon$ 
    - $y = x + u$ 
        - where $u = \epsilon + \sigma_{1}$ 

- "Classical" random measurement error in x‚Äôs will bias coefficient estimates toward zero

    - $x^{*} = x + \sigma_{2}$
    - Imagine that $x^{*}$ is a bunch of noise
    - It would not explain anything
    - Thus, your results are biased toward zero


<!-- https://web.stanford.edu/class/polisci100a/regress5.pdf  --> 









## 9) Observation bias   {.smaller background="#b0aeae"}

_The ninth source of bias is: Observation bias_

- This is analogous to the Hawthorne effect, in which observed subjects behave differently because they are observed

- Firms which change gov may behave differently because their managers or employees think the change in gov matters, when in fact it has no direct effect










## 10) Interdependent effects   {.smaller background="#b0aeae"}

_The tenth source of bias is: Interdependent effects_

- Imagine that a governance reform that will not affect share prices for a single firm might be effective if several firms adopt

- Conversely, a reform that improves efficiency for a single firm might not improve profitability if adopted widely because the gains will be competed away

- "One swallow doesn't make a summer" 






## 11) Selection bias   {.smaller background="#b0aeae"}

_The eleventh source of bias is: Selection bias_

- If you run a regression with two types of companies

    - High gov (let's say they are the treated group)
    - Low gov (let's say they are the control group)

    
- Without any matching method, these companies are likely not comparable

- Thus, the estimated beta will contain selection bias

- The bias can be either be positive or negative

- It is similar to OVB


  

## 12) Self-Selection  {.smaller background="#b0aeae"}

_The twelfth source of bias is: Self-Selection_

- Self-selection is a type of selection bias

- Usually, firms decide which level of governance they adopt

- There are reasons why firms adopt high governance

    - If observable, you need to control for
    - If unobservable, you have a problem

- It is like they "self-select" into the treatment.

    - Units decide whether they receive the treatment of not

- Your coefficients will be biased.






## 13) Collider Bias (endog. selection bias) {.smaller background="#b0aeae"}


**Let's assume an arbitrary population.**

- Two variables describe the population: IQ and luck. 

- These variables are random and normally distributed.

- Let's say that after you reach a certain level of IQ and Luck, you become successful (i.e., upper-right quadrant).

- Because you are interested in successful people, you only investigate such subsample. 




## 13) Collider Bias (endog. selection bias) {.smaller background="#b0aeae"}

**A representation of the population**

::: panel-tabset

### R
```{r}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output-location: default
#| code-fold: true
#| code-summary: "R"
#| code-line-numbers: true
#| eval: true
library(data.table)
library(ggplot2)
set.seed(100)
luck <- rnorm(1000, 100, 15)
iq   <- rnorm(1000, 100, 15)
pop <- data.frame(luck, iq)

ggplot(pop, aes(x = iq, y = luck)) + 
      geom_point() +
      labs(title = "The general population")  + 
  theme_minimal()
```



### Python

```{python}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Python"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

np.random.seed(100)

luck = np.random.normal(100, 15, 1000)
iq = np.random.normal(100, 15, 1000)
pop = pd.DataFrame({'luck': luck, 'iq': iq})

sns.set(style="whitegrid")  
plt.figure(figsize=(8, 6))
plt.scatter(pop['iq'], pop['luck'])
plt.title("The general population")
plt.xlabel("IQ")
plt.ylabel("Luck")
plt.show()

```

### Stata

```{stata}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Stata"
set seed 100
set obs 1000
gen luck = rnormal(100, 15)
gen iq = rnormal(100, 15)
twoway (scatter luck iq), title("The general population") 
quietly graph export figs/collider1.svg, replace
```       

![](figs/collider1.svg)

:::





## 13) Collider Bias (endog. selection bias) {.smaller background="#b0aeae"}

::: {.callout-important}
**Analyzing only successful people will suggest a negative correlation between luck and IQ.**

- "Success" is a collider in this example. It "collides" with luck and IQ.
::: 

::: panel-tabset

### R
```{r}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output-location: default
#| code-fold: true
#| code-summary: "R"
#| code-line-numbers: true
#| eval: true
library(data.table)
library(ggplot2)
set.seed(100)
luck <- rnorm(1000, 100, 15)
iq   <- rnorm(1000, 100, 15)
pop <- data.frame(luck, iq)

pop$comb <- pop$luck + pop$iq 
successfull <- pop[pop$comb > 240, ] 

ggplot() + 
  geom_point(data = pop, aes(x = iq, y = luck)) +  
  geom_point(data = successfull, aes(x = iq, y = luck), color = "red") +  
  labs(title = "The general population & successful subpopulation") + 
  theme_minimal()

```



### Python

```{python}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Python"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

np.random.seed(100)

luck = np.random.normal(100, 15, 1000)
iq = np.random.normal(100, 15, 1000)
pop = pd.DataFrame({'luck': luck, 'iq': iq})

pop['comb'] = pop['luck'] + pop['iq']

successful = pop[pop['comb'] > 240]

sns.set(style="whitegrid")  # Minimalistic theme similar to theme_minimal in ggplot2
plt.figure(figsize=(8, 6))
plt.scatter(pop['iq'], pop['luck'], label="General Population", alpha=0.5)
plt.scatter(successful['iq'], successful['luck'], color='red', label="Successful Subpopulation", alpha=0.7)
plt.title("The general population & successful subpopulation")
plt.xlabel("IQ")
plt.ylabel("Luck")
plt.legend()
plt.show()

```

### Stata

```{stata}
#| warning: false
#| message: false
#| fig-align: center
#| echo: true
#| output: true
#| output-location: default
#| code-fold: true
#| code-line-numbers: true
#| eval: true
#| code-summary: "Stata"

set seed 100
set obs 1000
gen luck = rnormal(100, 15)
gen iq = rnormal(100, 15)
gen comb = luck + iq
gen successful = comb > 240
twoway (scatter luck iq if successful == 0)  (scatter luck iq if successful == 1, mcolor(red)) , title("The general population & successful subpopulation") 

quietly graph export figs/collider2.svg, replace
```       

![](figs/collider2.svg)

:::



















## Conclus√£o  {.smaller background="#b0aeae"}

**Pesquisa quantitativa tem a parte _quanti (m√©todos, modelos, etc.)_...**

**... Mas talvez a parte mais importante seja o desenho da pesquisa (design emp√≠rico)!**


















## Preocupa√ß√µes recentes em pesquisa   {.smaller background="#b0aeae"}

**P-Hacking**

![](figs/slides4-phacking.png) 

Artigo original [aqui](https://doi.org/10.1111/jofi.12530).










## Preocupa√ß√µes recentes em pesquisa  {.smaller background="#b0aeae"}

**Publication bias**

![](figs/slides4-Harvey-2017.png) 


Artigo original [aqui](https://doi.org/10.1111/jofi.12530).







 

## Preocupa√ß√µes recentes em pesquisa   {.smaller background="#b0aeae"}

**Crise de replica√ß√£o**


![](figs/slides4-aguinis.png) 


Artigo original [aqui](https://link.springer.com/article/10.1057/s41267-017-0081-0).





## Some fun stuff  {.smaller background="#b0aeae"}

![](figs/selection bias.png) 


## Some fun stuff  {.smaller background="#b0aeae"}

![](figs/fig1.jpg) 





## Some fun stuff  {.smaller background="#b0aeae"}


![](figs/hypothesis2.png) 






## Some fun stuff {.smaller background="#b0aeae"}


![](figs/confounding variables.png) 









## Some fun stuff {.smaller background="#b0aeae"}

![](figs/proxy variable.png) 













## **THANK YOU!** {background="#b1cafa"}

::: columns
::: {.column width="60%"}
**QUESTIONS?**

![](figs/qa2.png){width="150%" heigth="150%"}
:::

::: {.column width="40%"}
**Henrique C. Martins**

-   [FGV/EAESP](https://eaesp.fgv.br/en/people/henrique-castro-martins)
-   [Personal Website](https://henriquemartins.net/)
-   [LinkedIn](https://www.linkedin.com/in/henriquecastror/)
-   [Lattes](http://lattes.cnpq.br/6076997472159785)
-   [Scholar](https://scholar.google.com.br/citations?user=7gIfkRMAAAAJ&hl=pt-BR&oi=ao)\
:::
:::

::: footer
:::
